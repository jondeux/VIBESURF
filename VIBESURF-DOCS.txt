Repository: vibesurf-ai/vibesurf
Commit: 391e8a484d556f248ea2245fbb6d48281a534dd7
Files analyzed: 122

Estimated tokens: 274.9k

Directory structure:
â””â”€â”€ vibesurf-ai-vibesurf/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ MANIFEST.in
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ README_zh.md
    â”œâ”€â”€ vibesurf.spec
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ EXECUTABLE_BUILD.md
    â”‚   â””â”€â”€ PYPI_SETUP.md
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ build-local.bat
    â”‚   â””â”€â”€ build-local.sh
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ test_agents.py
    â”‚   â”œâ”€â”€ test_backend_api.py
    â”‚   â”œâ”€â”€ test_browser.py
    â”‚   â”œâ”€â”€ test_tools.py
    â”‚   â””â”€â”€ test_voice_api.py
    â”œâ”€â”€ vibe_surf/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ cli.py
    â”‚   â”œâ”€â”€ common.py
    â”‚   â”œâ”€â”€ logger.py
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ browser_use_agent.py
    â”‚   â”‚   â”œâ”€â”€ report_writer_agent.py
    â”‚   â”‚   â”œâ”€â”€ views.py
    â”‚   â”‚   â””â”€â”€ prompts/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ report_writer_prompt.py
    â”‚   â”‚       â””â”€â”€ vibe_surf_prompt.py
    â”‚   â”œâ”€â”€ backend/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ llm_config.py
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ shared_state.py
    â”‚   â”‚   â”œâ”€â”€ voice_model_config.py
    â”‚   â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ activity.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ browser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ files.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ task.py
    â”‚   â”‚   â”‚   â””â”€â”€ voices.py
    â”‚   â”‚   â”œâ”€â”€ database/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ queries.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py
    â”‚   â”‚   â”‚   â””â”€â”€ migrations/
    â”‚   â”‚   â”‚       â”œâ”€â”€ v001_initial_schema.sql
    â”‚   â”‚   â”‚       â”œâ”€â”€ v002_add_agent_mode.sql
    â”‚   â”‚   â”‚       â”œâ”€â”€ v003_fix_task_status_case.sql
    â”‚   â”‚   â”‚       â””â”€â”€ v004_add_voice_profiles.sql
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ encryption.py
    â”‚   â”‚       â”œâ”€â”€ llm_factory.py
    â”‚   â”‚       â””â”€â”€ utils.py
    â”‚   â”œâ”€â”€ browser/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ agen_browser_profile.py
    â”‚   â”‚   â”œâ”€â”€ agent_browser_session.py
    â”‚   â”‚   â”œâ”€â”€ browser_manager.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â””â”€â”€ watchdogs/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ action_watchdog.py
    â”‚   â”‚       â””â”€â”€ dom_watchdog.py
    â”‚   â”œâ”€â”€ chrome_extension/
    â”‚   â”‚   â”œâ”€â”€ background.js
    â”‚   â”‚   â”œâ”€â”€ config.js
    â”‚   â”‚   â”œâ”€â”€ content.js
    â”‚   â”‚   â”œâ”€â”€ dev-reload.js
    â”‚   â”‚   â”œâ”€â”€ manifest.json
    â”‚   â”‚   â”œâ”€â”€ permission-iframe.html
    â”‚   â”‚   â”œâ”€â”€ permission-request.html
    â”‚   â”‚   â”œâ”€â”€ popup.html
    â”‚   â”‚   â”œâ”€â”€ sidepanel.html
    â”‚   â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â”‚   â”œâ”€â”€ api-client.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ file-manager.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ history-manager.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ main.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ modal-manager.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ permission-iframe-request.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ permission-request.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ session-manager.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ user-settings-storage.js
    â”‚   â”‚   â”‚   â””â”€â”€ voice-recorder.js
    â”‚   â”‚   â””â”€â”€ styles/
    â”‚   â”‚       â”œâ”€â”€ activity.css
    â”‚   â”‚       â”œâ”€â”€ animations.css
    â”‚   â”‚       â”œâ”€â”€ base.css
    â”‚   â”‚       â”œâ”€â”€ components.css
    â”‚   â”‚       â”œâ”€â”€ history-modal.css
    â”‚   â”‚       â”œâ”€â”€ input.css
    â”‚   â”‚       â”œâ”€â”€ layout.css
    â”‚   â”‚       â”œâ”€â”€ responsive.css
    â”‚   â”‚       â”œâ”€â”€ settings-environment.css
    â”‚   â”‚       â”œâ”€â”€ settings-forms.css
    â”‚   â”‚       â”œâ”€â”€ settings-modal.css
    â”‚   â”‚       â”œâ”€â”€ settings-profiles.css
    â”‚   â”‚       â”œâ”€â”€ settings-responsive.css
    â”‚   â”‚       â”œâ”€â”€ settings-utilities.css
    â”‚   â”‚       â””â”€â”€ variables.css
    â”‚   â”œâ”€â”€ llm/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ openai_compatible.py
    â”‚   â””â”€â”€ tools/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ browser_use_tools.py
    â”‚       â”œâ”€â”€ file_system.py
    â”‚       â”œâ”€â”€ finance_tools.py
    â”‚       â”œâ”€â”€ mcp_client.py
    â”‚       â”œâ”€â”€ report_writer_tools.py
    â”‚       â”œâ”€â”€ vibesurf_registry.py
    â”‚       â”œâ”€â”€ views.py
    â”‚       â”œâ”€â”€ voice_asr.py
    â”‚       â””â”€â”€ website_api/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ douyin/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ client.py
    â”‚           â”‚   â”œâ”€â”€ douyin.js
    â”‚           â”‚   â””â”€â”€ helpers.py
    â”‚           â”œâ”€â”€ weibo/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ client.py
    â”‚           â”‚   â””â”€â”€ helpers.py
    â”‚           â”œâ”€â”€ xhs/
    â”‚           â”‚   â”œâ”€â”€ __init__.py
    â”‚           â”‚   â”œâ”€â”€ client.py
    â”‚           â”‚   â””â”€â”€ helpers.py
    â”‚           â””â”€â”€ youtube/
    â”‚               â”œâ”€â”€ __init__.py
    â”‚               â””â”€â”€ helpers.py
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â””â”€â”€ publish.yml

================================================
FILE: README.md
================================================
# VibeSurf: A powerful browser assistant for vibe surfing
[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&label=Discord&logo=discord&logoColor=white)](https://discord.gg/EZ2YnUXP)
[![WarmShao](https://img.shields.io/twitter/follow/warmshao?style=social)](https://x.com/warmshao)

VibeSurf is an open-source AI agentic browser that revolutionizes browser automation and research.

If you're as excited about open-source AI browsing as I am, give it a star! â­

[ä¸­æ–‡](README_zh.md) | [English](README.md)

## âœ¨ Key Features

- ğŸ§  **Advanced AI Automation**: Beyond browser automation, VibeSurf performs deep research, intelligent crawling, content summarization, and more to exploration.

- ğŸš€ **Multi-Agent Parallel Processing**: Run multiple AI agents simultaneously in different browser tabs, enabling both deep research and wide research with massive efficiency gains.

- ğŸ¥· **Stealth-First Architecture**: Uses Chrome DevTools Protocol (CDP) instead of Playwright for superior stealth capabilities, preventing bot detection.

- ğŸ¨ **Seamless Chrome Extension UI**: Native browser integration without switching applications, providing an intuitive interface that feels like part of your browser.

- ğŸ”’ **Privacy-First LLM Support**: Supports local LLMs (Ollama, etc.) and custom LLM APIs to ensure your browsing data stays private and secure during vibe surfing.

## ğŸ› ï¸ Installation Guide

#### Step 1: Clone the Repository
```bash
git clone https://github.com/vvincent1234/VibeSurf.git
cd VibeSurf
```

### Step 2: Set Up Python Environment
We recommend using [uv](https://docs.astral.sh/uv/) for managing the Python environment. Install uv from [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/):

```bash
# On macOS and Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# On Windows
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### Step 3: Setup and Install python environment with [uv]
```bash
uv venv --python 3.12
```

Activate the virtual environment:
- Windows (Command Prompt):
```cmd
.venv\Scripts\activate
```
- Windows (PowerShell):
```powershell
.\.venv\Scripts\Activate.ps1
```
- macOS/Linux:
```bash
source .venv/bin/activate
```

#### Step 4: Install vibesurf Dependencies
Install Python packages:
```bash
uv pip install -e .
```

#### Step 5: Configure Environment
1. Create a copy of the example environment file:
- Windows (Command Prompt):
```bash
copy .env.example .env
```
- macOS/Linux/Windows (PowerShell):
```bash
cp .env.example .env
```
2. Open `.env` in your preferred text editor and add your API keys and other settings


### Step 6: Launch vibesurf

#### Option 1: Direct Server:-
```bash
uvicorn vibe_surf.backend.main:app --host 127.0.0.1 --port 9335
```

#### Option 2: CLI Entry:-
```bash
uv run vibesurf
```

## Browser Configuration
i. **Using Your Own Browser(Optional):**
    - Set `BROWSER_PATH` to the executable path of your browser and `BROWSER_USER_DATA` to the user data directory of your browser. Leave `BROWSER_USER_DATA` empty if you want to use local user data.
      - Windows
        ```env
         BROWSER_PATH="C:\Program Files\Google\Chrome\Application\chrome.exe"
         BROWSER_USER_DATA="C:\Users\YourUsername\AppData\Local\Google\Chrome\User Data"
        ```
        > Note: Replace `YourUsername` with your actual Windows username for Windows systems.
      - Mac
        ```env
         BROWSER_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
         BROWSER_USER_DATA="/Users/YourUsername/Library/Application Support/Google/Chrome"
        ```
    - Close all Chrome windows
    - Open the WebUI in a non-Chrome browser, such as Firefox or Edge. This is important because the persistent browser context will use the Chrome data when running the agent.
    - Check the "Use Own Browser" option within the Browser Settings.


## ğŸ—ºï¸ Roadmap

We're building VibeSurf to be your ultimate AI browser companion. Here's what's coming next:

- [x] **Smart Skills System** - *Completed*
  Add `/search` for quick information search and `/crawl` for automatic website data extraction. Integrated native APIs for Xiaohongshu, Douyin, Weibo, and YouTube.

- [ ] **Powerful Coding Agent** - *In Progress*
  Build a comprehensive coding assistant for data processing and analysis directly in your browser

- [ ] **Agentic Browser Workflow** - *Planned*
  Create custom drag-and-drop workflows for auto-login, data collection, and complex browser automation tasks

- [ ] **Third-Party Integrations** - *Planned*
  Connect with n8n workflows and other tools to combine browsing with automation

- [ ] **Intelligent Memory & Personalization** - *Planned*
  Transform VibeSurf into a truly human-like companion with persistent memory that learns your preferences, habits, and browsing patterns over time


## ğŸ¬ Demo

### How to use?
<video src="https://github.com/user-attachments/assets/0a4650c0-c4ed-423e-9e16-7889e9f9816d" controls="controls">Your browser does not support playing this video!</video>

### Dozens of agent running in on browser
<video src="https://github.com/user-attachments/assets/9c461a6e-5d97-4335-ba09-59e8ec4ad47b" controls="controls">Your browser does not support playing this video!</video>


## ğŸ“ License

This repository is licensed under the [VibeSurf Open Source License](./LICENSE), based on Apache 2.0 with additional conditions.

## ğŸ‘ Acknowledgments

VibeSurf builds on top of other awesome open-source projects:

- [Browser Use](https://github.com/browser-use/browser-use)
- [LangGraph](https://github.com/langchain-ai/langgraph)

Huge thanks to their creators and contributors!




================================================
FILE: LICENSE
================================================
# Open Source License

VibeSurf is licensed under a modified version of the Apache License 2.0, with the following additional conditions:

1. VibeSurf may be utilized commercially, including as a backend service for other applications or as an application development platform for enterprises. Should the conditions below be met, a commercial license must be obtained from the producer:

a. Multi-tenant service: Unless explicitly authorized by VibeSurf in writing, you may not use the VibeSurf source code to operate a multi-tenant environment.
    - Tenant Definition: Within the context of VibeSurf, one tenant corresponds to one workspace. The workspace provides a separated area for each tenant's data and configurations.

b. LOGO and copyright information: In the process of using VibeSurf's frontend, you may not remove or modify the LOGO or copyright information in the VibeSurf console or applications. This restriction is inapplicable to uses of VibeSurf that do not involve its frontend.
    - Frontend Definition: For the purposes of this license, the "frontend" of VibeSurf includes all components located in the `vibe_surf/chrome_extension/` directory when running VibeSurf from the raw source code, or the corresponding frontend container/image when running VibeSurf with Docker or other containerization.

2. As a contributor, you should agree that:

a. The producer can adjust the open-source agreement to be more strict or relaxed as deemed necessary.
b. Your contributed code may be used for commercial purposes, including but not limited to its cloud business operations.

Apart from the specific conditions mentioned above, all other rights and restrictions follow the Apache License 2.0. Detailed information about the Apache License 2.0 can be found at http://www.apache.org/licenses/LICENSE-2.0.

The interactive design of this product is protected by appearance patent.

Â© 2025 VibeSurf Authors. This repository is licensed under the VibeSurf Open Source License, based on Apache 2.0 with additional conditions.


================================================
FILE: MANIFEST.in
================================================
include README.md
include LICENSE
include .env.example
recursive-include vibe_surf/backend *.py
recursive-include vibe_surf/chrome_extension *.js *.html *.css *.json *.png *.md
global-exclude __pycache__
global-exclude *.py[co]
global-exclude .DS_Store
global-exclude Thumbs.db


================================================
FILE: pyproject.toml
================================================
[project]
name = "vibesurf"
dynamic = ["version"]
authors = [{ name = "WarmShao" }]
description = "VibeSurf: A powerful browser assistant for vibe surfing"
readme = "README.md"
requires-python = ">=3.11"
license = { text = "Apache-2.0" }
keywords = ["browser use", "browser automation", "browser assistant", "agentic browser", "vibe surf", "AI browser"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Internet :: WWW/HTTP :: Browsers",
]
dependencies = [
    "uuid7>=0.1.0",
    "aiofiles>=24.1.0",
    "anyio>=4.9.0",
    "psutil>=7.0.0",
    "pydantic>=2.11.5",
    "cdp-use>=1.4.1",
    "json-repair>=0.48.0",
    "aiohttp>=3.12.15",
    "scikit-image>=0.25.2",
    "python-socks>=2.7.2",
    "langgraph>=0.6.4",
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "python-multipart>=0.0.6",
    "websockets>=12.0",
    "python-dotenv>=1.0.0",
    "sqlalchemy>=2.0.43",
    "aiosqlite>=0.21.0",
    "rich>=13.0.0",
    "greenlet>=3.2.4",
    "getmac>=0.9.5",
    "browser-use==0.7.10",
    "markdown-pdf>=1.9",
    "nanoid>=2.0.0",
    "markdownify>=1.2.0",
    "pathvalidate>=3.3.1",
    "dashscope>=1.24.5",
    "yfinance>=0.2.66",
    "pyexecjs>=1.5.1",
    "youtube-transcript-api>=1.2.2",
]

[project.urls]
Repository = "https://github.com/vibesurf-ai/VibeSurf"

[project.scripts]
vibesurf = "vibe_surf.cli:main"

[build-system]
requires = ["setuptools>=61.0", "setuptools-scm>=6.2", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = { find = { where = ["."], include = ["vibe_surf*"] } }

[tool.setuptools.package-data]
"vibe_surf" = [
    "backend/**/*",
    "chrome_extension/**/*"
]

[tool.setuptools.package-dir]
"" = "."

[tool.setuptools_scm]
write_to = "vibe_surf/_version.py"



================================================
FILE: README_zh.md
================================================
# VibeSurfï¼šå¼ºå¤§çš„æµè§ˆå™¨åŠ©æ‰‹ï¼Œç”¨äºæ°›å›´å†²æµª

[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&label=Discord&logo=discord&logoColor=white)](https://discord.gg/EZ2YnUXP)
[![WarmShao](https://img.shields.io/twitter/follow/warmshao?style=social)](https://x.com/warmshao)

VibeSurf æ˜¯ä¸€ä¸ªå¼€æºçš„ AI ä»£ç†æµè§ˆå™¨ï¼Œå®ƒé©æ–°äº†æµè§ˆå™¨è‡ªåŠ¨åŒ–å’Œç ”ç©¶ã€‚

å¦‚æœä½ å’Œæˆ‘ä¸€æ ·å¯¹å¼€æº AI æµè§ˆæ„Ÿåˆ°å…´å¥‹ï¼Œè¯·ç»™å®ƒä¸€ä¸ª starï¼â­

[ä¸­æ–‡](README_zh.md) | [English](README.md)

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ§  **é«˜çº§ AI è‡ªåŠ¨åŒ–**ï¼šè¶…è¶Šæµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ŒVibeSurf æ‰§è¡Œæ·±åº¦ç ”ç©¶ã€æ™ºèƒ½çˆ¬å–ã€å†…å®¹æ‘˜è¦ç­‰ï¼Œä»¥è¿›è¡Œæ¢ç´¢ã€‚

- ğŸš€ **å¤šä»£ç†å¹¶è¡Œå¤„ç†**ï¼šåœ¨ä¸åŒçš„æµè§ˆå™¨æ ‡ç­¾é¡µä¸­åŒæ—¶è¿è¡Œå¤šä¸ª AI ä»£ç†ï¼Œå®ç°æ·±åº¦ç ”ç©¶å’Œå¹¿æ³›ç ”ç©¶ï¼Œå¤§å¹…æå‡æ•ˆç‡ã€‚

- ğŸ¥· **éšèº«ä¼˜å…ˆæ¶æ„**ï¼šä½¿ç”¨ Chrome DevTools åè®®ï¼ˆCDPï¼‰è€Œä¸æ˜¯ Playwrightï¼Œæä¾›å“è¶Šçš„éšèº«èƒ½åŠ›ï¼Œé˜²æ­¢æœºå™¨äººæ£€æµ‹ã€‚

- ğŸ¨ **æ— ç¼çš„ Chrome æ‰©å±• UI**ï¼šåŸç”Ÿæµè§ˆå™¨é›†æˆï¼Œæ— éœ€åˆ‡æ¢åº”ç”¨ç¨‹åºï¼Œæä¾›ç›´è§‚çš„ç•Œé¢ï¼Œæ„Ÿè§‰å°±åƒæµè§ˆå™¨çš„ä¸€éƒ¨åˆ†ã€‚

- ğŸ”’ **éšç§ä¼˜å…ˆçš„ LLM æ”¯æŒ**ï¼šæ”¯æŒæœ¬åœ° LLMï¼ˆOllama ç­‰ï¼‰å’Œè‡ªå®šä¹‰ LLM APIï¼Œç¡®ä¿åœ¨æ°›å›´å†²æµªæœŸé—´æ‚¨çš„æµè§ˆæ•°æ®ä¿æŒç§å¯†å’Œå®‰å…¨ã€‚

## ğŸ› ï¸ å®‰è£…

ä»…éœ€ä¸‰ä¸ªç®€å•æ­¥éª¤å³å¯å¯åŠ¨å¹¶è¿è¡Œ VibeSurfã€‚æ— éœ€å¤æ‚é…ç½®ã€‚

### 1. å®‰è£… uv
ä»å®˜æ–¹ç½‘ç«™å®‰è£… uv åŒ…ç®¡ç†å™¨

**MacOS/Linux**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows**
```bash
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. è®¾ç½®ç¯å¢ƒ
å®‰è£… VibeSurf

```bash
uv pip install vibesurf -U
```

### 3. å¯åŠ¨ VibeSurf
å¯åŠ¨ VibeSurf æµè§ˆå™¨åŠ©æ‰‹

```bash
uv run vibesurf
```

## ğŸ‘©â€ğŸ’» è´¡çŒ®è€…æŒ‡å—

æƒ³ä¸º VibeSurf åšè´¡çŒ®ï¼Ÿè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è®¾ç½®æ‚¨çš„å¼€å‘ç¯å¢ƒï¼š

### 1. å…‹éš†ä»“åº“
```bash
git clone https://github.com/vibesurf-ai/VibeSurf.git
cd VibeSurf
```

### 2. è®¾ç½®ç¯å¢ƒ
**MacOS/Linux**
```bash
uv venv --python 3.12
source .venv/bin/activate
uv pip install -e .
```

**Windows**
```bash
uv venv --python 3.12
.venv\Scripts\activate
uv pip install -e .
```

### 3. å¼€å§‹è°ƒè¯•
**é€‰é¡¹ 1ï¼šç›´æ¥æœåŠ¡å™¨**
```bash
uvicorn vibe_surf.backend.main:app --host 127.0.0.1 --port 9335
```

**é€‰é¡¹ 2ï¼šCLI å…¥å£**
```bash
uv run vibesurf
```

## ğŸ—ºï¸ è·¯çº¿å›¾

æˆ‘ä»¬æ­£åœ¨æ„å»º VibeSurfï¼Œä½¿å…¶æˆä¸ºæ‚¨ç»ˆæçš„ AI æµè§ˆå™¨ä¼´ä¾£ã€‚ä»¥ä¸‹æ˜¯æ¥ä¸‹æ¥çš„è®¡åˆ’ï¼š

- [x] **æ™ºèƒ½æŠ€èƒ½ç³»ç»Ÿ** - *å·²å®Œæˆ*
  æ·»åŠ  `/search` ç”¨äºå¿«é€Ÿä¿¡æ¯æœç´¢ï¼Œ`/crawl` ç”¨äºè‡ªåŠ¨ç½‘ç«™æ•°æ®æå–ã€‚é›†æˆäº†å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¾®åšå’Œ YouTube çš„åŸç”Ÿ APIã€‚

- [ ] **å¼ºå¤§çš„ç¼–ç ä»£ç†** - *è¿›è¡Œä¸­*
  æ„å»ºä¸€ä¸ªå…¨é¢çš„ç¼–ç åŠ©æ‰‹ï¼Œç”¨äºåœ¨æµè§ˆå™¨ä¸­ç›´æ¥è¿›è¡Œæ•°æ®å¤„ç†å’Œåˆ†æ

- [ ] **æ™ºèƒ½æµè§ˆå™¨å·¥ä½œæµ** - *è®¡åˆ’ä¸­*
  åˆ›å»ºè‡ªå®šä¹‰æ‹–æ‹½å¼å·¥ä½œæµï¼Œç”¨äºè‡ªåŠ¨ç™»å½•ã€æ•°æ®æ”¶é›†å’Œå¤æ‚çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–ä»»åŠ¡

- [ ] **ç¬¬ä¸‰æ–¹é›†æˆ** - *è®¡åˆ’ä¸­*
  ä¸ n8n å·¥ä½œæµå’Œå…¶ä»–å·¥å…·è¿æ¥ï¼Œå°†æµè§ˆä¸è‡ªåŠ¨åŒ–ç»“åˆ

- [ ] **æ™ºèƒ½è®°å¿†ä¸ä¸ªæ€§åŒ–** - *è®¡åˆ’ä¸­*
  å°† VibeSurf è½¬å˜ä¸ºçœŸæ­£çš„äººæ€§åŒ–ä¼´ä¾£ï¼Œå…·å¤‡æŒä¹…è®°å¿†åŠŸèƒ½ï¼Œèƒ½å¤Ÿå­¦ä¹ æ‚¨çš„åå¥½ã€ä¹ æƒ¯å’Œæµè§ˆæ¨¡å¼

## ğŸ¬ æ¼”ç¤º

### å¦‚ä½•ä½¿ç”¨ï¼Ÿ
<video src="https://github.com/user-attachments/assets/0a4650c0-c4ed-423e-9e16-7889e9f9816d" controls="controls">æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒæ’­æ”¾æ­¤è§†é¢‘ï¼</video>

### åœ¨æµè§ˆå™¨ä¸­è¿è¡Œæ•°åä¸ªä»£ç†
<video src="https://github.com/user-attachments/assets/9c461a6e-5d97-4335-ba09-59e8ec4ad47b" controls="controls">æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒæ’­æ”¾æ­¤è§†é¢‘ï¼</video>

## ğŸ“ è®¸å¯è¯

æœ¬ä»“åº“é‡‡ç”¨ [VibeSurf å¼€æºè®¸å¯è¯](./LICENSE)ï¼ŒåŸºäº Apache 2.0 å¹¶é™„åŠ é¢å¤–æ¡æ¬¾ã€‚

## ğŸ‘ è‡´è°¢

VibeSurf å»ºç«‹åœ¨å…¶ä»–ä¼˜ç§€çš„å¼€æºé¡¹ç›®ä¹‹ä¸Šï¼š

- [Browser Use](https://github.com/browser-use/browser-use)
- [LangGraph](https://github.com/langchain-ai/langgraph)

éå¸¸æ„Ÿè°¢ä»–ä»¬çš„åˆ›ä½œè€…å’Œè´¡çŒ®è€…ï¼


================================================
FILE: vibesurf.spec
================================================
# -*- mode: python ; coding: utf-8 -*-

import os
import sys
import platform
from pathlib import Path

# Ensure using current environment's Python
python_path = sys.executable
print(f"Using Python: {python_path}")

block_cipher = None

# Dynamically find vibe_surf installation location and get version
try:
    import vibe_surf
    vibe_surf_path = Path(vibe_surf.__file__).parent
    app_version = vibe_surf.__version__
    print(f"VibeSurf package location: {vibe_surf_path}")
    print(f"VibeSurf version: {app_version}")
    cli_path = vibe_surf_path / 'cli.py'
except ImportError:
    # Fallback to relative path (development environment)
    vibe_surf_path = Path.cwd() / 'vibe_surf'
    cli_path = vibe_surf_path / 'cli.py'
    
    # Try to get version from _version.py or fallback
    try:
        version_file = vibe_surf_path / '_version.py'
        if version_file.exists():
            version_locals = {}
            exec(version_file.read_text(), version_locals)
            app_version = version_locals.get('version', '0.0.0+dev')
        else:
            app_version = '0.0.0+dev'
    except Exception as e:
        app_version = '0.0.0+dev'
        print(f"WARNING: Could not determine version: {e}")
    
    print(f"Using development path: {vibe_surf_path}")
    print(f"Using fallback version: {app_version}")

# Dynamically find browser_use installation location for prompt templates
try:
    import browser_use
    browser_use_path = Path(browser_use.__file__).parent
    print(f"browser_use package location: {browser_use_path}")
except ImportError:
    print("WARNING: browser_use not found, system prompts may not work in executable")
    browser_use_path = None

# Check if CLI file exists
if not cli_path.exists():
    print(f"ERROR: CLI file not found at {cli_path}")
    print("Please ensure vibe_surf is properly installed or run from project directory")
    sys.exit(1)

# Platform detection and configuration
current_platform = platform.system()
print(f"Building for platform: {current_platform}")

# Configure icon and console mode based on platform
if current_platform == "Windows":
    # Windows can use ICO or PNG, but prefer ICO if available
    ico_file = vibe_surf_path / 'chrome_extension' / 'icons' / 'logo.ico'
    if ico_file.exists():
        icon_file = ico_file
        print(f"Windows detected - using ICO icon")
    else:
        icon_file = vibe_surf_path / 'chrome_extension' / 'icons' / 'logo.png'
        print(f"Windows detected - using PNG icon (ICO not found)")
    console_mode = True
else:  # Other platforms not supported
    print(f"ERROR: Platform {current_platform} is not supported")
    print("VibeSurf currently supports Windows only")
    sys.exit(1)

# Verify icon file exists
if not icon_file.exists():
    print(f"WARNING: Icon file not found at {icon_file}")
    icon_file = None
else:
    print(f"Using icon: {icon_file}")

# Data files collection - include all necessary static files
datas = [
    (str(vibe_surf_path / 'chrome_extension'), 'vibe_surf/chrome_extension'),
    (str(vibe_surf_path / 'backend'), 'vibe_surf/backend'),
]

# Add browser_use prompt template files if available
if browser_use_path:
    browser_use_agent_path = browser_use_path / 'agent'
    # Include the markdown system prompt files
    prompt_files = [
        'system_prompt.md',
        'system_prompt_no_thinking.md',
        'system_prompt_flash.md'
    ]
    for prompt_file in prompt_files:
        prompt_file_path = browser_use_agent_path / prompt_file
        if prompt_file_path.exists():
            datas.append((str(prompt_file_path), f'browser_use/agent'))
            print(f"Added browser_use prompt file: {prompt_file}")
        else:
            print(f"WARNING: browser_use prompt file not found: {prompt_file_path}")
    
    # Include JavaScript files for DOM operations
    browser_use_dom_path = browser_use_path / 'dom'
    if browser_use_dom_path.exists():
        datas.append((str(browser_use_dom_path), 'browser_use/dom'))
        print(f"Added browser_use DOM directory: {browser_use_dom_path}")

# Hidden imports - all dynamic imports that PyInstaller might miss
hiddenimports = [
    # Core web framework
    'uvicorn.main',
    'uvicorn.config', 
    'uvicorn.server',
    'uvicorn.protocols.http.auto',
    'uvicorn.protocols.websockets.auto',
    'uvicorn.lifespan.on',
    'fastapi.applications',
    'fastapi.routing',
    'fastapi.middleware',
    'fastapi.middleware.cors',
    
    # Browser automation
    'browser_use',
    'cdp_use',
    
    # HTTP and networking
    'aiohttp.web',
    'aiohttp.client',
    'websockets.server',
    'websockets.client',
    
    # Database
    'sqlalchemy.dialects.sqlite',
    'sqlalchemy.pool',
    'aiosqlite',
    
    # Core dependencies
    'rich.console',
    'rich.panel',
    'rich.prompt',
    'rich.text',
    'pydantic',
    'pydantic.main',
    'pydantic.fields',
    'langgraph',
    'langgraph.graph',
    
    # Image processing
    'scikit-image',
    'skimage.io',
    'skimage.transform',
    
    # JSON and data processing
    'json_repair',
    'uuid7',
    'psutil',
    'aiofiles',
    'anyio',
    'python_socks',
    'python_multipart',
    'python_dotenv',
    'greenlet',
    'getmac',
]

# Analysis configuration
a = Analysis(
    [str(cli_path)],
    pathex=[str(vibe_surf_path.parent)],
    binaries=[],
    datas=datas,
    hiddenimports=hiddenimports,
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[
        'matplotlib',  # Large plotting library not needed
        'tkinter',     # GUI library not needed
        'PyQt5',       # GUI library not needed
        'PyQt6',       # GUI library not needed
        'PySide2',     # GUI library not needed
        'PySide6',     # GUI library not needed
        'jupyter',     # Jupyter notebook not needed
        'IPython',     # Interactive Python not needed
    ],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)

# Remove duplicate files to reduce size
pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

# Create executable - use onefile mode for all platforms (CLI apps work better this way)
exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    [],
    name='vibesurf',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,  # Compress to reduce file size
    upx_exclude=[],
    runtime_tmpdir=None,
    console=console_mode,
    disable_windowed_traceback=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
    icon=str(icon_file) if icon_file else None,  # Set icon for all platforms
)
print(f"Using onefile mode for {current_platform} (CLI application)")
print(f"Console mode: {console_mode}")
print(f"Executable icon: {icon_file}")


================================================
FILE: .env.example
================================================
OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_API_KEY=

ANTHROPIC_ENDPOINT=https://api.anthropic.com
ANTHROPIC_API_KEY=

GOOGLE_API_KEY=

AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=2025-01-01-preview

DEEPSEEK_ENDPOINT=https://api.deepseek.com
DEEPSEEK_API_KEY=

MISTRAL_API_KEY=
MISTRAL_ENDPOINT=https://api.mistral.ai/v1

OLLAMA_ENDPOINT=http://localhost:11434

DASHSCOPE_ENDPOINT=https://dashscope.aliyuncs.com/compatible-mode/v1
DASHSCOPE_API_KEY=

MOONSHOT_ENDPOINT=https://api.moonshot.cn/v1
MOONSHOT_API_KEY=

UNBOUND_ENDPOINT=https://api.getunbound.ai
UNBOUND_API_KEY=

SiliconFLOW_ENDPOINT=https://api.siliconflow.cn/v1/
SiliconFLOW_API_KEY=

# Set to false to disable anonymized telemetry
ANONYMIZED_TELEMETRY=false

# LogLevel: Set to debug to enable verbose logging, set to result to get results only. Available: result | debug | info
BROWSER_USE_LOGGING_LEVEL=info

# Calculate costs: (beta) Add cost calculations to tokens. Available: true | false
BROWSER_USE_CALCULATE_COST=false

# set this to true to optimize browser-use's chrome for running inside docker
IN_DOCKER=false

VIBESURF_BACKEND_PORT=
VIBESURF_EXTENSION=
VIBESURF_WORKSPACE=
VIBESURF_DATABASE_URL=
VIBESURF_DEBUG=

BROWSER_EXECUTION_PATH=
BROWSER_USER_DATA=


================================================
FILE: .python-version
================================================
3.12



================================================
FILE: docs/EXECUTABLE_BUILD.md
================================================
# VibeSurf Executable Build Guide

This guide explains how to build standalone executable files for VibeSurf that can run without requiring Python installation.

## ğŸ¯ Overview

VibeSurf can be packaged into standalone executables for:
- **Windows**: `vibesurf-windows-x64.exe`
- **macOS Intel**: `vibesurf-macos-intel-x64` (compatible with Apple Silicon via Rosetta 2)
- **macOS Apple Silicon**: `vibesurf-macos-apple-silicon` (native arm64 performance)
- **Linux**: `vibesurf-linux-x64`

## ğŸ”§ Local Build Instructions

### Prerequisites

1. **Install uv** (Python package manager):
   ```bash
   # On macOS and Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh
   
   # On Windows (PowerShell)
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

2. **Clone the repository**:
   ```bash
   git clone https://github.com/your-repo/VibeSurf.git
   cd VibeSurf
   ```

### Building on Linux/macOS

1. **Run the build script**:
   ```bash
   chmod +x build-local.sh
   ./build-local.sh
   ```

2. **Find your executable**:
   - Location: `./dist/vibesurf`
   - Run with: `./dist/vibesurf`

**Note**:
- The script creates a dedicated `.build-env` directory for building, preserving your existing `.venv`
- Uses your current local code (`-e .`), not the PyPI published version

### Building on Windows

1. **Run the build script**:
   ```cmd
   build-local.bat
   ```

2. **Find your executable**:
   - Location: `.\dist\vibesurf.exe`
   - Run with: `.\dist\vibesurf.exe`

**Note**:
- The script creates a dedicated `.build-env` directory for building, preserving your existing `.venv`
- Uses your current local code (`-e .`), not the PyPI published version

## ğŸ§ª Testing the Executable

### Basic Test
```bash
# Linux/macOS
./dist/vibesurf --help

# Windows
.\dist\vibesurf.exe --help
```

### Full Test
```bash
# Linux/macOS
./dist/vibesurf

# Windows
.\dist\vibesurf.exe
```

The executable should:
1. Display the VibeSurf logo
2. Show version information
3. Detect browsers automatically
4. Configure ports and start the backend

## ğŸ“¦ What's Included

The executable contains:
- **Complete Python runtime** (Python 3.12)
- **All VibeSurf dependencies** (FastAPI, uvicorn, browser-use, etc.)
- **Chrome extension files** (bundled within the executable)
- **Backend API and database components**
- **VibeSurf logo as executable icon** (from `vibe_surf/chrome_extension/icons/logo.png`)

## ğŸ” Build Process Details

### Environment Setup

**Local Building:**
- Creates isolated `uv` build environment (`.build-env`) with Python 3.12
- Preserves existing development environment (`.venv`)
- Installs local VibeSurf in development mode (`-e .`) for latest code
- Adds PyInstaller for building

**GitHub Actions Building:**
- Uses matrix strategy with native OS runners (Windows, macOS, Linux)
- Each runner builds the executable for its native platform
- Installs current repository code (`-e .`) at release time
- Ensures executables match the exact released code version

### PyInstaller Configuration
- **Entry point**: `vibe_surf/cli.py`
- **Data files**: Chrome extension, backend templates
- **Hidden imports**: All dynamic imports declared
- **Exclusions**: Removes unnecessary packages (matplotlib, tkinter)
- **Compression**: Uses UPX for smaller file size
- **Icon**: VibeSurf logo embedded as executable icon

### Output
- **File size**: ~100-200MB (varies by platform)
- **Startup time**: 2-5 seconds (first run)
- **Dependencies**: None (fully self-contained)

## ğŸš€ Distribution

### Manual Distribution
1. Build the executable for your target platform
2. Copy the `dist/vibesurf*` file to target machines
3. Users can run directly without any installation

### GitHub Releases (Automated)
The repository includes GitHub Actions workflows that automatically:
1. **Multi-platform building**: Uses GitHub runners for each target platform
   - `windows-latest` â†’ `vibesurf-windows-x64.exe`
   - `macos-13` (Intel) â†’ `vibesurf-macos-intel-x64`
   - `macos-14` (Apple Silicon) â†’ `vibesurf-macos-apple-silicon`
   - `ubuntu-latest` â†’ `vibesurf-linux-x64`
2. **Current code**: Builds from the repository code at release time (`-e .`)
3. **Upload binaries**: Automatically uploads all platform executables to GitHub Releases
4. **User downloads**: Platform-specific executables ready for distribution

**macOS Compatibility:**
- **Intel build**: Works on Intel Macs natively, Apple Silicon Macs via Rosetta 2
- **Apple Silicon build**: Native arm64 performance on Apple Silicon Macs
- **Recommendation**: Use Apple Silicon build for M1/M2/M3 Macs for best performance

**Why different OS runners?**
PyInstaller creates native executables and requires the target operating system to build properly. Cross-compilation is not supported.

## ğŸ› Troubleshooting

### Build Issues

**"uv not found"**
- Install uv following the prerequisites

**"vibesurf.spec not found"**
- Ensure you're in the project root directory

**"Import errors during build"**
- Check that all dependencies are correctly installed
- Verify the `hiddenimports` list in `vibesurf.spec`

### Runtime Issues

**"Extension not found"**
- The executable includes bundled extensions
- Check the console output for path information

**"Port already in use"**
- VibeSurf automatically finds available ports
- Check for other running instances

**"Browser not detected"**
- The executable includes the same browser detection as the regular installation
- Manually specify browser path if needed

## ğŸ“Š Performance Comparison

| Aspect | Regular Install | Executable |
|--------|----------------|------------|
| Installation | Requires Python + pip/uv | Download & run |
| Startup Time | ~1 second | ~3 seconds |
| File Size | ~50MB (dependencies) | ~150MB (self-contained) |
| Updates | `uv pip install -U` | Download new executable |
| Portability | Requires Python | Fully portable |

## ğŸ”„ Updates

To update the executable:
1. Download the latest release executable, or
2. Rebuild locally with the latest code

The executable version matches the PyPI package version.


================================================
FILE: docs/PYPI_SETUP.md
================================================
# PyPI Automated Publishing Setup Guide

This document will guide you through setting up the automated PyPI publishing workflow for VibeSurf.

## ğŸ”‘ Step 1: Get PyPI API Token

1. Log in to your [PyPI account](https://pypi.org/)
2. Go to Account Settings â†’ API tokens
3. Click "Add API token"
4. Select "Entire account" or create a token for a specific project
5. Copy the generated token (format: `pypi-...`)

## ğŸ”§ Step 2: Configure GitHub Secrets

In your GitHub repository:

1. Go to Settings â†’ Secrets and variables â†’ Actions
2. Click "New repository secret"
3. Create a secret named `PYPI_API_TOKEN`
4. Paste your PyPI API token into the value field

## ğŸš€ Step 3: Create Release to Trigger Automated Publishing

### Method 1: Via GitHub Web Interface

1. On your GitHub repository page, click "Releases" on the right side
2. Click "Create a new release"
3. Create a new tag (e.g., `v0.1.0`)
4. Fill in the Release title and description
5. Click "Publish release"

### Method 2: Via Command Line

```bash
# Make sure you're on the main/master branch
git checkout main
git pull origin main

# Create and push tag
git tag v0.1.0
git push origin v0.1.0

# Or use annotated tag
git tag -a v0.1.0 -m "Release version 0.1.0"
git push origin v0.1.0
```

Then create a release from this tag on GitHub.

## ğŸ“¦ Version Management

This project uses `setuptools-scm` for dynamic version management:

- Version numbers are automatically generated from Git tags
- Development versions automatically get `+dev` suffix
- No need to manually update version numbers

### Version Number Convention

Recommended to use semantic versioning:
- `v1.0.0` - Major version
- `v1.1.0` - Feature update
- `v1.1.1` - Bug fix
- `v1.2.0-alpha.1` - Pre-release version

## ğŸ”„ Automated Workflow Explanation

When you create a release, GitHub Actions will automatically:

1. **Test Phase**:
   - Run tests on Python 3.11 and 3.12
   - Verify package import and CLI functionality

2. **Build Phase**:
   - Use `setuptools-scm` to get version from git tag
   - Build wheel and source distribution
   - Validate the built package

3. **Publish Phase**:
   - Automatically upload to PyPI
   - Use your configured API token

## ğŸ› ï¸ Local Build Testing

Before creating a release, you can test the build locally:

```bash
# Install build dependencies
pip install build setuptools-scm[toml]

# Build package
python -m build

# Check build results
pip install twine
twine check dist/*

# Local install test
pip install dist/*.whl
vibesurf  # Test CLI command
```

## ğŸ” Troubleshooting

### Common Issues

1. **Build Failure**:
   - Check `pyproject.toml` configuration
   - Ensure all dependencies are correctly listed
   - Verify `MANIFEST.in` includes necessary files

2. **Version Number Issues**:
   - Ensure git tag exists
   - Check `setuptools-scm` configuration
   - Verify `_version.py` file generation

3. **PyPI Upload Failure**:
   - Verify API token is correct
   - Check if package name already exists
   - Ensure version number is unique

### Manual Publishing (Emergency)

If automated workflow fails, you can publish manually:

```bash
# Build package
python -m build

# Upload to PyPI
twine upload dist/*
```

## ğŸ“‹ Pre-Release Checklist

Before publishing, confirm:

- [ ] Code merged to main branch
- [ ] All tests pass
- [ ] README and documentation updated
- [ ] PyPI API token configured
- [ ] Git tag version number is correct
- [ ] Release notes prepared

## ğŸ”„ Continuous Improvement

Regularly recommended:

- Update dependency versions
- Check for security vulnerabilities
- Optimize build process
- Monitor download statistics

## ğŸ“ Getting Help

If you encounter issues:

1. Check GitHub Actions logs
2. Review PyPI project page
3. Reference [PyPI official documentation](https://packaging.python.org/)
4. Check [setuptools-scm documentation](https://setuptools-scm.readthedocs.io/)

## ğŸ’¡ Additional Notes

### Dynamic Versioning Explained

The `_version.py` file is automatically generated by `setuptools-scm`:

- **When generated**: During `python -m build`, `pip install .`, or CI/CD builds
- **Content example**:
  ```python
  # coding: utf-8
  # file generated by setuptools_scm
  # don't change, don't track in version control
  __version__ = version = '0.1.0'
  __version_tuple__ = version_tuple = (0, 1, 0)
  ```
- **Version rules**:
  - With git tag `v0.1.0` â†’ version `0.1.0`
  - After tag with new commits â†’ version `0.1.0.dev1+g1234567`
  - No tags â†’ version `0.0.0+unknown`

### Development vs Production

**Development Environment**:
- `_version.py` may not exist (if never built)
- Uses fallback version `"0.0.0+dev"`
- Can manually generate: `python -m setuptools_scm`

**Production Environment**:
- `_version.py` automatically generated during packaging
- Contains correct version information
- Users get proper version after installation

**Important**: Don't add `_version.py` to version control. Add to `.gitignore`:
```
vibe_surf/_version.py


================================================
FILE: scripts/build-local.bat
================================================
@echo off
:: Local build script for Windows
:: This script creates a uv environment and builds a standalone executable

setlocal enabledelayedexpansion

echo ğŸš€ VibeSurf Local Build Script for Windows
echo ==========================================

:: Check if uv is installed
where uv >nul 2>nul
if %ERRORLEVEL% neq 0 (
    echo [ERROR] uv is not installed. Please install it first:
    echo powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
    exit /b 1
)

echo [SUCCESS] uv is installed

:: Use dedicated build environment directory
set BUILD_ENV=.build-env

:: Clean up existing build environment if it exists
if exist "%BUILD_ENV%" (
    echo [WARNING] Removing existing build environment directory
    rmdir /s /q "%BUILD_ENV%"
)

:: Step 1: Create dedicated build environment
echo [INFO] Creating dedicated build environment with Python 3.12...
uv venv %BUILD_ENV% --python 3.12
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to create build environment
    exit /b 1
)

:: Step 2: Activate build environment and install dependencies
echo [INFO] Activating build environment and installing dependencies...
call %BUILD_ENV%\Scripts\activate.bat

:: Verify Python version
python --version
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to activate Python environment
    exit /b 1
)

:: Install local VibeSurf and PyInstaller
echo [INFO] Installing local vibesurf in development mode and pyinstaller...
uv pip install -e .
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to install local vibesurf
    exit /b 1
)

uv pip install pyinstaller
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to install pyinstaller
    exit /b 1
)

:: Verify installation
echo [INFO] Verifying installation...
python -c "import vibe_surf; print(f'VibeSurf version: {vibe_surf.__version__}')"
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to import vibe_surf
    exit /b 1
)

python -c "from vibe_surf.cli import main; print('CLI import successful')"
if %ERRORLEVEL% neq 0 (
    echo [ERROR] Failed to import CLI
    exit /b 1
)

:: Step 3: Build executable
echo [INFO] Building executable with PyInstaller...
if not exist "vibesurf.spec" (
    echo [ERROR] vibesurf.spec file not found!
    exit /b 1
)

pyinstaller vibesurf.spec --clean --noconfirm
if %ERRORLEVEL% neq 0 (
    echo [ERROR] PyInstaller build failed
    exit /b 1
)

:: Step 4: Test executable
if exist "dist\vibesurf.exe" (
    echo [SUCCESS] Executable built successfully!
    
    echo [INFO] Testing executable...
    dist\vibesurf.exe --help >nul 2>&1
    if %ERRORLEVEL% equ 0 (
        echo [SUCCESS] Executable test passed!
    ) else (
        echo [WARNING] Executable test failed, but this might be expected for CLI apps
    )
    
    :: Show file info
    echo.
    echo ğŸ“Š Executable Information:
    echo =========================
    dir /q dist\vibesurf.exe
    
    echo.
    echo [SUCCESS] ğŸ‰ Build completed successfully!
    echo.
    echo ğŸ“ Your executable is located at: .\dist\vibesurf.exe
    echo ğŸš€ To run: .\dist\vibesurf.exe
    echo.
    
) else (
    echo [ERROR] Build failed - executable not found
    exit /b 1
)

echo Press any key to exit...
pause >nul


================================================
FILE: scripts/build-local.sh
================================================
#!/bin/bash
# Local build script for Unix-like systems (Linux/macOS)
# This script creates a uv environment and builds a standalone executable

set -e  # Exit on any error

echo "ğŸš€ VibeSurf Local Build Script"
echo "=============================="

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${CYAN}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if uv is installed
if ! command -v uv &> /dev/null; then
    print_error "uv is not installed. Please install it first:"
    echo "curl -LsSf https://astral.sh/uv/install.sh | sh"
    exit 1
fi

print_success "uv is installed"

# Use dedicated build environment directory
BUILD_ENV=".build-env"

# Clean up existing build environment if it exists
if [ -d "$BUILD_ENV" ]; then
    print_warning "Removing existing build environment directory"
    rm -rf "$BUILD_ENV"
fi

# Step 1: Create dedicated build environment
print_status "Creating dedicated build environment with Python 3.12..."
uv venv "$BUILD_ENV" --python 3.12

# Step 2: Activate build environment and install dependencies
print_status "Activating build environment and installing dependencies..."
source "$BUILD_ENV/bin/activate"

# Verify Python version
PYTHON_VERSION=$(python --version)
print_success "Using Python: $PYTHON_VERSION"

# Install local VibeSurf and PyInstaller
print_status "Installing local vibesurf in development mode and pyinstaller..."
uv pip install -e .
uv pip install pyinstaller

# Verify installation
print_status "Verifying installation..."
python -c "import vibe_surf; print(f'VibeSurf version: {vibe_surf.__version__}')" || {
    print_error "Failed to import vibe_surf"
    exit 1
}

python -c "from vibe_surf.cli import main; print('CLI import successful')" || {
    print_error "Failed to import CLI"
    exit 1
}

# Step 3: Build executable
print_status "Building executable with PyInstaller..."
if [ ! -f "vibesurf.spec" ]; then
    print_error "vibesurf.spec file not found!"
    exit 1
fi

pyinstaller vibesurf.spec --clean --noconfirm

# Step 4: Check build results and handle platform-specific post-processing
PLATFORM=$(uname -s)
print_status "Detected platform: $PLATFORM"

if [ "$PLATFORM" = "Darwin" ]; then
    # macOS - check for single executable (CLI application with icon)
    if [ -f "dist/vibesurf" ]; then
        print_success "macOS CLI executable with icon built successfully!"
        
        # Make executable and apply code signing
        chmod +x dist/vibesurf
        
        print_status "Applying ad-hoc code signature..."
        codesign --force --sign - dist/vibesurf || {
            print_warning "Code signing failed, but executable should still work"
        }
        
        # Remove quarantine attribute
        xattr -c dist/vibesurf 2>/dev/null || {
            print_warning "No quarantine attributes to remove"
        }
        
        # Verify signing
        print_status "Verifying code signature..."
        codesign --verify dist/vibesurf && {
            print_success "âœ… Signature verified"
        } || {
            print_warning "âš ï¸ Signature verification failed"
        }
        
        print_status "Testing executable..."
        ./dist/vibesurf --help > /dev/null 2>&1 && {
            print_success "Executable test passed!"
        } || {
            print_warning "Executable test failed, but this might be expected for CLI apps"
        }
        
        # Show file info
        echo ""
        echo "ğŸ“Š Executable Information:"
        echo "========================="
        ls -lh dist/vibesurf
        
        if command -v file &> /dev/null; then
            file dist/vibesurf
        fi
        
        echo ""
        print_success "ğŸ‰ Build completed successfully!"
        echo ""
        echo "ğŸ“ Your executable is located at: ./dist/vibesurf"
        echo "ğŸš€ To run:"
        echo "   â€¢ Double-click vibesurf (opens in Terminal with console interface)"
        echo "   â€¢ Or run: ./dist/vibesurf"
        echo "ğŸ’¡ This executable has an icon and will open console interface when double-clicked"
        echo ""
        
    else
        print_error "Build failed - vibesurf executable not found"
        exit 1
    fi
    
else
    # Other Unix-like systems are not supported
    print_error "This build script currently only supports macOS."
    print_error "For other platforms, please use the GitHub Actions workflow."
    exit 1
fi


================================================
FILE: tests/test_agents.py
================================================
import os
import asyncio
import pprint

from dotenv import load_dotenv
import sys
import pdb

sys.path.append(".")
load_dotenv()

from browser_use.browser.session import BrowserSession, BrowserProfile
from vibe_surf.browser.browser_manager import BrowserManager
from vibe_surf.browser.agent_browser_session import AgentBrowserSession

from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.browser.agen_browser_profile import AgentBrowserProfile
from vibe_surf.tools.browser_use_tools import BrowserUseTools
from vibe_surf.tools.vibesurf_tools import VibeSurfTools
from vibe_surf.llm.openai_compatible import ChatOpenAICompatible
from browser_use.llm.deepseek.chat import ChatDeepSeek
from vibe_surf.agents.browser_use_agent import BrowserUseAgent
from vibe_surf.agents.vibe_surf_agent import VibeSurfAgent


async def run_single_bu_agent():
    import platform
    if platform.system() != "Darwin":
        browser_exec_path = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
    else:
        browser_exec_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    browser_profile = AgentBrowserProfile(
        executable_path=browser_exec_path,
        user_data_dir=os.path.abspath('./tmp/chrome/profiles/default'),
        headless=False,
        keep_alive=True
    )
    # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
    main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
    await main_browser_session.start()
    bu_tools = BrowserUseTools()

    # llm = ChatOpenAICompatible(model='gemini-2.5-flash',
    #                            base_url=os.getenv("OPENAI_ENDPOINT"),
    #                            api_key=os.getenv("OPENAI_API_KEY"))

    # llm = ChatOpenAICompatible(model='qwen-plus',
    #                            base_url=os.getenv("ALIBABA_ENDPOINT"),
    #                            api_key=os.getenv("ALIBABA_API_KEY"))

    # llm = ChatOpenAICompatible(model='kimi-k2-turbo-preview',
    #                            base_url=os.getenv("MOONSHOT_ENDPOINT"),
    #                            api_key=os.getenv("MOONSHOT_API_KEY"))

    llm = ChatOpenAICompatible(model='deepseek-reasoner',
                               base_url=os.getenv("DEEPSEEK_ENDPOINT"),
                               api_key=os.getenv("DEEPSEEK_API_KEY"))

    task = "Search Google for 'Elon Mask' and tell me the top 3 results"

    # task = r"""
    # 1. åœ¨æ–°çš„tab å¯¼èˆªåˆ° https://github.com/
    # 2. åœ¨æ–°çš„tab å¯¼èˆªåˆ° https://vibemotion.co/
    # 3. åœ¨æ–°çš„tab å¯¼èˆªåˆ° https://browser-use.com/
    # 4. åˆ†åˆ«æ€»ç»“æ‰€æœ‰tabçš„å†…å®¹(åœ¨ä¸€æ­¥ä¸­ä½¿ç”¨parallelçš„extractæ“ä½œ, ä¸è¦åˆ†å¼€ä¸‰æ­¥)ï¼Œç„¶åä¿å­˜åˆ° tabs_summary.txt
    # """
    agent = BrowserUseAgent(task=task,
                            llm=llm,
                            browser_session=main_browser_session,
                            tools=bu_tools,
                            task_id=main_browser_session.id,
                            file_system_path="./tmp/single_bu_tests")
    history = await agent.run()
    print(history.final_result())
    await main_browser_session.kill()


async def run_multi_bu_agents():
    import platform
    if platform.system() != "Darwin":
        browser_exec_path = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
    else:
        browser_exec_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    browser_profile = AgentBrowserProfile(
        executable_path=browser_exec_path,
        user_data_dir=os.path.abspath('./tmp/chrome/profiles/default'),
        headless=False,
        keep_alive=True
    )
    mcp_server_config = {
        "mcpServers": {
            "filesystem": {
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-filesystem",
                    "E:\\AIBrowser\\VibeSurf\\tmp\\code",
                ]
            },
        }
    }
    # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
    # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
    main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
    await main_browser_session.start()
    bu_tools = BrowserUseTools()
    browser_manager = BrowserManager(main_browser_session=main_browser_session)
    # await tools.register_mcp_clients(mcp_server_config)

    llm = ChatOpenAICompatible(model='gemini-2.5-flash',
                               base_url=os.getenv("OPENAI_ENDPOINT"),
                               api_key=os.getenv("OPENAI_API_KEY"))
    agent_browser_sessions = await asyncio.gather(
        browser_manager.register_agent("agent-1"),
        browser_manager.register_agent("agent-2"),
        browser_manager.register_agent("agent-3")
    )
    agents = [
        BrowserUseAgent(task=task,
                        llm=llm,
                        browser_session=agent_browser_sessions[i],
                        tools=bu_tools,
                        file_system_path="./tmp/multi_bu_tests")
        for i, task in enumerate([
            # 'Search Google for weather in Tokyo',
            # 'Check Reddit front page title',
            # 'Look up Bitcoin price on Coinbase',
            # 'Find NASA image of the day',
            # 'Check top story on CNN',
            # 'Search latest SpaceX launch date',
            # 'Look up population of Paris',
            # 'Find current time in Sydney',
            # 'Check who won last Super Bowl',
            # 'Search trending topics on Twitter',
            'search browser-use and click into the most relevant url and scroll down one page',
            'search langflow and click into the most relevant url and scroll down one page',
            'search langgraph and click into the most relevant url and scroll down one page',
        ])
    ]

    results = await asyncio.gather(*[agent.run() for agent in agents])
    for i, ret in enumerate(results):
        print(await agent_browser_sessions[i].get_tabs())
        print(ret.final_result())
    await browser_manager.close()
    await main_browser_session.kill()
    await bu_tools.unregister_mcp_clients()


async def test_vibe_surf_agent():
    """Test VibeSurfAgent with both simple and browser tasks"""
    import platform
    if platform.system() != "Darwin":
        browser_exec_path = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
    else:
        browser_exec_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    browser_profile = AgentBrowserProfile(
        executable_path=browser_exec_path,
        user_data_dir=os.path.abspath('./tmp/chrome/profiles/default'),
        headless=False,
        keep_alive=True
    )
    # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
    main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
    await main_browser_session.start()
    vs_tools = VibeSurfTools()
    browser_manager = BrowserManager(main_browser_session=main_browser_session)
    # llm = ChatOpenAICompatible(model='gemini-2.5-flash',
    #                            base_url=os.getenv("OPENAI_ENDPOINT"),
    #                            api_key=os.getenv("OPENAI_API_KEY"))

    llm = ChatOpenAICompatible(model='deepseek-chat',
                               base_url=os.getenv("DEEPSEEK_ENDPOINT"),
                               api_key=os.getenv("DEEPSEEK_API_KEY"))

    # Create VibeSurfAgent
    agent = VibeSurfAgent(
        llm=llm,
        browser_manager=browser_manager,
        tools=vs_tools,
        workspace_dir=os.path.abspath("./tmp/vibesurf_tests")
    )

    try:
        # Test 1: Simple task (should not require browser)
        print("ğŸ§ª Testing simple task...")
        simple_task = "What is 2 + 2? Explain the basic concept."
        result1 = await agent.run(simple_task)
        print(f"âœ… Simple task result: {result1}...")
        assert result1 is not None and len(result1) > 0
        #
        # # Test 2: Simple task with upload files
        # print("ğŸ§ª Testing simple task with upload files...")
        # upload_files = [r"E:\AIBrowser\VibeSurf\tmp\code\test.py"]
        # simple_task_with_files = "What files were uploaded? Summarize their purpose."
        # result2 = await agent.run(simple_task_with_files, upload_files=upload_files)
        # print(f"âœ… Simple task with files result: {result2}...")
        # assert result2 is not None and len(result2) > 0
        #
        # # Test 3: Browser task
        # print("ğŸ§ª Testing browser task...")
        # browser_task = "Search Google for 'LangGraph framework' and get basic information"
        # result3 = await agent.run(browser_task)
        # print(f"âœ… Browser task result:")
        # pprint.pprint(result3)
        # assert result3 is not None and len(result3) > 0
        # pdb.set_trace()
        # print("ğŸ‰ All VibeSurfAgent tests passed!")

        # Test 4: Browser parallel task
        # print("ğŸ§ª Testing browser parallel tasks...")
        # browser_task = "Search for Dify, n8n, browser-use and click into their own homepage, take screenshot and save"
        # result4 = await agent.run(browser_task)
        # print(f"âœ… Browser task result:")
        # pprint.pprint(result4)
        # with open("./tmp/vibesurf_tests/parallel_test.md", "w", encoding='utf-8') as fw:
        #     fw.write(result4)
        # assert result4 is not None and len(result4) > 0
        print("ğŸ‰ All VibeSurfAgent tests passed!")

    except Exception as e:
        print(f"âŒ VibeSurfAgent test failed: {e}")
        raise e
    finally:
        # Cleanup
        await browser_manager.close()
        await main_browser_session.kill()


async def test_vibe_surf_agent_control():
    """Test VibeSurfAgent control functionality (pause/resume/stop)"""
    import platform
    if platform.system() != "Darwin":
        browser_exec_path = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
    else:
        browser_exec_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    browser_profile = AgentBrowserProfile(
        executable_path=browser_exec_path,
        user_data_dir=os.path.abspath('./tmp/chrome/profiles/default'),
        headless=False,
        keep_alive=True
    )
    # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
    main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
    await main_browser_session.start()
    vs_tools = VibeSurfTools()
    browser_manager = BrowserManager(main_browser_session=main_browser_session)
    llm = ChatOpenAICompatible(model='gemini-2.5-pro',
                               base_url=os.getenv("OPENAI_ENDPOINT"),
                               api_key=os.getenv("OPENAI_API_KEY"))

    # Create VibeSurfAgent
    agent = VibeSurfAgent(
        llm=llm,
        browser_manager=browser_manager,
        tools=vs_tools,
        workspace_dir=os.path.abspath("./tmp/vibesurf_tests"),
        calculate_token_cost=True
    )

    try:
        print("ğŸ§ª Testing VibeSurfAgent control functionality...")

        # Test 1: Status check when idle
        print("ğŸ“Š Testing initial status...")
        status = agent.get_status()
        print(f"Initial status: {status.overall_status}")
        assert status.overall_status == "idle"

        # Test 2: Start a long-running browser task
        print("ğŸš€ Starting long-running browser task...")
        browser_task = "Search for Dify, n8n, langflow and gather relative information, and generate a detailed report for comparison"

        # Start task in background
        async def run_task():
            return await agent.run(browser_task)

        task_coroutine = asyncio.create_task(run_task())

        # Wait a bit for task to start
        await asyncio.sleep(10)

        # Test 3: Check status during execution
        print("ğŸ“Š Checking status during execution...")
        status = agent.get_status()
        print(f"Running status: {status.overall_status}")
        print(f"Progress: {status.progress}")
        print(f"Active agents: {len(status.agent_statuses)}")

        # Test 4: Pause execution
        print("â¸ï¸ Testing pause functionality...")
        pause_result = await agent.pause("Testing pause functionality")
        print(f"Pause result: {pause_result.success} - {pause_result.message}")
        assert pause_result.success

        # Check status after pause
        await asyncio.sleep(1)
        status = agent.get_status()
        print(f"Paused status: {status.overall_status}")
        assert status.overall_status == "paused"

        # Test 5: Resume execution
        print("â–¶ï¸ Testing resume functionality...")
        resume_result = await agent.resume("Testing resume functionality")
        print(f"Resume result: {resume_result.success} - {resume_result.message}")
        assert resume_result.success

        # Check status after resume
        await asyncio.sleep(1)
        status = agent.get_status()
        print(f"Resumed status: {status.overall_status}")

        # Let it run a bit more
        await asyncio.sleep(50)

        # Test 6: Stop execution
        print("ğŸ›‘ Testing stop functionality...")
        stop_result = await agent.stop("Testing stop functionality")
        print(f"Stop result: {stop_result.success} - {stop_result.message}")

        # Check status after stop (should be stopped even if stop had issues)
        await asyncio.sleep(1)
        status = agent.get_status()
        print(f"Stopped status: {status.overall_status}")

        # Wait for task to complete (it should be cancelled)
        try:
            result = await asyncio.wait_for(task_coroutine, timeout=3)
            print(f"Task result after stop: {result[:100]}...")
        except asyncio.TimeoutError:
            print("âœ… Task properly cancelled after stop (timeout)")
            task_coroutine.cancel()
            try:
                await task_coroutine
            except asyncio.CancelledError:
                pass
        except asyncio.CancelledError:
            print("âœ… Task was cancelled as expected")

        # Verify stop worked (may have timed out but should still be effective)
        if stop_result.success:
            assert status.overall_status == "idle"
        else:
            print(f"âš ï¸ Stop operation had issues but continuing: {stop_result.message}")

        # Test 7: Test simple task control (should work quickly)
        print("ğŸ”„ Testing control on simple task...")
        simple_task = "Find out who is the founder of Browser-Use."

        async def run_simple_task():
            return await agent.run(simple_task)

        simple_task_coroutine = asyncio.create_task(run_simple_task())

        # Pause quickly
        await asyncio.sleep(0.5)
        pause_result = await agent.pause("Testing simple task pause")
        print(f"Simple task pause: {pause_result.success}")

        # Resume
        await asyncio.sleep(0.5)
        resume_result = await agent.resume("Testing simple task resume")
        print(f"Simple task resume: {resume_result.success}")

        # Let it complete
        simple_result = await simple_task_coroutine
        print(f"Simple task completed: {len(simple_result) > 0}")
        print(simple_result)
        print("ğŸ‰ All VibeSurfAgent control tests passed!")

    except Exception as e:
        print(f"âŒ VibeSurfAgent control test failed: {e}")
        import traceback
        traceback.print_exc()
        raise e
    finally:
        # Cleanup
        try:
            await agent.stop("Cleanup")
        except:
            pass
        await browser_manager.close()
        await main_browser_session.kill()


if __name__ == "__main__":
    # asyncio.run(run_single_bu_agent())
    # asyncio.run(run_multi_bu_agents())
    asyncio.run(test_vibe_surf_agent())
    # asyncio.run(test_vibe_surf_agent_control())



================================================
FILE: tests/test_backend_api.py
================================================
import asyncio
import pdb

import aiohttp
import json
import time
import sys
import os
from datetime import datetime
from typing import Dict, Any, Optional

from dotenv import load_dotenv
import sys
import pdb

sys.path.append(".")
load_dotenv()

# Base URL for the backend API
BASE_URL = "http://127.0.0.1:9335"
API_BASE = f"{BASE_URL}/api"


class BackendAPITester:
    """Real API test client for VibeSurf Backend"""

    def __init__(self):
        self.session = None
        self.test_session_id_1 = "068ac695-6ea2-795c-8000-ad03fc9c2b6c"
        self.test_session_id_2 = "068ac696-00e4-77c1-8000-a134f2f75d0b"
        self.created_profiles = []  # Track created profiles for cleanup

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def wait_for_backend(self, max_retries=30):
        """Wait for backend to be ready"""
        for i in range(max_retries):
            try:
                async with self.session.get(f"{BASE_URL}/health") as resp:
                    if resp.status == 200:
                        print("âœ… Backend is ready")
                        return True
            except:
                pass
            await asyncio.sleep(1)
        return False

    # Helper methods for API calls
    async def get(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Make GET request with optional query parameters"""
        url = f"{API_BASE}{endpoint}"
        if params:
            async with self.session.get(url, params=params) as resp:
                result = await resp.json()
                # params_str = f"?{params}" if params else ""
                return {"status": resp.status, "data": result}
        else:
            async with self.session.get(url) as resp:
                result = await resp.json()
                return {"status": resp.status, "data": result}

    async def post(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Make POST request"""
        async with self.session.post(
                f"{API_BASE}{endpoint}",
                json=data,
                headers={"Content-Type": "application/json"}
        ) as resp:
            result = await resp.json()
            print(f"POST {endpoint}: {resp.status} - {result}")
            return {"status": resp.status, "data": result}

    async def put(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Make PUT request"""
        async with self.session.put(
                f"{API_BASE}{endpoint}",
                json=data,
                headers={"Content-Type": "application/json"}
        ) as resp:
            result = await resp.json()
            print(f"PUT {endpoint}: {resp.status} - {result}")
            return {"status": resp.status, "data": result}

    async def delete(self, endpoint: str) -> Dict[str, Any]:
        """Make DELETE request"""
        async with self.session.delete(f"{API_BASE}{endpoint}") as resp:
            result = await resp.json()
            print(f"DELETE {endpoint}: {resp.status} - {result}")
            return {"status": resp.status, "data": result}


async def test_health_check():
    """Test basic health check"""
    print("ğŸ§ª Testing health check...")
    async with BackendAPITester() as tester:
        if not await tester.wait_for_backend():
            print("âŒ Backend not ready")
            return False

        async with tester.session.get(f"{BASE_URL}/health") as resp:
            result = await resp.json()
            print(f"Health check: {resp.status} - {result}")
            assert resp.status == 200
            assert result["status"] == "healthy"
            print("âœ… Health check passed")
            return True


async def test_llm_profile_management():
    """Test LLM profile CRUD operations"""
    print("\nğŸ§ª Testing LLM Profile Management...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # 1. List available providers
        print("\nğŸ“‹ Testing get available providers...")
        providers_resp = await tester.get("/config/llm/providers")
        assert providers_resp["status"] == 200
        providers = providers_resp["data"]["providers"]
        print(f"Found {len(providers)} providers")

        # 2. Create a test LLM profile
        print("\nâ• Testing create LLM profile...")
        test_profile = {
            "profile_name": f"test-profile-{int(time.time())}",
            "provider": "openai",
            "model": "gpt-4o-mini",
            "api_key": "test-api-key-123",
            "base_url": "https://api.openai.com/v1",
            "temperature": 0.7,
            "max_tokens": 2000,
            "description": "Test profile for API testing",
            "is_default": False
        }

        create_resp = await tester.post("/config/llm-profiles", test_profile)
        if create_resp["status"] == 200:
            print("âœ… LLM profile created successfully")
            tester.created_profiles.append(test_profile["profile_name"])
        else:
            print(f"âŒ Failed to create LLM profile: {create_resp}")

        # 3. List LLM profiles
        print("\nğŸ“‹ Testing list LLM profiles...")
        list_resp = await tester.get("/config/llm-profiles")
        assert list_resp["status"] == 200
        profiles = list_resp["data"]
        print(f"Found {len(profiles)} profiles")

        # 4. Get specific profile
        print("\nğŸ” Testing get specific profile...")
        profile_resp = await tester.get(f"/config/llm-profiles/{test_profile['profile_name']}")
        if profile_resp["status"] == 200:
            print("âœ… Retrieved profile successfully")
            profile_data = profile_resp["data"]
            assert profile_data["profile_name"] == test_profile["profile_name"]
            assert profile_data["provider"] == test_profile["provider"]

        # 5. Update profile
        print("\nâœï¸ Testing update profile...")
        update_data = {
            "temperature": 0.5,
            "description": "Updated test profile"
        }
        update_resp = await tester.put(f"/config/llm-profiles/{test_profile['profile_name']}", update_data)
        if update_resp["status"] == 200:
            print("âœ… Profile updated successfully")

        # 6. Create a second profile and set as default
        print("\nâ• Testing create second profile as default...")
        default_profile = {
            "profile_name": f"default-profile-{int(time.time())}",
            "provider": "openai",
            "model": "gpt-4o",
            "api_key": "default-api-key-123",
            "temperature": 0.3,
            "description": "Default test profile",
            "is_default": True
        }

        default_resp = await tester.post("/config/llm-profiles", default_profile)
        if default_resp["status"] == 200:
            print("âœ… Default profile created successfully")
            tester.created_profiles.append(default_profile["profile_name"])

        # 7. Get default profile
        print("\nğŸ† Testing get default profile...")
        default_get_resp = await tester.get("/config/llm-profiles/default/current")
        if default_get_resp["status"] == 200:
            print("âœ… Retrieved default profile successfully")

        # 8. Delete non-default profile (cleanup)
        print("\nğŸ—‘ï¸ Testing delete profile...")
        delete_resp = await tester.delete(f"/config/llm-profiles/{test_profile['profile_name']}")
        if delete_resp["status"] == 200:
            print("âœ… Profile deleted successfully")
            tester.created_profiles.remove(test_profile["profile_name"])

        print("âœ… LLM Profile Management tests completed")


async def test_controller_configuration():
    """Test tools/MCP server configuration"""
    print("\nğŸ§ª Testing Controller Configuration...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # 1. Get current tools config
        print("\nğŸ“‹ Testing get tools config...")
        config_resp = await tester.get("/config/tools")
        print(f"Controller config response: {config_resp}")

        # 2. Update tools config (using new Pydantic model)
        print("\nâœï¸ Testing update tools config...")
        controller_config = {
            "exclude_actions": ["scroll_up", "scroll_down"],
            "max_actions_per_task": 150,
            "display_files_in_done_text": False
        }

        update_resp = await tester.post("/config/tools", controller_config)
        if update_resp["status"] == 200:
            print("âœ… Controller configuration updated successfully")
        else:
            print(f"âŒ Controller config update failed: {update_resp}")

        # 3. Verify the update
        print("\nğŸ” Testing verify tools config update...")
        verify_resp = await tester.get("/config/tools")
        if verify_resp["status"] == 200:
            print("âœ… Controller configuration verified")

        print("âœ… Controller Configuration tests completed")


async def test_task_lifecycle_same_session():
    """Test task submission, control operations (pause/resume/stop) with same session"""
    print("\nğŸ§ª Testing Task Lifecycle - Same Session...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # Ensure we have a default LLM profile
        default_profile_name = "vibesurf_openai"
        default_resp = await tester.get(f"/config/llm-profiles/{default_profile_name}")
        if default_resp["status"] != 200:
            print("âš ï¸ No default LLM profile found, creating one...")
            test_profile = {
                "profile_name": f"{default_profile_name}",
                "provider": "openai_compatible",
                "model": "gemini-2.5-flash",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_ENDPOINT"),
                "temperature": 0.7,
                "is_default": True
            }
            create_resp = await tester.post("/config/llm-profiles", test_profile)
            if create_resp["status"] == 200:
                tester.created_profiles.append(test_profile["profile_name"])
                default_profile_name = test_profile["profile_name"]
            else:
                print("âŒ Failed to create default profile for testing")
                return

        # 1. Check initial status
        print("\nğŸ“Š Testing initial task status...")
        status_resp = await tester.get("/tasks/status")
        print(f"Initial status: {status_resp}")

        # 2. Submit first task
        print("\nğŸš€ Testing submit first task...")
        task_1 = {
            "session_id": tester.test_session_id_1,
            "task_description": "search for the founders of browser-use.",
            "llm_profile_name": default_profile_name
        }

        submit_resp = await tester.post("/tasks/submit", task_1)
        if submit_resp["status"] == 200:
            print("âœ… Task 1 submitted successfully")
            task_1_id = submit_resp["data"]["task_id"]
            while True:
                status_resp = await tester.get("/tasks/status")
                print("Check agent is available")
                print(status_resp)
                if status_resp.get("data", {}).get("has_active_task", False):
                    break
                await asyncio.sleep(1)

            # 4. Test pause (using new JSON body format)
            print("\nâ¸ï¸ Testing pause task...")
            pause_data = {"reason": "Testing pause functionality"}
            pause_resp = await tester.post("/tasks/pause", pause_data)
            if pause_resp["status"] == 200:
                print("âœ… Task paused successfully")

                # Check status after pause
                await asyncio.sleep(1)
                status_resp = await tester.get("/tasks/status")
                print(f"Status after pause: {status_resp}")

                # 5. Test resume (using new JSON body format)
                print("\nâ–¶ï¸ Testing resume task...")
                resume_data = {"reason": "Testing resume functionality"}
                resume_resp = await tester.post("/tasks/resume", resume_data)
                if resume_resp["status"] == 200:
                    print("âœ… Task resumed successfully")

                    # 6. Test stop (using new JSON body format)
                    print("\nğŸ›‘ Testing stop task...")
                    stop_data = {"reason": "Testing stop functionality"}
                    stop_resp = await tester.post("/tasks/stop", stop_data)
                    if stop_resp["status"] == 200:
                        print("âœ… Task stopped successfully")
                    else:
                        print(f"âš ï¸ Stop failed: {stop_resp}")
                else:
                    print(f"âŒ Resume failed: {resume_resp}")
            else:
                print(f"âŒ Pause failed: {pause_resp}")

        # 7. Wait for task to complete and try another task in same session
        while True:
            status_resp = await tester.get("/tasks/status")
            if not status_resp.get("data", {}).get("has_active_task", False):
                break
            await asyncio.sleep(1)

        print("\nğŸ”„ Testing second task in same session...")

        task_2 = {
            "session_id": tester.test_session_id_1,  # Same session
            "task_description": "Please say 'Task 2 in same session executed successfully'.",
            "llm_profile_name": default_profile_name
        }

        submit_resp_2 = await tester.post("/tasks/submit", task_2)
        if submit_resp_2["status"] == 200:
            print("âœ… Task 2 (same session) submitted successfully")
            # Let it run to completion
            await asyncio.sleep(10)
        else:
            print(f"âŒ Task 2 submission failed: {submit_resp_2}")

        print("âœ… Task Lifecycle - Same Session tests completed")


async def test_activity_and_history():
    """Test activity logs and task history retrieval"""
    print("\nğŸ§ª Testing Activity Logs and History...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # 1. Get recent tasks
        default_profile_name = "vibesurf_openai"
        default_resp = await tester.get(f"/config/llm-profiles/{default_profile_name}")
        if default_resp["status"] != 200:
            print("âš ï¸ No default LLM profile found, creating one...")
            test_profile = {
                "profile_name": f"{default_profile_name}",
                "provider": "openai_compatible",
                "model": "gemini-2.5-flash",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_ENDPOINT"),
                "temperature": 0.7,
                "is_default": True
            }
            create_resp = await tester.post("/config/llm-profiles", test_profile)
            if create_resp["status"] == 200:
                tester.created_profiles.append(test_profile["profile_name"])
                default_profile_name = test_profile["profile_name"]
            else:
                print("âŒ Failed to create default profile for testing")
                return

        # 1. Check initial status
        print("\nğŸ“Š Testing initial task status...")
        status_resp = await tester.get("/tasks/status")
        print(f"Initial status: {status_resp}")

        # 2. Submit first task
        print("\nğŸš€ Testing submit first task...")
        task_1 = {
            "session_id": tester.test_session_id_1,
            "task_description": "search for the founders of langflow.",
            "llm_profile_name": default_profile_name
        }

        # get current activity logs before submit
        activity_resp = await tester.get(f"/activity/sessions/{tester.test_session_id_1}/activity")
        activity_logs = activity_resp.get("data", {}).get("activity_logs", [])
        print(f"Current has {len(activity_logs)} activity logs")
        prev_activity_log = activity_logs[-1] if activity_logs else None

        submit_resp = await tester.post("/tasks/submit", task_1)

        # 3. Get session tasks
        print(f"\nğŸ“‹ Testing get session tasks for {tester.test_session_id_1}...")

        while True:
            # Use query parameters for activity endpoint
            activity_resp = await tester.get(f"/activity/sessions/{tester.test_session_id_1}/activity",
                                             params={"message_index": len(activity_logs)})
            cur_activity_log = activity_resp.get("data", {}).get("activity_log", None)
            if activity_resp["status"] == 200 and cur_activity_log and (prev_activity_log is None or
                    prev_activity_log and prev_activity_log != cur_activity_log):
                print(cur_activity_log)
                activity_logs.append(cur_activity_log)
                prev_activity_log = cur_activity_log
                if cur_activity_log["agent_status"] == "done":
                    break
            else:
                await asyncio.sleep(1)

        # 5. Get latest activity
        print(f"\nğŸ“‹ Testing get latest activity...")
        latest_resp = await tester.get(f"/activity/sessions/{tester.test_session_id_1}/latest_activity")
        if latest_resp["status"] == 200:
            print("âœ… Latest activity retrieved successfully")
        print(latest_resp)
        print("âœ… Activity and History tests completed")


async def test_configuration_status():
    """Test configuration status endpoints"""
    print("\nğŸ§ª Testing Configuration Status...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # 1. Get overall configuration status
        print("\nğŸ“Š Testing get configuration status...")
        status_resp = await tester.get("/config/status")
        if status_resp["status"] == 200:
            print("âœ… Configuration status retrieved successfully")
            status_data = status_resp["data"]
            print(f"Overall status: {status_data.get('overall_status')}")
            print(f"LLM profiles: {status_data.get('llm_profiles')}")
            print(f"Controller initialized: {status_data.get('tools', {}).get('initialized')}")
            print(f"Browser manager initialized: {status_data.get('browser_manager', {}).get('initialized')}")
            print(f"Swarm agent initialized: {status_data.get('swarm_agent', {}).get('initialized')}")

        # 2. Get system status
        print("\nğŸ“Š Testing get system status...")
        async with tester.session.get(f"{API_BASE}/status") as resp:
            if resp.status == 200:
                result = await resp.json()
                print(f"System status: {result}")
                print("âœ… System status retrieved successfully")

        print("âœ… Configuration Status tests completed")


async def test_file_upload():
    """Test file upload functionality"""
    print("\nğŸ§ª Testing File Upload...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # 1. Test file upload with session_id
        print("\nğŸ“ Testing file upload with session ID...")
        test_file_path = "tmp/swarm_surf_workspace/activity_logs.pkl"
        
        # Check if test file exists
        if not os.path.exists(test_file_path):
            print(f"âš ï¸ Test file not found: {test_file_path}")
            return
        
        # Prepare multipart form data
        with open(test_file_path, 'rb') as f:
            file_content = f.read()
        
        # Create form data for upload
        data = aiohttp.FormData()
        data.add_field('files', file_content, filename='activity_logs.pkl', content_type='application/octet-stream')
        data.add_field('session_id', tester.test_session_id_1)
        
        # Upload file
        async with tester.session.post(f"{API_BASE}/files/upload", data=data) as resp:
            if resp.status == 200:
                result = await resp.json()
                print(f"âœ… File uploaded successfully: {result}")
                
                # Verify response structure
                assert "message" in result
                assert "files" in result
                assert len(result["files"]) == 1
                
                uploaded_file = result["files"][0]
                assert uploaded_file["original_filename"] == "activity_logs.pkl"
                assert uploaded_file["session_id"] == tester.test_session_id_1
                assert "file_id" in uploaded_file
                
                file_id = uploaded_file["file_id"]
                print(f"ğŸ“„ File ID: {file_id}")
                
                # 2. Test file listing
                print("\nğŸ“‹ Testing file listing...")
                list_resp = await tester.get(f"/files?session_id={tester.test_session_id_1}")
                if list_resp["status"] == 200:
                    files_data = list_resp["data"]
                    assert "files" in files_data
                    assert files_data["total_count"] >= 1
                    print(f"âœ… Found {files_data['total_count']} files in session")

                # 3. Test file download
                print("\nğŸ“¥ Testing file download...")
                async with tester.session.get(f"{API_BASE}/files/{file_id}") as resp:
                    if resp.status == 200:
                        content = await resp.read()
                        print(f"âœ… File download successful, size: {len(content)} bytes")
                    else:
                        print(f"âš ï¸ File download failed: {resp.status}")
                
                # 4. Test file deletion (cleanup)
                print("\nğŸ—‘ï¸ Testing file deletion...")
                delete_resp = await tester.delete(f"/files/{file_id}")
                if delete_resp["status"] == 200:
                    print("âœ… File deleted successfully")
                else:
                    print(f"âš ï¸ File deletion failed: {delete_resp}")
                    
            else:
                result = await resp.json()
                print(f"âŒ File upload failed: {resp.status} - {result}")

        # 5. Test file upload without session_id
        print("\nğŸ“ Testing file upload without session ID...")
        data_no_session = aiohttp.FormData()
        data_no_session.add_field('files', file_content, filename='activity_logs_global.pkl', content_type='application/octet-stream')
        
        async with tester.session.post(f"{API_BASE}/files/upload", data=data_no_session) as resp:
            if resp.status == 200:
                result = await resp.json()
                print(f"âœ… Global file uploaded successfully")
                
                # Clean up global file
                if result["files"]:
                    global_file_id = result["files"][0]["file_id"]
                    await tester.delete(f"/files/{global_file_id}")
                    print("âœ… Global file cleaned up")
            else:
                result = await resp.json()
                print(f"âš ï¸ Global file upload failed: {resp.status} - {result}")

        print("âœ… File Upload tests completed")


async def cleanup_test_profiles():
    """Clean up test profiles created during testing"""
    print("\nğŸ§¹ Cleaning up test profiles...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()

        # Get all profiles
        list_resp = await tester.get("/config/llm-profiles", params={"active_only": "false"})
        if list_resp["status"] == 200:
            profiles = list_resp["data"]

            # Delete test profiles (ones containing 'test' or 'auto-default')
            for profile in profiles:
                profile_name = profile["profile_name"]
                if ("test" in profile_name.lower() or
                        "auto-default" in profile_name.lower() or
                        "default-profile" in profile_name.lower()):

                    if not profile["is_default"]:  # Can't delete default profile
                        print(f"ğŸ—‘ï¸ Deleting test profile: {profile_name}")
                        delete_resp = await tester.delete(f"/config/llm-profiles/{profile_name}")
                        if delete_resp["status"] == 200:
                            print(f"âœ… Deleted {profile_name}")
                        else:
                            print(f"âš ï¸ Failed to delete {profile_name}: {delete_resp}")
                    else:
                        print(f"âš ï¸ Skipping default profile: {profile_name}")


async def test_mcp_profile_management():
    """Test MCP profile CRUD operations"""
    print("\nğŸ§ª Testing MCP Profile Management...")
    async with BackendAPITester() as tester:
        await tester.wait_for_backend()
        created_mcp_profiles = []  # Track for cleanup

        try:
            # 1. Create a test MCP profile
            print("\nâ• Testing create MCP profile...")
            test_mcp_profile = {
                "display_name": f"test-mcp-filesystem-{int(time.time())}",
                "mcp_server_name": f"filesystem-test-{int(time.time())}",
                "mcp_server_params": {
                    "command": "npx",
                    "args": [
                        "-y",
                        "@modelcontextprotocol/server-filesystem",
                        "/tmp/test"
                    ]
                },
                "description": "Test filesystem MCP server"
            }

            create_resp = await tester.post("/config/mcp-profiles", test_mcp_profile)
            if create_resp["status"] == 200:
                print("âœ… MCP profile created successfully")
                created_profile = create_resp["data"]
                created_mcp_profiles.append(created_profile["mcp_id"])
                
                # Verify created profile structure
                assert created_profile["display_name"] == test_mcp_profile["display_name"]
                assert created_profile["mcp_server_name"] == test_mcp_profile["mcp_server_name"]
                assert created_profile["mcp_server_params"] == test_mcp_profile["mcp_server_params"]
                assert created_profile["is_active"] == True  # Default value
            else:
                print(f"âŒ Failed to create MCP profile: {create_resp}")
                return

            # 2. Test uniqueness constraint on mcp_server_name
            print("\nğŸ”’ Testing mcp_server_name uniqueness constraint...")
            duplicate_profile = {
                "display_name": f"duplicate-test-{int(time.time())}",
                "mcp_server_name": test_mcp_profile["mcp_server_name"],  # Same server name
                "mcp_server_params": {
                    "command": "docker",
                    "args": ["run", "test"]
                }
            }
            
            duplicate_resp = await tester.post("/config/mcp-profiles", duplicate_profile)
            if duplicate_resp["status"] == 400:
                print("âœ… Uniqueness constraint working correctly")
            else:
                print(f"âš ï¸ Uniqueness constraint test unexpected result: {duplicate_resp}")

            # 3. List MCP profiles
            print("\nğŸ“‹ Testing list MCP profiles...")
            list_resp = await tester.get("/config/mcp-profiles")
            if list_resp["status"] == 200:
                profiles = list_resp["data"]
                print(f"Found {len(profiles)} MCP profiles")
                
                # Verify our profile is in the list
                profile_found = any(p["mcp_id"] == created_profile["mcp_id"] for p in profiles)
                assert profile_found, "Created profile not found in list"
                print("âœ… Created profile found in list")

            # 4. Get specific MCP profile
            print("\nğŸ” Testing get specific MCP profile...")
            profile_resp = await tester.get(f"/config/mcp-profiles/{created_profile['mcp_id']}")
            if profile_resp["status"] == 200:
                profile_data = profile_resp["data"]
                assert profile_data["mcp_id"] == created_profile["mcp_id"]
                assert profile_data["display_name"] == test_mcp_profile["display_name"]
                print("âœ… Retrieved MCP profile successfully")

            # 5. Update MCP profile
            print("\nâœï¸ Testing update MCP profile...")
            update_data = {
                "description": "Updated test filesystem MCP server",
                "is_active": False,
                "mcp_server_params": {
                    "command": "npx",
                    "args": [
                        "-y",
                        "@modelcontextprotocol/server-filesystem",
                        "/tmp/updated"
                    ]
                }
            }
            
            update_resp = await tester.put(f"/config/mcp-profiles/{created_profile['mcp_id']}", update_data)
            if update_resp["status"] == 200:
                updated_profile = update_resp["data"]
                assert updated_profile["description"] == update_data["description"]
                assert updated_profile["is_active"] == False
                assert updated_profile["mcp_server_params"]["args"][-1] == "/tmp/updated"
                print("âœ… MCP profile updated successfully")

            # 6. Create another profile for different server
            print("\nâ• Testing create second MCP profile...")
            second_profile = {
                "display_name": f"test-mcp-markitdown-{int(time.time())}",
                "mcp_server_name": f"markitdown-test-{int(time.time())}",
                "mcp_server_params": {
                    "command": "docker",
                    "args": [
                        "run",
                        "--rm",
                        "-i",
                        "markitdown-mcp:latest"
                    ]
                },
                "description": "Test markitdown MCP server"
            }
            
            second_resp = await tester.post("/config/mcp-profiles", second_profile)
            if second_resp["status"] == 200:
                second_created = second_resp["data"]
                created_mcp_profiles.append(second_created["mcp_id"])
                print("âœ… Second MCP profile created successfully")

            # 7. List only active profiles
            print("\nğŸ“‹ Testing list active MCP profiles...")
            active_resp = await tester.get("/config/mcp-profiles", params={"active_only": "true"})
            if active_resp["status"] == 200:
                active_profiles = active_resp["data"]
                # First profile should be inactive, second should be active
                active_ids = [p["mcp_id"] for p in active_profiles]
                assert created_profile["mcp_id"] not in active_ids, "Inactive profile found in active list"
                assert second_created["mcp_id"] in active_ids, "Active profile not found in active list"
                print("âœ… Active profile filtering working correctly")

            # 8. Test 404 for non-existent profile
            print("\nğŸ” Testing get non-existent MCP profile...")
            notfound_resp = await tester.get("/config/mcp-profiles/non-existent-id")
            if notfound_resp["status"] == 404:
                print("âœ… 404 returned for non-existent profile")

        finally:
            # Cleanup: Delete created profiles
            print("\nğŸ—‘ï¸ Cleaning up MCP profiles...")
            for mcp_id in created_mcp_profiles:
                delete_resp = await tester.delete(f"/config/mcp-profiles/{mcp_id}")
                if delete_resp["status"] == 200:
                    print(f"âœ… Deleted MCP profile {mcp_id}")
                else:
                    print(f"âš ï¸ Failed to delete MCP profile {mcp_id}: {delete_resp}")

        print("âœ… MCP Profile Management tests completed")


async def run_all_tests():
    """Run all API tests in sequence"""
    print("ğŸš€ Starting VibeSurf Backend API Tests")
    print("=" * 60)

    try:
        # Basic connectivity
        # await test_health_check()

        # Configuration tests
        # await test_llm_profile_management()
        # await test_mcp_profile_management()
        # await test_controller_configuration()
        # await test_configuration_status()
        #
        # # Task execution tests
        # await test_task_lifecycle_same_session()
        #
        # # Activity and history tests
        await test_activity_and_history()
        
        # File upload tests
        # await test_file_upload()

        print("\nğŸ‰ All API tests completed successfully!")

    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()

    print("=" * 60)
    print("âœ… VibeSurf Backend API Testing Complete")


if __name__ == "__main__":
    print("VibeSurf Backend API Tester")
    print("Make sure your backend is running on http://127.0.0.1:9335")
    print("Command: uvicorn vibe_surf.backend.main:app --host 127.0.0.1 --port 9335")
    print()

    asyncio.run(run_all_tests())



================================================
FILE: tests/test_browser.py
================================================
import asyncio
import base64
import logging
import os
import sys
import pdb

sys.path.append(".")

from dotenv import load_dotenv

load_dotenv()

from browser_use.browser.events import NavigateToUrlEvent
from browser_use.browser.session import BrowserSession, BrowserProfile

from vibe_surf.browser.browser_manager import BrowserManager
from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.browser.agen_browser_profile import AgentBrowserProfile


async def test_multi_agent_isolation(manager: BrowserManager):
    """Verify that two agents can operate on separate pages concurrently."""
    logging.info("--- Running Test: Multi-Agent Isolation ---")
    agent1 = await manager.register_agent("agent-iso-1")
    agent2 = await manager.register_agent("agent-iso-2")

    nav1 = agent1.event_bus.dispatch(
        NavigateToUrlEvent(url=f"https://www.google.com/search?q=browser-use")
    )
    nav2 = agent2.event_bus.dispatch(
        NavigateToUrlEvent(url=f"https://www.google.com/search?q=langflow")
    )
    await asyncio.gather(nav1, nav2)
    logging.info("âœ… Agents navigated to different pages concurrently.")
    await asyncio.sleep(3)


async def test_agent_cleanup(manager: BrowserManager):
    """Verify that unregistering an agent closes all its pages."""
    logging.info("--- Running Test: Agent Cleanup ---")
    agent = await manager.register_agent("agent-cleanup")
    await manager.assign_target_to_agent("agent-cleanup")  # Assign a second page

    agent_tabs = await agent.get_tabs()
    print(agent_tabs)
    num_pages = len(agent_tabs)
    logging.info(f"Agent for cleanup has {num_pages} pages.")
    assert num_pages > 1

    logging.info("ğŸ§¹ Unregistering agent...")
    await manager.unregister_agent("agent-cleanup", close_tabs=True)

    # Verify the agent and its pages are gone
    assert not manager.get_agent_sessions("agent-cleanup")
    logging.info("âœ… Agent unregistered and all its pages were closed.")
    await asyncio.sleep(3)


async def test_agent_tab_isolation(manager: BrowserManager):
    """Verify that one agent closing its tab does not affect another agent."""
    logging.info("--- Running Test: Agent Tab Isolation ---")
    agent1 = await manager.register_agent("agent-tab-iso-1")
    agent2 = await manager.register_agent("agent-tab-iso-2")

    # Both agents have one page
    agent1_tabs = await agent1.get_tabs()
    agent2_tabs = await agent2.get_tabs()
    assert len(agent1_tabs) == 1
    assert len(agent2_tabs) == 1

    # Agent 1 closes its page
    logging.info("Agent 1 closing its page...")
    await manager.unregister_agent("agent-tab-iso-1", close_tabs=True)

    # Verify agent 1 is gone, but agent 2 remains
    assert not manager.get_agent_sessions("agent-tab-iso-1")
    agent2_tabs = await agent2.get_tabs()
    assert len(agent2_tabs) == 1
    logging.info("âœ… Agent 1's tab closed, Agent 2's tab remains.")
    await asyncio.sleep(3)


async def test_browser_state_capture(manager: BrowserManager):
    """Verify two agents can concurrently capture their state and save screenshots."""
    logging.info("--- Running Test: Concurrent State Capture ---")

    agent1, agent2, agent3 = await asyncio.gather(
        manager.register_agent("agent-state-1"),
        manager.register_agent("agent-state-2"),
        manager.register_agent("agent-state-3")
    )

    # æ·»åŠ å¹¶å‘è¯Šæ–­æ—¥å¿—
    import time
    start_time = time.time()
    logging.info(f"ğŸ” DIAGNOSIS: Starting concurrent navigation at {start_time}")

    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¯¼èˆªä»»åŠ¡
    async def navigate_with_timing(agent, url, agent_name):
        task_start = time.time()
        logging.info(f"ğŸ” DIAGNOSIS: {agent_name} navigation started at {task_start - start_time:.3f}s")
        # ä½¿ç”¨æ–°çš„å¹¶å‘å¯¼èˆªæ–¹æ³•ç»•è¿‡ä¸²è¡Œç“¶é¢ˆ
        await agent.navigate_to_url(url)
        task_end = time.time()
        logging.info(
            f"ğŸ” DIAGNOSIS: {agent_name} navigation completed at {task_end - start_time:.3f}s (duration: {task_end - task_start:.3f}s)")
        return None

    # async def navigate_with_timing(agent, url, agent_name):
    #     task_start = time.time()
    #     logging.info(f"ğŸ” DIAGNOSIS: {agent_name} navigation started at {task_start - start_time:.3f}s")
    #     # ä½¿ç”¨æ–°çš„å¹¶å‘å¯¼èˆªæ–¹æ³•ç»•è¿‡ä¸²è¡Œç“¶é¢ˆ
    #     await agent.event_bus.dispatch(NavigateToUrlEvent(url=url, new_tab=False))
    #     task_end = time.time()
    #     logging.info(
    #         f"ğŸ” DIAGNOSIS: {agent_name} navigation completed at {task_end - start_time:.3f}s (duration: {task_end - task_start:.3f}s)")
    #     return None

    # Navigate to different pages
    await asyncio.gather(
        navigate_with_timing(agent1, "https://www.python.org", "agent1"),
        navigate_with_timing(agent2, "https://www.rust-lang.org", "agent2"),
        navigate_with_timing(agent3, "https://www.github.com", "agent3"),
    )

    end_time = time.time()
    total_duration = end_time - start_time
    logging.info(f"ğŸ” DIAGNOSIS: All navigation completed in {total_duration:.3f}s")

    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¯¼èˆªä»»åŠ¡
    async def take_screenshot_with_timing(agent: AgentBrowserSession, agent_name):
        task_start = time.time()
        logging.info(f"ğŸ” DIAGNOSIS: {agent_name} taking screenshot started at {task_start - start_time:.3f}s")
        # ä½¿ç”¨æ–°çš„å¹¶å‘å¯¼èˆªæ–¹æ³•ç»•è¿‡ä¸²è¡Œç“¶é¢ˆ
        image = await agent.take_screenshot()
        task_end = time.time()
        logging.info(
            f"ğŸ” DIAGNOSIS: {agent_name} taking screenshot completed at {task_end - start_time:.3f}s (duration: {task_end - task_start:.3f}s)")
        return image

    # Navigate to different pages
    screenshot1, screenshot2, screenshot3 = await asyncio.gather(
        take_screenshot_with_timing(agent1, "agent1"),
        take_screenshot_with_timing(agent2, "agent2"),
        take_screenshot_with_timing(agent3, "agent3"),
    )
    os.makedirs("./tmp/screenshots", exist_ok=True)
    with open("./tmp/screenshots/agent1_python.png", "wb") as f:
        f.write(base64.b64decode(screenshot1))
    with open("./tmp/screenshots/agent2_rust.png", "wb") as f:
        f.write(base64.b64decode(screenshot2))
    with open("./tmp/screenshots/agent3_github.png", "wb") as f:
        f.write(base64.b64decode(screenshot3))
    end_time = time.time()
    total_duration = end_time - start_time
    logging.info(f"ğŸ” DIAGNOSIS: All taking screenshot completed in {total_duration:.3f}s")
    # Concurrently get browser state
    start_time = time.time()

    async def get_state_with_timing(agent, agent_name):
        task_start = time.time()
        logging.info(f"ğŸ” DIAGNOSIS: {agent_name} get state started at {task_start - start_time:.3f}s")
        # ä½¿ç”¨æ–°çš„å¹¶å‘å¯¼èˆªæ–¹æ³•ç»•è¿‡ä¸²è¡Œç“¶é¢ˆ
        state = await agent.get_browser_state_summary()
        # state = await agent.get_browser_state()
        task_end = time.time()
        logging.info(
            f"ğŸ” DIAGNOSIS: {agent_name} get state completed at {task_end - start_time:.3f}s (duration: {task_end - task_start:.3f}s)")
        return state

    # async def get_state_with_timing(agent, agent_name):
    #     task_start = time.time()
    #     logging.info(f"ğŸ” DIAGNOSIS: {agent_name} get state started at {task_start - start_time:.3f}s")
    #     # ä½¿ç”¨æ–°çš„å¹¶å‘å¯¼èˆªæ–¹æ³•ç»•è¿‡ä¸²è¡Œç“¶é¢ˆ
    #     # state = await agent.get_browser_state_summary()
    #     state = await agent.get_browser_state_summary()
    #     task_end = time.time()
    #     logging.info(
    #         f"ğŸ” DIAGNOSIS: {agent_name} get state completed at {task_end - start_time:.3f}s (duration: {task_end - task_start:.3f}s)")
    #     return state

    logging.info("Capturing browser states concurrently...")
    state1, state2, state3 = await asyncio.gather(
        get_state_with_timing(agent1, "agent1"),
        get_state_with_timing(agent2, "agent2"),
        get_state_with_timing(agent3, "agent3"),
    )
    end_time = time.time()
    total_duration = end_time - start_time
    logging.info(f"ğŸ” DIAGNOSIS: All Get state completed in {total_duration:.3f}s")
    print(state1.tabs)
    print(state2.tabs)
    print(state3.tabs)

    # Ensure screenshots are present
    assert state1.screenshot, "Agent 1 failed to capture screenshot."
    assert state2.screenshot, "Agent 2 failed to capture screenshot."
    logging.info("âœ… Both agents captured screenshots.")

    # Save screenshots
    os.makedirs("./tmp/screenshots", exist_ok=True)
    with open("./tmp/screenshots/agent1_python_highlight.png", "wb") as f:
        f.write(base64.b64decode(state1.screenshot))
    with open("./tmp/screenshots/agent2_rust_highlight.png", "wb") as f:
        f.write(base64.b64decode(state2.screenshot))
    with open("./tmp/screenshots/agent3_github_highlight.png", "wb") as f:
        f.write(base64.b64decode(state3.screenshot))
    logging.info("âœ… Screenshots saved to ./tmp/screenshots/")

    # Agent 1 creates a new page and navigates
    logging.info("Agent 1 creating a new page and navigating to GitHub...")
    new_page_target_id = await agent1.navigate_to_url(url="https://www.realestate.com.au/", new_tab=True)

    # Get new state for agent 1
    logging.info("Capturing new state for agent 1...")
    state1_new = await agent1.get_browser_state_summary()

    # Print new tabs and save screenshot
    logging.info(f"Agent 1 new tabs: {state1_new.tabs}")
    assert state1_new.screenshot, "Agent 1 failed to capture new screenshot."
    with open("./tmp/screenshots/agent1_new_realestate.png", "wb") as f:
        f.write(base64.b64decode(state1_new.screenshot))
    logging.info("âœ… Agent 1's new screenshot saved.")

    # Use manager to get active page info
    logging.info("Verifying active page with manager...")
    active_target_id = await manager._get_active_target()
    logging.info(f"Manager sees active target: {active_target_id}")
    active_tab1 = await manager._get_activate_tab_info()
    print(active_tab1)
    await agent2.active_focus_page()
    active_target_id = await manager._get_active_target()
    logging.info(f"After switching, Manager sees active target: {active_target_id}")
    active_tab2 = await manager._get_activate_tab_info()
    print(active_tab2)
    await manager.unregister_agent("agent-state-1", close_tabs=True)
    main_tabs = await manager.main_browser_session.get_tabs()
    print(main_tabs)


async def get_all_css_selector(browser_session: AgentBrowserSession):
    target_id = await browser_session.navigate_to_url("https://github.com/", new_tab=True)
    result = await browser_session.cdp_client.send.Target.attachToTarget({'targetId': target_id, 'flatten': True})
    session_id = result['sessionId']
    doc_result = await browser_session.cdp_client.send.DOM.getDocument(session_id=session_id)
    document_node_id = doc_result['root']['nodeId']
    from browser_use.dom.service import DomService
    dom_service = DomService(browser_session)
    pdb.set_trace()
    # Query selector all
    query_params = {'nodeId': document_node_id}
    result = await browser_session.cdp_client.send.DOM.querySelectorAll(query_params, session_id=session_id)

    elements = []

    # Convert node IDs to backend node IDs
    for node_id in result['nodeIds']:
        # Get backend node ID
        describe_params = {'nodeId': node_id}
        node_result = await browser_session.cdp_client.send.DOM.describeNode(describe_params, session_id=session_id)
        pdb.set_trace()


async def test_website_api(main_browser_session: AgentBrowserSession):
    # from vibe_surf.tools.website_api.xhs.client import XiaoHongShuApiClient, SearchType
    # xhs_client = XiaoHongShuApiClient(browser_session=main_browser_session)
    # await xhs_client.setup()
    # user_info = await xhs_client.get_me()
    # user_id = user_info['user_id']
    # ret = await xhs_client.search_content_by_keyword("browser-use", sort_type=SearchType.POPULAR)
    # pdb.set_trace()
    # ret = await xhs_client.get_home_recommendations()
    # pdb.set_trace()
    # ret = await xhs_client.get_user_profile(user_id=user_id)
    # ret = await xhs_client.fetch_all_user_content(user_id=user_id)
    # note_id = ret[1]['note_id']
    # xsec_token = ret[1]['xsec_token']
    # ret1 = await xhs_client.fetch_content_details(content_id=note_id, xsec_token=xsec_token)
    # pdb.set_trace()
    # ret2 = await xhs_client.fetch_all_content_comments(content_id=note_id, xsec_token=xsec_token)
    # pdb.set_trace()

    from vibe_surf.tools.website_api.weibo.client import WeiboApiClient
    from vibe_surf.tools.website_api.weibo.helpers import SearchType
    wb_client = WeiboApiClient(browser_session=main_browser_session)
    await wb_client.setup()
    # ret = await wb_client.search_posts_by_keyword("é‚“ç´«æ£‹", page=4, search_type=SearchType.POPULAR)
    # pdb.set_trace()
    # mid = ret[0]['note_id']
    # user_id = ret[0]['user_id']
    # ret = await wb_client.get_post_detail(mid=mid)
    # pdb.set_trace()
    # ret = await wb_client.get_all_post_comments(mid=mid)
    # pdb.set_trace()
    # ret1 = await wb_client.get_user_info(user_id=user_id)
    # ret3 = await wb_client.get_all_user_posts(user_id=user_id)
    # pdb.set_trace()
    ret = await wb_client.get_hot_posts()
    pdb.set_trace()
    ret = await wb_client.get_trending_posts()
    pdb.set_trace()

    # from vibe_surf.tools.website_api.douyin.client import DouyinApiClient
    #
    # dy_client = DouyinApiClient(main_browser_session)
    # await dy_client.setup()
    # ret = await dy_client.search_content_by_keyword("Sora2")
    # aweme_id = ret[0]['aweme_id']
    # user_id = ret[0]['user_id']
    # ret1 = await dy_client.fetch_video_details(aweme_id=aweme_id)
    # ret2 = await dy_client.fetch_video_comments(aweme_id=aweme_id)
    # ret3 = await dy_client.fetch_user_info(sec_user_id=user_id)
    # ret3 = await dy_client.fetch_user_videos(sec_user_id=user_id)
    # pdb.set_trace()

    # from vibe_surf.tools.website_api.youtube.client import YouTubeApiClient
    # yt_client = YouTubeApiClient(browser_session=main_browser_session)
    # await yt_client.setup()
    # ret = await yt_client.get_trending_videos()
    # pdb.set_trace()
    # ret = await yt_client.search_videos(query="ä½•åŒå­¦", max_results=30)
    # pdb.set_trace()
    # ret = await yt_client.get_video_details(ret[0]['video_id'])
    # pdb.set_trace()
    # ret = await yt_client.get_video_comments(ret[0]['video_id'])
    # pdb.set_trace()
    # ret = await yt_client.get_channel_info(ret[0]['channel_id'])
    # pdb.set_trace()
    # ret = await yt_client.get_channel_videos(ret[0]['channel_id'], max_videos=50)
    # pdb.set_trace()


async def main():
    """
    Main function to run all browser session tests.
    """
    main_browser_session = None
    try:
        logging.info("ğŸš€ Launching browser...")
        import platform
        if platform.system() != "Darwin":
            browser_exec_path = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
        else:
            browser_exec_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
        from pathlib import Path
        current_file = Path(__file__)
        project_root = current_file.parent.parent  # vibe_surf/browser -> vibe_surf -> project_root
        chrome_extension_path = project_root / "vibe_surf" / "chrome_extension"
        assert os.path.exists(chrome_extension_path)

        browser_profile = AgentBrowserProfile(
            executable_path=browser_exec_path,
            user_data_dir=os.path.abspath('./tmp/chrome/profiles/default'),
            headless=False,
            highlight_elements=True,
            custom_extensions=[str(chrome_extension_path.absolute())]
        )
        # Use SwarmBrowserSession instead of BrowserSession to disable DVD animation
        main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
        await main_browser_session.start()
        async with BrowserManager(main_browser_session=main_browser_session) as manager:
            # await test_multi_agent_isolation(manager)
            # await test_manual_page_assignment(manager)
            # await test_agent_cleanup(manager)
            # await test_agent_tab_isolation(manager)
            # await test_browser_state_capture(manager)
            # await get_all_css_selector(main_browser_session)
            await test_website_api(main_browser_session)

    except Exception as e:
        logging.error(f"An error occurred during tests: {e}", exc_info=True)

    finally:
        if main_browser_session:
            await main_browser_session.kill()
            logging.info("âœ… Browser session killed.")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: tests/test_tools.py
================================================
import asyncio
import pdb
import sys
import time

sys.path.append(".")

from dotenv import load_dotenv

load_dotenv()


async def test_tools_with_mcp():
    import os
    from vibe_surf.tools.vibesurf_tools import VibeSurfTools

    mcp_server_config = {
        "mcpServers": {
            # "markitdown": {
            #     "command": "docker",
            #     "args": [
            #         "run",
            #         "--rm",
            #         "-i",
            #         "markitdown-mcp:latest"
            #     ]
            # },
            # "desktop-commander": {
            #     "command": "npx",
            #     "args": [
            #         "-y",
            #         "@wonderwhy-er/desktop-commander"
            #     ],
            # },
            "filesystem": {
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-filesystem",
                    "E:\\AIBrowser\\VibeSurf\\tmp\\code",
                ]
            },
        }
    }

    controller = VibeSurfTools(mcp_server_config=mcp_server_config)
    await controller.register_mcp_clients()
    pdb.set_trace()
    # action_name = "mcp.desktop-commander.start_process"
    action_name = "mcp.filesystem.list_directory"
    action_info = controller.registry.registry.actions[action_name]
    param_model = action_info.param_model
    print(param_model.model_json_schema())
    # params = {
    #     "command": f"python ./tmp/code/test.py",
    #     "timeout_ms": 30000
    # }
    params = {
        "path": r"E:\AIBrowser\VibeSurf\tmp\code",
    }
    validated_params = param_model(**params)
    ActionModel_ = controller.registry.create_action_model()
    # Create ActionModel instance with the validated parameters
    action_model = ActionModel_(**{action_name: validated_params})
    result = await controller.act(action_model)
    result = result.extracted_content

    # print(result)
    # if result and "Command is still running. Use read_output to get more output." in result and "PID" in \
    #         result.split("\n")[0]:
    #     pid = int(result.split("\n")[0].split("PID")[-1].strip())
    #     pdb.set_trace()
    #     action_name = "mcp.desktop-commander.read_process_output"
    #     action_info = tools.registry.registry.actions[action_name]
    #     param_model = action_info.param_model
    #     print(param_model.model_json_schema())
    #     params = {"pid": pid}
    #     validated_params = param_model(**params)
    #     action_model = ActionModel_(**{action_name: validated_params})
    #     output_result = ""
    #     while True:
    #         time.sleep(1)
    #         result = await tools.act(action_model)
    #         result = result.extracted_content
    #         if result:
    #             output_result = result
    #             break
    #     print(output_result)
    await controller.unregister_mcp_clients()


async def test_filesystem():
    from vibe_surf.tools.file_system import CustomFileSystem

    file_system_path = r"E:\AIBrowser\VibeSurf\tmp\vibesurf_workspace"
    filesystem = CustomFileSystem(file_system_path)
    result = await filesystem.create_file("reports/final_report.html")
    print(result)
    result = filesystem.get_absolute_path("reports/final_report.html")
    print(result)


async def test_bu_tools():
    import os
    from vibe_surf.tools.browser_use_tools import BrowserUseTools

    tools = BrowserUseTools()
    print(tools.registry.registry.actions.keys())


async def test_vibesurf_tools():
    import os
    from vibe_surf.tools.vibesurf_tools import VibeSurfTools

    tools = VibeSurfTools()
    print(tools.registry.registry.actions.keys())


async def test_finance_tools():
    from vibe_surf.tools.finance_tools import FinanceDataRetriever

    retriever = FinanceDataRetriever('TSLA')

    result = retriever.get_finance_data(["get_news"])

    pdb.set_trace()


if __name__ == '__main__':
    # asyncio.run(test_tools_with_mcp())
    # asyncio.run(test_filesystem())
    # asyncio.run(test_bu_tools())
    # asyncio.run(test_vibesurf_tools())
    asyncio.run(test_finance_tools())



================================================
FILE: tests/test_voice_api.py
================================================
import os
import pdb
import time
import random
import dashscope
import sys

sys.path.append(".")

from dotenv import load_dotenv

load_dotenv()


async def test_qwen3_asr_flash():
    from vibe_surf.tools.voice_asr import QwenASR

    qwen_asr = QwenASR(model="qwen3-asr-flash")
    asr_text = qwen_asr.asr(wav_url="./tmp/voices/qiji-10s.mp3")
    print(asr_text)


async def test_openai_asr_flash():
    from vibe_surf.tools.voice_asr import OpenAIASR

    openai_asr = OpenAIASR()
    asr_text = openai_asr.asr(wav_url="./tmp/voices/qiji-10s.mp3")
    print(asr_text)


async def test_gemini_asr_flash():
    from vibe_surf.tools.voice_asr import GeminiASR

    gemini_asr = GeminiASR()
    asr_text = gemini_asr.asr(wav_url="./tmp/voices/qiji-10s.mp3")
    print(asr_text)


if __name__ == "__main__":
    import asyncio

    # asyncio.run(test_qwen3_asr_flash())
    # asyncio.run(test_openai_asr_flash())
    asyncio.run(test_gemini_asr_flash())



================================================
FILE: vibe_surf/__init__.py
================================================
from dotenv import load_dotenv
import os

try:
    from ._version import version as __version__
except ImportError:
    # Fallback version if _version.py doesn't exist (development mode)
    __version__ = "0.0.0+dev"

project_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))

load_dotenv(os.path.join(project_dir, ".env"))


================================================
FILE: vibe_surf/cli.py
================================================
[Binary file]


================================================
FILE: vibe_surf/common.py
================================================
"""
Common utilities and configurations for VibeSurf.
"""
import os
import platform
from dotenv import load_dotenv

project_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))

load_dotenv(os.path.join(project_dir, ".env"))


def get_workspace_dir():
    """
    Get the workspace directory for VibeSurf.
    
    Returns:
        str: The absolute path to the workspace directory.
    """
    env_workspace_dir = os.getenv("VIBESURF_WORKSPACE", "")
    if not env_workspace_dir or not env_workspace_dir.strip():
        # Set default workspace directory based on OS
        if platform.system() == "Windows":
            default_workspace = os.path.join(os.environ.get("APPDATA", ""), "VibeSurf")
        elif platform.system() == "Darwin":  # macOS
            default_workspace = os.path.join(os.path.expanduser("~"), "Library", "Application Support", "VibeSurf")
        else:  # Linux and others
            default_workspace = os.path.join(os.path.expanduser("~"), ".vibesurf")
        workspace_dir = default_workspace
    else:
        workspace_dir = env_workspace_dir

    workspace_dir = os.path.abspath(workspace_dir)
    os.makedirs(workspace_dir, exist_ok=True)
    return workspace_dir



================================================
FILE: vibe_surf/logger.py
================================================
"""
Logger configuration for VibeSurf.
"""
import logging
import os
from datetime import datetime
from logging.handlers import RotatingFileHandler

from .common import get_workspace_dir


def setup_logger(name: str = "vibesurf") -> logging.Logger:
    """
    Set up and configure the logger for VibeSurf.
    
    Args:
        name (str): Logger name, defaults to "vibesurf"
        
    Returns:
        logging.Logger: Configured logger instance
    """
    # Get debug flag from environment variable
    debug_mode = os.getenv("VIBESURF_DEBUG", "false").lower() in ("true", "1", "yes", "on")
    log_level = logging.DEBUG if debug_mode else logging.INFO
    
    # Create logger
    logger = logging.getLogger(name)
    logger.setLevel(log_level)
    
    # Avoid adding handlers multiple times
    if logger.handlers:
        return logger
    
    # Create formatter with file and line info
    if log_level == logging.DEBUG:
        formatter = logging.Formatter(
            fmt='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(funcName)s() - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
    else:
        formatter = logging.Formatter(
            fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
    
    # Console handler - log to terminal
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler - log to file
    try:
        workspace_dir = get_workspace_dir()
        logs_dir = os.path.join(workspace_dir, "logs")
        os.makedirs(logs_dir, exist_ok=True)
        
        # Create log filename with current date
        current_date = datetime.now().strftime("%Y-%m-%d")
        log_filename = f"log_{current_date}.log"
        log_filepath = os.path.join(logs_dir, log_filename)
        
        # Use RotatingFileHandler to manage log file size
        file_handler = RotatingFileHandler(
            log_filepath,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setLevel(log_level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        logger.info(f"Logger initialized. Log level: {logging.getLevelName(log_level)}")
        logger.info(f"WorkSpace directory: {workspace_dir}")
        logger.info(f"Log file: {log_filepath}")
        
    except Exception as e:
        logger.error(f"Failed to setup file logging: {e}")
        logger.warning("Continuing with console logging only")
    
    return logger


def get_logger(name: str = "vibesurf") -> logging.Logger:
    """
    Get or create a logger instance.
    
    Args:
        name (str): Logger name, defaults to "vibesurf"
        
    Returns:
        logging.Logger: Logger instance
    """
    return setup_logger(name)


# Create default logger instance
default_logger = get_logger()


================================================
FILE: vibe_surf/agents/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/agents/browser_use_agent.py
================================================
import asyncio
import gc
import inspect
import json
import logging
import os.path
import pdb
import re
import sys
import tempfile
import time
from collections.abc import Awaitable, Callable
from datetime import datetime
from pathlib import Path
from typing import Any, Generic, Literal, TypeVar, Optional
from urllib.parse import urlparse

from dotenv import load_dotenv

from browser_use.agent.cloud_events import (
    CreateAgentOutputFileEvent,
    CreateAgentSessionEvent,
    CreateAgentStepEvent,
    CreateAgentTaskEvent,
    UpdateAgentTaskEvent,
)
from browser_use.agent.message_manager.utils import save_conversation
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import BaseMessage, UserMessage
from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.tokens.service import TokenCost

from bubus import EventBus
from pydantic import ValidationError
from uuid_extensions import uuid7str

# Lazy import for gif to avoid heavy agent.views import at startup
# from browser_use.agent.gif import create_history_gif
from browser_use.agent.message_manager.service import (
    MessageManager,
)
from browser_use.agent.prompts import SystemPrompt
from browser_use.agent.views import (
    ActionResult,
    AgentError,
    AgentHistory,
    AgentHistoryList,
    AgentOutput,
    AgentSettings,
    AgentState,
    AgentStepInfo,
    AgentStructuredOutput,
    BrowserStateHistory,
    StepMetadata,
)
from pydantic import BaseModel, ConfigDict, Field, ValidationError, create_model, model_validator
from browser_use import Browser, BrowserProfile, BrowserSession
from browser_use.browser.session import DEFAULT_BROWSER_PROFILE
from browser_use.browser.views import BrowserStateSummary
from browser_use.config import CONFIG
from browser_use.tools.registry.views import ActionModel
from browser_use.tools.service import Controller, Tools
from browser_use.dom.views import DOMInteractedElement
from browser_use.filesystem.file_system import FileSystem
from browser_use.observability import observe, observe_debug
from browser_use.sync import CloudSync
from browser_use.telemetry.service import ProductTelemetry
from browser_use.telemetry.views import AgentTelemetryEvent
from browser_use.utils import (
    _log_pretty_path,
    get_browser_use_version,
    get_git_info,
    time_execution_async,
    time_execution_sync,
)

from browser_use.agent.service import Agent, AgentHookFunc
from vibe_surf.tools.file_system import CustomFileSystem

Context = TypeVar('Context')


class BrowserUseAgent(Agent):
    @time_execution_sync('--init')
    def __init__(
            self,
            task: str,
            llm: BaseChatModel = ChatOpenAI(model='gpt-4.1-mini'),
            # Optional parameters
            browser_profile: BrowserProfile | None = None,
            browser_session: BrowserSession | None = None,
            browser: Browser | None = None,  # Alias for browser_session
            tools: Tools[Context] | None = None,
            controller: Tools[Context] | None = None,  # Alias for tools
            # Initial agent run parameters
            sensitive_data: dict[str, str | dict[str, str]] | None = None,
            initial_actions: list[dict[str, dict[str, Any]]] | None = None,
            # Cloud Callbacks
            register_new_step_callback: (
                    Callable[['BrowserStateSummary', 'AgentOutput', int], None]  # Sync callback
                    | Callable[['BrowserStateSummary', 'AgentOutput', int], Awaitable[None]]  # Async callback
                    | None
            ) = None,
            register_done_callback: (
                    Callable[['AgentHistoryList'], Awaitable[None]]  # Async Callback
                    | Callable[['AgentHistoryList'], None]  # Sync Callback
                    | None
            ) = None,
            register_external_agent_status_raise_error_callback: Callable[[], Awaitable[bool]] | None = None,
            # Agent settings
            output_model_schema: type[AgentStructuredOutput] | None = None,
            use_vision: bool = True,
            save_conversation_path: str | Path | None = None,
            save_conversation_path_encoding: str | None = 'utf-8',
            max_failures: int = 3,
            override_system_message: str | None = None,
            extend_system_message: str | None = None,
            generate_gif: bool | str = False,
            available_file_paths: list[str] | None = None,
            include_attributes: list[str] | None = None,
            max_actions_per_step: int = 10,
            use_thinking: bool = True,
            flash_mode: bool = False,
            max_history_items: int | None = None,
            page_extraction_llm: BaseChatModel | None = None,
            injected_agent_state: AgentState | None = None,
            source: str | None = None,
            file_system_path: str | None = None,
            task_id: str | None = None,
            cloud_sync: CloudSync | None = None,
            calculate_cost: bool = False,
            display_files_in_done_text: bool = True,
            include_tool_call_examples: bool = False,
            vision_detail_level: Literal['auto', 'low', 'high'] = 'auto',
            llm_timeout: int = 90,
            step_timeout: int = 120,
            directly_open_url: bool = False,
            include_recent_events: bool = False,
            allow_parallel_action_types: list[str] = ["extract_structured_data", "extract_content_from_file"],
            _url_shortening_limit: int = 25,
            token_cost_service: Optional[TokenCost] = None,
            **kwargs,
    ):
        if page_extraction_llm is None:
            page_extraction_llm = llm
        if available_file_paths is None:
            available_file_paths = []

        self.id = task_id or uuid7str()
        self.task_id: str = self.id
        self.session_id: str = uuid7str()
        self.allow_parallel_action_types = allow_parallel_action_types
        self._url_shortening_limit = _url_shortening_limit

        browser_profile = browser_profile or DEFAULT_BROWSER_PROFILE

        # Handle browser vs browser_session parameter (browser takes precedence)
        if browser and browser_session:
            raise ValueError(
                'Cannot specify both "browser" and "browser_session" parameters. Use "browser" for the cleaner API.')
        browser_session = browser or browser_session

        self.browser_session = browser_session or BrowserSession(
            browser_profile=browser_profile,
            id=uuid7str()[:-4] + self.id[-4:],  # re-use the same 4-char suffix so they show up together in logs
        )

        # Initialize available file paths as direct attribute
        self.available_file_paths = available_file_paths

        # Core components
        self.task = task
        self.llm = llm
        self.directly_open_url = directly_open_url
        self.include_recent_events = include_recent_events
        if tools is not None:
            self.tools = tools
        elif controller is not None:
            self.tools = controller
        else:
            self.tools = Tools(display_files_in_done_text=display_files_in_done_text)

        # Structured output
        self.output_model_schema = output_model_schema
        if self.output_model_schema is not None:
            self.tools.use_structured_output_action(self.output_model_schema)

        self.sensitive_data = sensitive_data

        self.settings = AgentSettings(
            use_vision=use_vision,
            vision_detail_level=vision_detail_level,
            save_conversation_path=save_conversation_path,
            save_conversation_path_encoding=save_conversation_path_encoding,
            max_failures=max_failures,
            override_system_message=override_system_message,
            extend_system_message=extend_system_message,
            generate_gif=generate_gif,
            include_attributes=include_attributes,
            max_actions_per_step=max_actions_per_step,
            use_thinking=use_thinking,
            flash_mode=flash_mode,
            max_history_items=max_history_items,
            page_extraction_llm=page_extraction_llm,
            calculate_cost=calculate_cost,
            include_tool_call_examples=include_tool_call_examples,
            llm_timeout=llm_timeout,
            step_timeout=step_timeout,
        )

        # Token cost service
        if token_cost_service is None:
            self.token_cost_service = TokenCost(include_cost=calculate_cost)
        else:
            self.token_cost_service = token_cost_service
        self.token_cost_service.register_llm(llm)
        self.token_cost_service.register_llm(page_extraction_llm)

        # Initialize state
        self.state = injected_agent_state or AgentState()

        # Initialize history
        self.history = AgentHistoryList(history=[], usage=None)

        # Initialize agent directory
        import time

        timestamp = int(time.time())
        base_tmp = Path(tempfile.gettempdir())
        self.agent_directory = base_tmp / f'browser_use_agent_{self.id}_{timestamp}'

        # Initialize file system and screenshot service
        self._set_file_system(file_system_path)
        self._set_screenshot_service()

        # Action setup
        self._setup_action_models()
        self._set_browser_use_version_and_source(source)

        initial_url = None

        # only load url if no initial actions are provided
        if self.directly_open_url and not self.state.follow_up_task and not initial_actions:
            initial_url = self._extract_url_from_task(self.task)
            if initial_url:
                self.logger.info(f'ğŸ”— Found URL in task: {initial_url}, adding as initial action...')
                initial_actions = [{'go_to_url': {'url': initial_url, 'new_tab': False}}]

        self.initial_url = initial_url

        self.initial_actions = self._convert_initial_actions(initial_actions) if initial_actions else None
        # Verify we can connect to the model
        self._verify_and_setup_llm()

        # TODO: move this logic to the LLMs
        # Handle users trying to use use_vision=True with DeepSeek models
        if 'deepseek' in self.llm.model.lower():
            self.logger.warning(
                'âš ï¸ DeepSeek models do not support use_vision=True yet. Setting use_vision=False for now...')
            self.settings.use_vision = False

        if 'kimi-k2' in self.llm.model.lower():
            self.logger.warning(
                'âš ï¸ Kimi-k2 models do not support use_vision=True yet. Setting use_vision=False for now...')
            self.settings.use_vision = False

        if "qwen" in self.llm.model.lower() and "vl" not in self.llm.model.lower():
            self.logger.warning("âš ï¸ Qwen without VL doesn't support vision. Ignore Vision input.")
            self.settings.use_vision = False

        # Handle users trying to use use_vision=True with XAI models
        if 'grok' in self.llm.model.lower():
            self.logger.warning('âš ï¸ XAI models do not support use_vision=True yet. Setting use_vision=False for now...')
            self.settings.use_vision = False

        self.logger.info(f'ğŸ§  Starting a browser-use version {self.version} with model={self.llm.model}')
        self.logger.info(
            f'{" +vision" if self.settings.use_vision else ""}'
            f' extraction_model={self.settings.page_extraction_llm.model if self.settings.page_extraction_llm else "Unknown"}'
            f'{" +file_system" if self.file_system else ""}'
        )

        # Initialize available actions for system prompt (only non-filtered actions)
        # These will be used for the system prompt to maintain caching
        self.unfiltered_actions = self.tools.registry.get_prompt_description()

        # Initialize message manager with state
        # Initial system prompt with all actions - will be updated during each step
        self._message_manager = MessageManager(
            task=task,
            system_message=SystemPrompt(
                action_description=self.unfiltered_actions,
                max_actions_per_step=self.settings.max_actions_per_step,
                override_system_message=override_system_message,
                extend_system_message=extend_system_message,
                use_thinking=self.settings.use_thinking,
                flash_mode=self.settings.flash_mode,
            ).get_system_message(),
            file_system=self.file_system,
            state=self.state.message_manager_state,
            use_thinking=self.settings.use_thinking,
            # Settings that were previously in MessageManagerSettings
            include_attributes=self.settings.include_attributes,
            sensitive_data=sensitive_data,
            max_history_items=self.settings.max_history_items,
            vision_detail_level=self.settings.vision_detail_level,
            include_tool_call_examples=self.settings.include_tool_call_examples,
            include_recent_events=self.include_recent_events,
        )

        if self.sensitive_data:
            # Check if sensitive_data has domain-specific credentials
            has_domain_specific_credentials = any(isinstance(v, dict) for v in self.sensitive_data.values())

            # If no allowed_domains are configured, show a security warning
            if not self.browser_profile.allowed_domains:
                self.logger.error(
                    'âš ï¸âš ï¸âš ï¸ Agent(sensitive_data=â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢) was provided but BrowserSession(allowed_domains=[...]) is not locked down! âš ï¸âš ï¸âš ï¸\n'
                    '          â˜ ï¸ If the agent visits a malicious website and encounters a prompt-injection attack, your sensitive_data may be exposed!\n\n'
                    '             https://docs.browser-use.com/customize/browser-settings#restrict-urls\n'
                    'Waiting 10 seconds before continuing... Press [Ctrl+C] to abort.'
                )
                if sys.stdin.isatty():
                    try:
                        time.sleep(10)
                    except KeyboardInterrupt:
                        print(
                            '\n\n ğŸ›‘ Exiting now... set BrowserSession(allowed_domains=["example.com", "example.org"]) to only domains you trust to see your sensitive_data.'
                        )
                        sys.exit(0)
                else:
                    pass  # no point waiting if we're not in an interactive shell
                self.logger.warning(
                    'â€¼ï¸ Continuing with insecure settings for now... but this will become a hard error in the future!'
                )

            # If we're using domain-specific credentials, validate domain patterns
            elif has_domain_specific_credentials:
                # For domain-specific format, ensure all domain patterns are included in allowed_domains
                domain_patterns = [k for k, v in self.sensitive_data.items() if isinstance(v, dict)]

                # Validate each domain pattern against allowed_domains
                for domain_pattern in domain_patterns:
                    is_allowed = False
                    for allowed_domain in self.browser_profile.allowed_domains:
                        # Special cases that don't require URL matching
                        if domain_pattern == allowed_domain or allowed_domain == '*':
                            is_allowed = True
                            break

                        # Need to create example URLs to compare the patterns
                        # Extract the domain parts, ignoring scheme
                        pattern_domain = domain_pattern.split('://')[-1] if '://' in domain_pattern else domain_pattern
                        allowed_domain_part = allowed_domain.split('://')[
                            -1] if '://' in allowed_domain else allowed_domain

                        # Check if pattern is covered by an allowed domain
                        # Example: "google.com" is covered by "*.google.com"
                        if pattern_domain == allowed_domain_part or (
                                allowed_domain_part.startswith('*.')
                                and (
                                        pattern_domain == allowed_domain_part[2:]
                                        or pattern_domain.endswith('.' + allowed_domain_part[2:])
                                )
                        ):
                            is_allowed = True
                            break

                    if not is_allowed:
                        self.logger.warning(
                            f'âš ï¸ Domain pattern "{domain_pattern}" in sensitive_data is not covered by any pattern in allowed_domains={self.browser_profile.allowed_domains}\n'
                            f'   This may be a security risk as credentials could be used on unintended domains.'
                        )

        # Callbacks
        self.register_new_step_callback = register_new_step_callback
        self.register_done_callback = register_done_callback
        self.register_external_agent_status_raise_error_callback = register_external_agent_status_raise_error_callback

        # Telemetry
        self.telemetry = ProductTelemetry()

        if self.settings.save_conversation_path:
            self.settings.save_conversation_path = Path(self.settings.save_conversation_path).expanduser().resolve()
            self.logger.info(f'ğŸ’¬ Saving conversation to {_log_pretty_path(self.settings.save_conversation_path)}')

        # Initialize download tracking
        assert self.browser_session is not None, 'BrowserSession is not set up'
        self.has_downloads_path = self.browser_session.browser_profile.downloads_path is not None
        if self.has_downloads_path:
            self._last_known_downloads: list[str] = []
            self.logger.debug('ğŸ“ Initialized download tracking for agent')

        self._external_pause_event = asyncio.Event()
        self._external_pause_event.set()

    def _set_file_system(self, file_system_path: str | None = None) -> None:
        # Check for conflicting parameters
        if self.state.file_system_state and file_system_path:
            raise ValueError(
                'Cannot provide both file_system_state (from agent state) and file_system_path. '
                'Either restore from existing state or create new file system at specified path, not both.'
            )

        # Check if we should restore from existing state first
        if self.state.file_system_state:
            try:
                # Restore file system from state at the exact same location
                self.file_system = CustomFileSystem.from_state(self.state.file_system_state)
                # The parent directory of base_dir is the original file_system_path
                self.file_system_path = str(self.file_system.base_dir)
                self.logger.debug(f'ğŸ’¾ File system restored from state to: {self.file_system_path}')
                return
            except Exception as e:
                self.logger.error(f'ğŸ’¾ Failed to restore file system from state: {e}')
                raise e

        # Initialize new file system
        try:
            if file_system_path:
                self.file_system = CustomFileSystem(file_system_path)
                self.file_system_path = file_system_path
            else:
                # Use the agent directory for file system
                self.file_system = CustomFileSystem(self.agent_directory)
                self.file_system_path = str(self.agent_directory)
        except Exception as e:
            self.logger.error(f'ğŸ’¾ Failed to initialize file system: {e}.')
            raise e

        # Save file system state to agent state
        self.state.file_system_state = self.file_system.get_state()

        self.logger.debug(f'ğŸ’¾ File system path: {self.file_system_path}')

    @property
    def logger(self) -> logging.Logger:
        """Get instance-specific logger with task ID and browser session info"""
        # Update target ID dynamically if available
        _browser_session_id = self.browser_session.id if self.browser_session else self.id
        _current_target_id = (
            self.browser_session.agent_focus.target_id[-4:]
            if self.browser_session and hasattr(self.browser_session,
                                                'agent_focus') and self.browser_session.agent_focus and hasattr(
                self.browser_session.agent_focus, 'target_id')
            else '--'
        )
        return logging.getLogger(
            f'browser-use.Agent:{self.task_id[-4:]} on target:{_current_target_id} of browser:{_browser_session_id[-4:]}')

    async def _finalize(self, browser_state_summary: BrowserStateSummary | None) -> None:
        """Finalize the step with history, logging, and events"""
        step_end_time = time.time()
        if not self.state.last_result:
            return

        if browser_state_summary:
            metadata = StepMetadata(
                step_number=self.state.n_steps,
                step_start_time=self.step_start_time,
                step_end_time=step_end_time,
            )

            # Use _make_history_item like main branch
            await self._make_history_item(self.state.last_model_output, browser_state_summary, self.state.last_result,
                                          metadata)

        # Log step completion summary
        self._log_step_completion_summary(self.step_start_time, self.state.last_result)

        # Save file system state after step completion
        self.save_file_system_state()

        # Emit both step created and executed events
        if browser_state_summary and self.state.last_model_output:
            # Extract key step data for the event
            actions_data = []
            if self.state.last_model_output.action:
                for action in self.state.last_model_output.action:
                    action_dict = action.model_dump() if hasattr(action, 'model_dump') else {}
                    actions_data.append(action_dict)

        # Increment step counter after step is fully completed
        self.state.n_steps += 1

    def add_new_task(self, new_task: str) -> None:
        """Add a new task to the agent, keeping the same task_id as tasks are continuous"""
        # Simply delegate to message manager - no need for new task_id or events
        # The task continues with new instructions, it doesn't end and start a new one
        self.task = new_task
        self._message_manager.add_new_task(new_task)

    @observe(name='agent.run', metadata={'task': '{{task}}', 'debug': '{{debug}}'})
    @time_execution_async('--run')
    async def run(
            self,
            max_steps: int = 100,
            on_step_start: AgentHookFunc | None = None,
            on_step_end: AgentHookFunc | None = None,
    ) -> AgentHistoryList[AgentStructuredOutput]:
        """Execute the task with maximum number of steps"""

        loop = asyncio.get_event_loop()
        agent_run_error: str | None = None  # Initialize error tracking variable
        self._force_exit_telemetry_logged = False  # ADDED: Flag for custom telemetry on force exit

        # Set up the  signal handler with callbacks specific to this agent
        from browser_use.utils import SignalHandler

        # Define the custom exit callback function for second CTRL+C
        def on_force_exit_log_telemetry():
            self._log_agent_event(max_steps=max_steps, agent_run_error='SIGINT: Cancelled by user')
            # NEW: Call the flush method on the telemetry instance
            if hasattr(self, 'telemetry') and self.telemetry:
                self.telemetry.flush()
            self._force_exit_telemetry_logged = True  # Set the flag

        signal_handler = SignalHandler(
            loop=loop,
            pause_callback=self.pause,
            resume_callback=self.resume,
            custom_exit_callback=on_force_exit_log_telemetry,  # Pass the new telemetrycallback
            exit_on_second_int=True,
        )
        signal_handler.register()

        try:
            await self._log_agent_run()

            self.logger.debug(
                f'ğŸ”§ Agent setup: Task ID {self.task_id[-4:]}, Session ID {self.session_id[-4:]}, Browser Session ID {self.browser_session.id[-4:] if self.browser_session else "None"}'
            )

            # Initialize timing for session and task
            self._session_start_time = time.time()
            self._task_start_time = self._session_start_time  # Initialize task start time

            self.logger.debug('ğŸ”§ Browser session started with watchdogs attached')

            # Execute initial actions if provided
            if self.initial_actions:
                self.logger.debug(f'âš¡ Executing {len(self.initial_actions)} initial actions...')
                result = await self.multi_act(self.initial_actions, check_for_new_elements=False)
                self.state.last_result = result
                self.logger.debug('âœ… Initial actions completed')

            self.logger.debug(f'ğŸ”„ Starting main execution loop with max {max_steps} steps...')
            for step in range(max_steps):
                # Replace the polling with clean pause-wait
                if self.state.paused:
                    self.logger.debug(f'â¸ï¸ Step {step}: Agent paused, waiting to resume...')
                    await self._external_pause_event.wait()
                    signal_handler.reset()

                # Check if we should stop due to too many failures
                if (self.state.consecutive_failures) >= self.settings.max_failures + int(
                        self.settings.final_response_after_failure
                ):
                    self.logger.error(f'âŒ Stopping due to {self.settings.max_failures} consecutive failures')
                    agent_run_error = f'Stopped due to {self.settings.max_failures} consecutive failures'
                    break

                # Check control flags before each step
                if self.state.stopped:
                    self.logger.info('ğŸ›‘ Agent stopped')
                    agent_run_error = 'Agent stopped programmatically'
                    break

                if on_step_start is not None:
                    await on_step_start(self)

                self.logger.debug(f'ğŸš¶ Starting step {step + 1}/{max_steps}...')
                step_info = AgentStepInfo(step_number=step, max_steps=max_steps)

                try:
                    await asyncio.wait_for(
                        self.step(step_info),
                        timeout=self.settings.step_timeout,
                    )
                    self.logger.debug(f'âœ… Completed step {step + 1}/{max_steps}')
                except TimeoutError:
                    # Handle step timeout gracefully
                    error_msg = f'Step {step + 1} timed out after {self.settings.step_timeout} seconds'
                    self.logger.error(f'â° {error_msg}')
                    self.state.consecutive_failures += 1
                    self.state.last_result = [ActionResult(error=error_msg)]

                if on_step_end is not None:
                    await on_step_end(self)

                if self.history.is_done():
                    self.logger.debug(f'ğŸ¯ Task completed after {step + 1} steps!')
                    await self.log_completion()

                    if self.register_done_callback:
                        if inspect.iscoroutinefunction(self.register_done_callback):
                            await self.register_done_callback(self.history)
                        else:
                            self.register_done_callback(self.history)

                    # Task completed
                    break
            else:
                agent_run_error = 'Failed to complete task in maximum steps'

                self.history.add_item(
                    AgentHistory(
                        model_output=None,
                        result=[ActionResult(error=agent_run_error, include_in_memory=True)],
                        state=BrowserStateHistory(
                            url='',
                            title='',
                            tabs=[],
                            interacted_element=[],
                            screenshot_path=None,
                        ),
                        metadata=None,
                    )
                )

                self.logger.info(f'âŒ {agent_run_error}')

            self.logger.debug('ğŸ“Š Collecting usage summary...')
            self.history.usage = await self.token_cost_service.get_usage_summary()

            # set the model output schema and call it on the fly
            if self.history._output_model_schema is None and self.output_model_schema is not None:
                self.history._output_model_schema = self.output_model_schema

            self.logger.debug('ğŸ Agent.run() completed successfully')
            return self.history

        except KeyboardInterrupt:
            # Already handled by our signal handler, but catch any direct KeyboardInterrupt as well
            self.logger.debug('Got KeyboardInterrupt during execution, returning current history')
            agent_run_error = 'KeyboardInterrupt'

            self.history.usage = await self.token_cost_service.get_usage_summary()

            return self.history

        except Exception as e:
            self.logger.error(f'Agent run failed with exception: {e}', exc_info=True)
            agent_run_error = str(e)
            raise e

        finally:
            # Log token usage summary
            await self.token_cost_service.log_usage_summary()

            self.save_history(os.path.join(self.file_system_path, 'AgentHistory.json'))

            # Unregister signal handlers before cleanup
            signal_handler.unregister()

            if not self._force_exit_telemetry_logged:  # MODIFIED: Check the flag
                try:
                    self._log_agent_event(max_steps=max_steps, agent_run_error=agent_run_error)
                except Exception as log_e:  # Catch potential errors during logging itself
                    self.logger.error(f'Failed to log telemetry event: {log_e}', exc_info=True)
            else:
                # ADDED: Info message when custom telemetry for SIGINT was already logged
                self.logger.debug('Telemetry for force exit (SIGINT) was logged by custom exit callback.')

            # Generate GIF if needed before stopping event bus
            if self.settings.generate_gif:
                output_path: str = 'agent_history.gif'
                if isinstance(self.settings.generate_gif, str):
                    output_path = self.settings.generate_gif

                # Lazy import gif module to avoid heavy startup cost
                from browser_use.agent.gif import create_history_gif

                create_history_gif(task=self.task, history=self.history, output_path=output_path)

            await self.close()

    def _matches_action_type(self, action_type: str, allowed_pattern: str) -> bool:
        """
        Check if an action type matches an allowed pattern, supporting wildcards.
        
        Args:
            action_type: The actual action type (e.g., "mcp.filesystem.read_file")
            allowed_pattern: The pattern to match (e.g., "mcp.filesystem*")
            
        Returns:
            True if the action type matches the pattern
        """
        if allowed_pattern.endswith('*'):
            # Wildcard matching
            prefix = allowed_pattern[:-1]
            return action_type.startswith(prefix)
        else:
            # Exact matching
            return action_type == allowed_pattern

    def _is_action_parallel_allowed(self, action: ActionModel) -> bool:
        """
        Check if an action is allowed to be executed in parallel.
        
        Args:
            action: The action to check
            
        Returns:
            True if the action can be executed in parallel
        """
        action_data = action.model_dump(exclude_unset=True)
        action_type = next(iter(action_data.keys())) if action_data else None

        if not action_type:
            return False

        for allowed_pattern in self.allow_parallel_action_types:
            if self._matches_action_type(action_type, allowed_pattern):
                return True

        return False

    def _group_actions_for_parallel_execution(self, actions: list[ActionModel]) -> list[list[ActionModel]]:
        """
        Group consecutive actions that can be executed in parallel.
        
        Args:
            actions: List of actions to group
            
        Returns:
            List of action groups, where each group can be executed in parallel
        """
        if not actions:
            return []

        groups = []
        current_group = [actions[0]]

        for i in range(1, len(actions)):
            current_action = actions[i]
            previous_action = actions[i - 1]

            # Check if both current and previous actions can be executed in parallel
            if (self._is_action_parallel_allowed(current_action) and
                    self._is_action_parallel_allowed(previous_action)):
                # Add to current group
                current_group.append(current_action)
            else:
                # Start a new group
                groups.append(current_group)
                current_group = [current_action]

        # Add the last group
        groups.append(current_group)

        return groups

    @observe_debug(ignore_input=True, ignore_output=True)
    @time_execution_async('--multi_act')
    async def multi_act(
            self,
            actions: list[ActionModel],
            check_for_new_elements: bool = True,
    ) -> list[ActionResult]:
        """Execute multiple actions, with parallel execution for allowed action types"""
        results: list[ActionResult] = []
        time_elapsed = 0
        total_actions = len(actions)

        assert self.browser_session is not None, 'BrowserSession is not set up'
        try:
            if (
                    self.browser_session._cached_browser_state_summary is not None
                    and self.browser_session._cached_browser_state_summary.dom_state is not None
            ):
                cached_selector_map = dict(self.browser_session._cached_browser_state_summary.dom_state.selector_map)
                cached_element_hashes = {e.parent_branch_hash() for e in cached_selector_map.values()}
            else:
                cached_selector_map = {}
                cached_element_hashes = set()
        except Exception as e:
            self.logger.error(f'Error getting cached selector map: {e}')
            cached_selector_map = {}
            cached_element_hashes = set()

        # Group actions for potential parallel execution
        action_groups = self._group_actions_for_parallel_execution(actions)

        # Track global action index for logging and DOM checks
        global_action_index = 0

        for group_index, action_group in enumerate(action_groups):
            group_size = len(action_group)

            # Check if this group can be executed in parallel
            can_execute_in_parallel = (
                    group_size > 1 and
                    all(self._is_action_parallel_allowed(action) for action in action_group)
            )

            if can_execute_in_parallel:
                self.logger.info(
                    f'ğŸš€ Executing {group_size} actions in parallel: group {group_index + 1}/{len(action_groups)}')
                # Execute actions in parallel using asyncio.gather
                parallel_results = await self._execute_actions_in_parallel(
                    action_group, global_action_index, total_actions,
                    cached_selector_map, cached_element_hashes, check_for_new_elements
                )
                results.extend(parallel_results)
                global_action_index += group_size

                # Check if any result indicates completion or error
                if any(result.is_done or result.error for result in parallel_results):
                    break
            else:
                # Execute actions sequentially
                for local_index, action in enumerate(action_group):
                    i = global_action_index + local_index

                    # Original sequential execution logic continues here...
                    # if i > 0:
                    #     # ONLY ALLOW TO CALL `done` IF IT IS A SINGLE ACTION
                    #     if action.model_dump(exclude_unset=True).get('done') is not None:
                    #         msg = f'Done action is allowed only as a single action - stopped after action {i} / {total_actions}.'
                    #         self.logger.debug(msg)
                    #         break

                    # DOM synchronization check - verify element indexes are still valid AFTER first action
                    if action.get_index() is not None and i != 0:
                        result = await self._check_dom_synchronization(
                            action, i, total_actions, cached_selector_map, cached_element_hashes,
                            check_for_new_elements, actions
                        )
                        if result:
                            results.append(result)
                            break

                    # wait between actions (only after first action)
                    if i > 0:
                        await asyncio.sleep(self.browser_profile.wait_between_actions)

                    # Execute single action
                    try:
                        action_result = await self._execute_single_action(action, i, total_actions)
                        results.append(action_result)

                        if action_result.is_done or action_result.error or i == total_actions - 1:
                            break

                    except Exception as e:
                        self.logger.error(f'âŒ Executing action {i + 1} failed: {type(e).__name__}: {e}')
                        raise e

                global_action_index += len(action_group)

        return results

    async def _execute_actions_in_parallel(
            self,
            actions: list[ActionModel],
            start_index: int,
            total_actions: int,
            cached_selector_map: dict,
            cached_element_hashes: set,
            check_for_new_elements: bool
    ) -> list[ActionResult]:
        """Execute a group of actions in parallel using asyncio.gather"""

        async def execute_single_parallel_action(action: ActionModel, action_index: int) -> ActionResult:
            """Execute a single action for parallel execution"""
            await self._raise_if_stopped_or_paused()

            # Get action info for logging
            action_data = action.model_dump(exclude_unset=True)
            action_name = next(iter(action_data.keys())) if action_data else 'unknown'
            action_params = getattr(action, action_name, '') or str(action.model_dump(mode='json'))[:140].replace(
                '"', ''
            ).replace('{', '').replace('}', '').replace("'", '').strip().strip(',')
            action_params = str(action_params)
            action_params = f'{action_params[:122]}...' if len(action_params) > 128 else action_params

            time_start = time.time()
            blue = '\033[34m'
            reset = '\033[0m'
            self.logger.info(f'  ğŸ¦¾ {blue}[PARALLEL ACTION {action_index + 1}/{total_actions}]{reset} {action_params}')

            # Execute the action
            result = await self.tools.act(
                action=action,
                browser_session=self.browser_session,
                file_system=self.file_system,
                page_extraction_llm=self.settings.page_extraction_llm,
                sensitive_data=self.sensitive_data,
                available_file_paths=self.available_file_paths,
            )

            time_end = time.time()
            time_elapsed = time_end - time_start

            green = '\033[92m'
            self.logger.debug(
                f'â˜‘ï¸ Parallel action {action_index + 1}/{total_actions}: {green}{action_params}{reset} in {time_elapsed:.2f}s'
            )

            return result

        # Create tasks for parallel execution
        tasks = [
            execute_single_parallel_action(action, start_index + i)
            for i, action in enumerate(actions)
        ]

        # Execute all tasks in parallel
        parallel_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and handle any exceptions
        processed_results = []
        for i, result in enumerate(parallel_results):
            if isinstance(result, Exception):
                action_index = start_index + i
                self.logger.error(f'âŒ Parallel action {action_index + 1} failed: {type(result).__name__}: {result}')
                raise result
            else:
                processed_results.append(result)

        return processed_results

    async def _check_dom_synchronization(
            self,
            action: ActionModel,
            action_index: int,
            total_actions: int,
            cached_selector_map: dict,
            cached_element_hashes: set,
            check_for_new_elements: bool,
            all_actions: list[ActionModel]
    ) -> ActionResult | None:
        """Check DOM synchronization and return result if page changed"""
        new_browser_state_summary = await self.browser_session.get_browser_state_summary(
            cache_clickable_elements_hashes=False,
            include_screenshot=False,
        )
        new_selector_map = new_browser_state_summary.dom_state.selector_map

        # Detect index change after previous action
        orig_target = cached_selector_map.get(action.get_index())
        orig_target_hash = orig_target.parent_branch_hash() if orig_target else None

        new_target = new_selector_map.get(action.get_index())  # type: ignore
        new_target_hash = new_target.parent_branch_hash() if new_target else None

        def get_remaining_actions_str(actions: list[ActionModel], index: int) -> str:
            remaining_actions = []
            for remaining_action in actions[index:]:
                action_data = remaining_action.model_dump(exclude_unset=True)
                action_name = next(iter(action_data.keys())) if action_data else 'unknown'
                remaining_actions.append(action_name)
            return ', '.join(remaining_actions)

        if orig_target_hash != new_target_hash:
            # Get names of remaining actions that won't be executed
            remaining_actions_str = get_remaining_actions_str(all_actions, action_index)
            msg = f'Page changed after action {action_index} / {total_actions}: actions {remaining_actions_str} were not executed'
            self.logger.info(msg)
            return ActionResult(
                extracted_content=msg,
                include_in_memory=True,
                long_term_memory=msg,
            )

        # Check for new elements that appeared
        new_element_hashes = {e.parent_branch_hash() for e in new_selector_map.values()}
        if check_for_new_elements and not new_element_hashes.issubset(cached_element_hashes):
            # next action requires index but there are new elements on the page
            remaining_actions_str = get_remaining_actions_str(all_actions, action_index)
            msg = f'Something new appeared after action {action_index} / {total_actions}: actions {remaining_actions_str} were not executed'
            self.logger.info(msg)
            return ActionResult(
                extracted_content=msg,
                include_in_memory=True,
                long_term_memory=msg,
            )

        return None

    async def _execute_single_action(self, action: ActionModel, action_index: int, total_actions: int) -> ActionResult:
        """Execute a single action in sequential mode"""
        await self._raise_if_stopped_or_paused()

        # Get action name from the action model
        action_data = action.model_dump(exclude_unset=True)
        action_name = next(iter(action_data.keys())) if action_data else 'unknown'
        action_params = getattr(action, action_name, '') or str(action.model_dump(mode='json'))[:140].replace(
            '"', ''
        ).replace('{', '').replace('}', '').replace("'", '').strip().strip(',')
        # Ensure action_params is always a string before checking length
        action_params = str(action_params)
        action_params = f'{action_params[:122]}...' if len(action_params) > 128 else action_params

        time_start = time.time()

        red = '\033[91m'
        green = '\033[92m'
        blue = '\033[34m'
        reset = '\033[0m'

        self.logger.info(f'  ğŸ¦¾ {blue}[ACTION {action_index + 1}/{total_actions}]{reset} {action_params}')

        result = await self.tools.act(
            action=action,
            browser_session=self.browser_session,
            file_system=self.file_system,
            page_extraction_llm=self.settings.page_extraction_llm,
            sensitive_data=self.sensitive_data,
            available_file_paths=self.available_file_paths,
        )

        time_end = time.time()
        time_elapsed = time_end - time_start

        self.logger.debug(
            f'â˜‘ï¸ Executed action {action_index + 1}/{total_actions}: {green}{action_params}{reset} in {time_elapsed:.2f}s'
        )

        return result



================================================
FILE: vibe_surf/agents/report_writer_agent.py
================================================
import logging
import os
import time
import re
import asyncio
from datetime import datetime
from typing import Any, Dict, List
import json

from pydantic import BaseModel
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import UserMessage, SystemMessage, AssistantMessage
from browser_use.utils import SignalHandler

from vibe_surf.agents.prompts.report_writer_prompt import REPORT_WRITER_PROMPT
from vibe_surf.tools.file_system import CustomFileSystem
from vibe_surf.tools.report_writer_tools import ReportWriterTools
from vibe_surf.agents.views import CustomAgentOutput

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class ReportTaskResult(BaseModel):
    """Result of a report generation task"""
    success: bool  # True only if LLM completed successfully
    msg: str  # Success message or error details
    report_path: str  # Path to the generated report file


class ReportWriterAgent:
    """Agent responsible for generating HTML reports using LLM-controlled flow"""

    def __init__(self, llm: BaseChatModel, workspace_dir: str, step_callback=None, use_thinking: bool = True):
        """
        Initialize ReportWriterAgent
        
        Args:
            llm: Language model for generating report content
            workspace_dir: Directory to save reports
            step_callback: Optional callback function to log each step
        """
        self.llm = llm
        self.workspace_dir = os.path.abspath(workspace_dir)
        self.step_callback = step_callback
        self.use_thinking = use_thinking

        # Initialize file system and tools
        self.file_system = CustomFileSystem(self.workspace_dir)
        self.tools = ReportWriterTools()

        # Setup action model and agent output
        self.ActionModel = self.tools.registry.create_action_model()
        if self.use_thinking:
            self.AgentOutput = CustomAgentOutput.type_with_custom_actions(self.ActionModel)
        else:
            self.AgentOutput = CustomAgentOutput.type_with_custom_actions_no_thinking(self.ActionModel)

        # State management for pause/resume/stop control
        self.paused = False
        self.stopped = False
        self.consecutive_failures = 0
        self._external_pause_event = asyncio.Event()
        self._external_pause_event.set()
        
        # Initialize message history as instance variable
        self.message_history = []

        logger.info("ğŸ“„ ReportWriterAgent initialized with LLM-controlled flow")

    def pause(self) -> None:
        """Pause the agent before the next step"""
        logger.info('\n\nâ¸ï¸ Paused report writer agent.\n\tPress [Enter] to resume or [Ctrl+C] again to quit.')
        self.paused = True
        self._external_pause_event.clear()

    def resume(self) -> None:
        """Resume the agent"""
        logger.info('â–¶ï¸  Resuming report writer agent execution where it left off...\n')
        self.paused = False
        self._external_pause_event.set()

    def stop(self) -> None:
        """Stop the agent"""
        logger.info('â¹ï¸ Report writer agent stopping')
        self.stopped = True
        # Signal pause event to unblock any waiting code so it can check the stopped state
        self._external_pause_event.set()

    def add_new_task(self, new_task: str) -> None:
        """
        Add a new task or guidance to the report writer agent during execution.
        The new_task parameter contains a pre-formatted prompt from VibeSurfAgent.
        """
        # Add the pre-formatted prompt directly to message history
        from browser_use.llm.messages import UserMessage
        self.message_history.append(UserMessage(content=new_task))
        logger.info(f"ğŸ“ Report writer agent received new task guidance")

    async def generate_report(self, report_data: Dict[str, Any]) -> ReportTaskResult:
        """
        Generate HTML report using LLM-controlled flow
        
        Args:
            report_data: Dictionary containing:
                - report_task: Report requirements, tips, and possible insights
                - information: Collected information for the report
        
        Returns:
            ReportTaskResult: Result containing success status, message, and report path
        """
        logger.info("ğŸ“ Starting LLM-controlled report generation...")

        # Get current event loop
        loop = asyncio.get_event_loop()

        signal_handler = SignalHandler(
            loop=loop,
            pause_callback=self.pause,
            resume_callback=self.resume,
            exit_on_second_int=True,
        )
        signal_handler.register()

        try:
            # Extract task and information
            report_task = report_data.get('report_task', 'Generate a comprehensive report')
            report_information = report_data.get('report_information', 'No additional information provided')

            # Create report file with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_filename = f"reports/report-{timestamp}.html"

            # Create the report file
            create_result = await self.file_system.create_file(report_filename)
            logger.info(f"Created report file: {create_result}")

            max_iterations = 10  # Prevent infinite loops

            # Add system message with unified prompt only if message history is empty
            if not self.message_history:
                self.message_history.append(SystemMessage(content=REPORT_WRITER_PROMPT))

            # Add initial user message with task details
            user_message = f"""Please generate a report within MAX {max_iterations} steps based on the following:

**Report Task:**
{report_task}

**Available Information:**
{json.dumps(report_information, indent=2, ensure_ascii=False)}

**Report File:**
{report_filename}

The report file '{report_filename}' has been created and is ready for you to write content.
Please analyze the task, determine if you need to read any additional files, then generate the complete report content and format it as professional HTML.
"""
            self.message_history.append(UserMessage(content=user_message))

            # LLM-controlled loop
            iteration = 0
            agent_run_error = None
            task_completed = False

            while iteration < max_iterations:
                # Use the consolidated pause state management
                if self.paused:
                    logger.info(f'â¸ï¸ Step {iteration}: Agent paused, waiting to resume...')
                    await self._external_pause_event.wait()
                    signal_handler.reset()

                # Check control flags before each step
                if self.stopped:
                    logger.info('ğŸ›‘ Agent stopped')
                    agent_run_error = 'Agent stopped programmatically because user interrupted.'
                    break
                iteration += 1
                logger.info(f"ğŸ”„ LLM iteration {iteration}")
                self.message_history.append(UserMessage(content=f"Current step: {iteration} / {max_iterations}"))
                # Get LLM response
                response = await self.llm.ainvoke(self.message_history, output_format=self.AgentOutput)
                parsed = response.completion
                actions = parsed.action

                # Call step callback if provided to log thinking + action
                if self.step_callback:
                    await self.step_callback(parsed, iteration)

                # Add assistant message to history
                self.message_history.append(AssistantMessage(
                    content=json.dumps(response.completion.model_dump(exclude_none=True, exclude_unset=True),
                                       ensure_ascii=False)))

                # Execute actions
                results = []
                time_start = time.time()

                for i, action in enumerate(actions):
                    action_data = action.model_dump(exclude_unset=True)
                    action_name = next(iter(action_data.keys())) if action_data else 'unknown'
                    logger.info(f"ğŸ› ï¸ Executing action {i + 1}/{len(actions)}: {action_name}")

                    result = await self.tools.act(
                        action=action,
                        file_system=self.file_system,
                        llm=self.llm,
                    )

                    time_end = time.time()
                    time_elapsed = time_end - time_start
                    results.append(result)

                    logger.info(f"âœ… Action completed in {time_elapsed:.2f}s")

                    # Check if task is done
                    if action_name == 'task_done':
                        logger.info("ğŸ‰ Report Writing Task completed")
                        task_completed = True
                        break

                # Check if task is done - break out of main loop if task completed
                if task_completed:
                    break

                # Add results to message history using improved action result processing
                action_results = ''
                for idx, action_result in enumerate(results):
                    if hasattr(action_result, 'extracted_content') and action_result.extracted_content:
                        action_results += f'{action_result.extracted_content}\n'
                        logger.debug(f'Added extracted_content to action_results: {action_result.extracted_content}')

                    if hasattr(action_result, 'error') and action_result.error:
                        if len(action_result.error) > 200:
                            error_text = action_result.error[:100] + '......' + action_result.error[-100:]
                        else:
                            error_text = action_result.error
                        action_results += f'{error_text}\n'
                        logger.debug(f'Added error to action_results: {error_text}')

                if action_results:
                    formatted_results = f'Result:\n{action_results}'
                    self.message_history.append(UserMessage(content=formatted_results))

                # If no progress, add a prompt to continue
                if not results:
                    self.message_history.append(UserMessage(content="Please continue with the report generation."))

            # Handle different completion scenarios
            report_path = await self._finalize_report(report_filename)
            
            if agent_run_error:
                # Agent was stopped
                return ReportTaskResult(
                    success=False,
                    msg=agent_run_error,
                    report_path=report_path
                )
            elif task_completed:
                # Task completed successfully by LLM
                logger.info(f"âœ… Report generated successfully: {report_path}")
                return ReportTaskResult(
                    success=True,
                    msg="Report generated successfully by LLM",
                    report_path=report_path
                )
            elif iteration >= max_iterations:
                # Maximum iterations reached
                logger.warning("âš ï¸ Maximum iterations reached, finishing report generation")
                return ReportTaskResult(
                    success=False,
                    msg="Maximum iterations reached without task completion",
                    report_path=report_path
                )
            else:
                # Unexpected exit from loop
                return ReportTaskResult(
                    success=False,
                    msg="Report generation ended unexpectedly",
                    report_path=report_path
                )

        except Exception as e:
            logger.error(f"âŒ Failed to generate report: {e}")
            # Generate a simple fallback report
            fallback_path = await self._generate_fallback_report(report_data)
            return ReportTaskResult(
                success=False,
                msg=f"Error occurred during report generation: {str(e)}",
                report_path=fallback_path
            )
        finally:
            signal_handler.unregister()
            self.stopped = False
            self.paused = False

    async def _finalize_report(self, report_filename: str) -> str:
        """
        Finalize the report by cleaning HTML and converting links
        
        Args:
            report_filename: Name of the report file
            
        Returns:
            str: Absolute path to the finalized report
        """
        try:
            # Read the current content
            content = await self.file_system.read_file(report_filename)

            # Extract HTML content from the read result
            if content.startswith('Read from file'):
                # Extract content between <content> tags
                start_tag = '<content>'
                end_tag = '</content>'
                start_idx = content.find(start_tag)
                end_idx = content.find(end_tag)

                if start_idx != -1 and end_idx != -1:
                    html_content = content[start_idx + len(start_tag):end_idx].strip()
                else:
                    html_content = content
            else:
                html_content = content

            # Clean HTML content
            cleaned_html = self._clean_html_content(html_content)

            # Convert relative file paths to absolute file:// URLs
            final_html = self._convert_file_links(cleaned_html)

            # Write the final content
            await self.file_system.write_file(report_filename, final_html)

            # Get absolute path
            # absolute_path = self.file_system.get_absolute_path(report_filename)

            return report_filename

        except Exception as e:
            logger.error(f"âŒ Failed to finalize report: {e}")
            # Return the path anyway
            return self.file_system.get_absolute_path(report_filename)

    def _clean_html_content(self, html_content: str) -> str:
        """Clean and validate HTML content"""
        # Remove markdown code block markers if present
        html_content = html_content.strip()
        if html_content.startswith("```html"):
            html_content = html_content[7:].strip()
        if html_content.startswith("```"):
            html_content = html_content[3:].strip()
        if html_content.endswith("```"):
            html_content = html_content[:-3].strip()

        # Ensure it starts with <!DOCTYPE html> or <html>
        if not html_content.lower().startswith(('<!doctype', '<html')):
            html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VibeSurf Report</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; line-height: 1.6; color: #333; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        .section {{ margin: 20px 0; padding: 15px; background: #f8f9fa; border-left: 4px solid #007bff; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="container">
        {html_content}
    </div>
</body>
</html>"""

        return html_content

    def _convert_file_links(self, html_content: str) -> str:
        """
        Convert relative file paths to absolute file:// URLs
        
        Args:
            html_content: HTML content with relative file paths
            
        Returns:
            str: HTML content with converted file:// URLs
        """
        # Pattern to match HTML href and src attributes with relative paths
        patterns = [
            (r'href\s*=\s*["\']([^"\']+)["\']', 'href'),  # <a href="path">
            (r'src\s*=\s*["\']([^"\']+)["\']', 'src'),  # <img src="path">
        ]

        for pattern, attr_name in patterns:
            def replace_path(match):
                full_match = match.group(0)
                file_path = match.group(1)

                # Check if it's already a URL or absolute path
                if file_path.startswith(('http://', 'https://', 'file://', '#', 'mailto:', 'tel:')):
                    return full_match  # Return unchanged

                # Convert to absolute path
                if not os.path.isabs(file_path):
                    absolute_path = os.path.abspath(os.path.join(self.workspace_dir, file_path))
                else:
                    absolute_path = file_path
                normalized_path = absolute_path.replace(os.path.sep, '/')
                file_url = f"file:///{normalized_path}"

                # Return the updated attribute
                quote = '"' if '"' in full_match else "'"
                return f'{attr_name}={quote}{file_url}{quote}'

            html_content = re.sub(pattern, replace_path, html_content)

        return html_content

    async def _generate_fallback_report(self, report_data: Dict[str, Any]) -> str:
        """Generate a simple fallback report when LLM generation fails"""
        logger.info("ğŸ“ Generating fallback report...")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"vibesurf_fallback_report-{timestamp}.html"

        # Create a simple HTML report
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VibeSurf Task Report</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }}
        .container {{
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.2em;
            font-weight: 300;
        }}
        .section {{
            margin: 0;
            padding: 25px;
            border-bottom: 1px solid #eee;
        }}
        .section:last-child {{
            border-bottom: none;
        }}
        .section h2 {{
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.4em;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }}
        .meta {{
            background: #ecf0f1;
            color: #7f8c8d;
            text-align: center;
            padding: 15px;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>VibeSurf Task Report</h1>
            <p>Generated on {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}</p>
        </div>
        
        <div class="section">
            <h2>Report Task</h2>
            <p>{report_data.get('report_task', 'No task specified')}</p>
        </div>
        
        <div class="section">
            <h2>Available Information</h2>
            <p>{report_data.get('information', 'No information provided')}</p>
        </div>
        
        <div class="section">
            <h2>Notice</h2>
            <p>This is a fallback report generated when the advanced LLM-controlled report generation encountered an issue. The report contains basic information provided for the task.</p>
            <p>For future runs, ensure that the LLM service is properly configured and accessible for enhanced report generation capabilities.</p>
        </div>
        
        <div class="meta">
            Generated by VibeSurf Agent Framework - Fallback Mode
        </div>
    </div>
</body>
</html>"""

        # Create and write fallback report
        await self.file_system.create_file(report_filename)
        await self.file_system.write_file(report_filename, html_content)

        # Get absolute path
        # absolute_path = self.file_system.get_absolute_path(report_filename)

        logger.info(f"âœ… Fallback report generated: {report_filename}")
        return report_filename



================================================
FILE: vibe_surf/agents/views.py
================================================
import asyncio
import json
import logging
import os
import pickle
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from uuid_extensions import uuid7str
from json_repair import repair_json

from browser_use.browser.session import BrowserSession
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import UserMessage, SystemMessage, BaseMessage, AssistantMessage, ContentPartTextParam, \
    ContentPartImageParam, ImageURL
from browser_use.browser.views import TabInfo, BrowserStateSummary
from browser_use.filesystem.file_system import FileSystem
from browser_use.agent.views import AgentSettings
from pydantic import BaseModel, Field, ConfigDict, create_model
from browser_use.agent.views import AgentSettings, DEFAULT_INCLUDE_ATTRIBUTES
from browser_use.tools.registry.views import ActionModel


class VibeSurfAgentOutput(BaseModel):
    """Agent output model following browser_use patterns"""
    model_config = ConfigDict(arbitrary_types_allowed=True, extra='forbid')

    thinking: str | None = None
    action: List[Any] = Field(
        ...,
        description='List of actions to execute',
        json_schema_extra={'min_items': 1},
    )

    @classmethod
    def model_json_schema(cls, **kwargs):
        schema = super().model_json_schema(**kwargs)
        schema['required'] = ['thinking', 'action']
        return schema

    @staticmethod
    def type_with_custom_actions(custom_actions: type) -> type:
        """Extend actions with custom actions"""
        model_ = create_model(
            'VibeSurfAgentOutput',
            __base__=VibeSurfAgentOutput,
            action=(
                list[custom_actions],  # type: ignore
                Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
            ),
            __module__=VibeSurfAgentOutput.__module__,
        )
        model_.__doc__ = 'VibeSurfAgentOutput model with custom actions'
        return model_


class VibeSurfAgentSettings(BaseModel):
    use_vision: bool = True
    max_failures: int = 3
    override_system_message: str | None = None
    extend_system_message: str | None = None
    include_attributes: list[str] | None = DEFAULT_INCLUDE_ATTRIBUTES
    max_actions_per_step: int = 4
    max_history_items: int | None = None
    include_token_cost: bool = False

    calculate_cost: bool = True
    include_tool_call_examples: bool = False
    llm_timeout: int = 60  # Timeout in seconds for LLM calls
    step_timeout: int = 180  # Timeout in seconds for each step

    agent_mode: str = "thinking"  # thinking, no-thinking, flash


class CustomAgentOutput(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True, extra='forbid')

    thinking: str | None = None
    action: list[ActionModel] = Field(
        ...,
        description='List of actions to execute',
        json_schema_extra={'min_items': 1},  # Ensure at least one action is provided
    )

    @classmethod
    def model_json_schema(cls, **kwargs):
        schema = super().model_json_schema(**kwargs)
        schema['required'] = ['action']
        return schema

    @staticmethod
    def type_with_custom_actions(custom_actions: type[ActionModel]) -> type['CustomAgentOutput']:
        """Extend actions with custom actions"""

        model_ = create_model(
            'AgentOutput',
            __base__=CustomAgentOutput,
            action=(
                list[custom_actions],  # type: ignore
                Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
            ),
            __module__=CustomAgentOutput.__module__,
        )
        model_.__doc__ = 'AgentOutput model with custom actions'
        return model_

    @staticmethod
    def type_with_custom_actions_no_thinking(custom_actions: type[ActionModel]) -> type['CustomAgentOutput']:
        """Extend actions with custom actions and exclude thinking field"""

        class AgentOutputNoThinking(CustomAgentOutput):
            @classmethod
            def model_json_schema(cls, **kwargs):
                schema = super().model_json_schema(**kwargs)
                del schema['properties']['thinking']
                schema['required'] = ['action']
                return schema

        model = create_model(
            'AgentOutputNoThinking',
            __base__=AgentOutputNoThinking,
            action=(
                list[custom_actions],  # type: ignore
                Field(..., description='List of actions to execute', json_schema_extra={'min_items': 1}),
            ),
            __module__=AgentOutputNoThinking.__module__,
        )

        model.__doc__ = 'AgentOutput model with custom actions'
        return model



================================================
FILE: vibe_surf/agents/prompts/__init__.py
================================================
# Prompts module for VibeSurf agents


================================================
FILE: vibe_surf/agents/prompts/report_writer_prompt.py
================================================
REPORT_WRITER_PROMPT = """
You are an intelligent report writing assistant that can read files, generate content, and create professional HTML reports.

## Your Capabilities:
1. **read_file**: Read existing files to gather additional context or reference material
2. **write_file**: Write content to files, including generating report content and creating HTML output

## Workflow (MUST Follow These Steps):
1. **Analyze the task**: Understand what type of report is needed and what information you have
2. **Determine if you need more information**:
   - If you need to read existing files for context, use `read_file`
   - Look for references to files in the task or information that might be helpful
   - **IMPORTANT for BrowserTaskResult inputs**: If you receive browser_results data containing BrowserTaskResult objects:
     * Each BrowserTaskResult has an `agent_workdir` field with the actual working directory path
     * For any file paths in `important_files` or other file references from that result:
       - Check if the file path already starts with the `agent_workdir` value
       - If NOT, prepend the `agent_workdir` value + "/" to the file path when calling read_file
       - This ensures you can access files created by the browser agent correctly
     * Example: If BrowserTaskResult shows `agent_workdir: "/tmp/session123"` and `important_files: ["data/report.csv"]`,
       use `/tmp/session123/data/report.csv` when calling read_file
3. **Generate the report content**: Create comprehensive, professional content that directly addresses the task requirements
4. **MANDATORY FORMATTING STEP**: **THIS STEP IS REQUIRED** - Format the content as a professional HTML document with:
   - Complete HTML5 structure with DOCTYPE
   - Professional styling with embedded CSS
   - Responsive design and clean typography
   - Visual hierarchy with proper sections
   - Data tables where appropriate
   - Professional color scheme (blues, grays, whites)
   - Cross-browser compatibility and print-friendly design
5. **Final output**: Write the fully formatted HTML to the target file using `write_file`

## Content Guidelines:
- Focus ONLY on what the user specifically requested - ignore technical execution details
- Create content that directly addresses the user's needs (comparison, analysis, research findings, etc.)
- DO NOT include methodology, task overview, or technical process information
- DO NOT mention agents, browser automation, or technical execution methods
- Write as if you're delivering exactly what the user asked for
- Use a professional, clear, and engaging style
- Structure content with clear sections relevant to the user's request

## HTML Requirements:
- Complete HTML5 document with DOCTYPE
- Embedded CSS (no external dependencies)
- Responsive design with proper meta tags
- Professional styling with modern CSS features
- Clean, readable typography
- Proper spacing, margins, and visual hierarchy
- Cross-browser compatibility
- Print-friendly design
- Semantic HTML elements
- **For local files (images, documents, etc.)**: Use relative paths in standard HTML format:
  - Images: `<img src="path/to/image.jpg" alt="description">`
  - Links: `<a href="path/to/document.pdf">Link text</a>`
  - The system will automatically convert these to absolute file:// URLs. Please do not use `file://` before path.

## Title Guidelines:
- Create titles based on the actual content/topic
- NOT "Task Execution Report" or similar generic titles
- Make it specific to what was researched/analyzed

## Execution Requirements:
- **ALWAYS** start by analyzing if you need to read any files first
- Generate comprehensive content that addresses the user's specific request
- **MANDATORY**: Complete the formatting step - transform content into professional HTML format
- **CRITICAL**: The formatting step cannot be skipped - it is required for every report
- Write the final formatted HTML to the target file using `write_file`
- Call `task_done` only after the report is fully formatted and written

## Key Reminder:
**Every report MUST include a dedicated formatting step** (typically the final step before output). This step transforms your content into a professional, well-structured HTML document. Raw content without proper HTML formatting is not acceptable.

Remember: You are creating a professional deliverable that directly fulfills the user's request. Focus on the subject matter, not the technical process.
"""



================================================
FILE: vibe_surf/agents/prompts/vibe_surf_prompt.py
================================================
# VibeSurf Agent System Prompt - Professional AI Browser Assistant
VIBESURF_SYSTEM_PROMPT = """
# VibeSurf AI Browser Assistant

You are VibeSurf Agent, a professional AI browser assistant developed by [WarmShao](https://github.com/warmshao). You specialize in intelligent web automation, search, research, file operation, file extraction and report generation with advanced concurrent execution capabilities.

## Core Architecture

You operate using with followed primary agents for collaboration:

1. **Browser Automation**: Execute web tasks using `execute_browser_use_agent`
    - **Parallel Task Processing**: Execute multiple independent browser tasks simultaneously
    - **Efficiency Optimization**: Dramatically reduce execution time for multi-step workflows
    - **Intelligent Task Distribution**: Automatically identify parallelize subtasks
    - **Resource Management**: Optimal browser session allocation across concurrent agents
    - **Autonomous Operation**: Browser agents have strong planning capabilities - provide goals, not step-by-step instructions
    - **Multi-format Support**: Handle documents, images, data extraction, and automation
    
2. **Report Generation**: Create structured HTML reports using `execute_report_writer_agent`
    - **Professional Report Writer**: Generate professional HTML report

## Key Capabilities
### Intelligent Task Management
- **TODO System**: Generate, track, and manage complex task hierarchies using todo tools
- **Progress Monitoring**: Real-time status tracking across all concurrent operations
- **Adaptive Planning**: Dynamic task breakdown based on complexity and dependencies

### File System Management
- **Workspace Directory**: You operate within a dedicated workspace directory structure
- **Relative Path Usage**: All file paths are relative to the workspace directory (e.g., "data/report.pdf", "uploads/document.txt")
- **File Operations**: Use relative paths when calling file-related functions - the system automatically resolves to the correct workspace location
- **File Processing**: Support for documents, images, spreadsheets, PDFs with seamless workspace integration

## Context Processing

You will receive contextual information including:
- **Current Browser Tabs**: Available browsing sessions with tab IDs
- **Current Active Browser Tab ID**: Current active browser tab id
- **Previous Results**: Outcomes from completed browser tasks
- **Generated Reports**: Paths to created report files
- **Session State**: Current workflow progress and status

### Tab Reference Processing
- **Tab Reference Format**: When users include `@ tab_id: title` markers in their requests, this indicates they want to process those specific tabs
- **Tab ID Assignment**: When generating browser tasks, you MUST assign the exact same tab_id as specified in the user's request
- **Target Tab Processing**: Use the referenced tab_id as the target for browser automation tasks to ensure operations are performed on the correct tabs

## Operational Guidelines

### Task Design Principles
1. **Simple Response**: Directly return response content or answer in task_done action if you think this is a simple task, such as Basic conversions or General advice or recommendations based on common knowledge and etc.
2. **Goal-Oriented Descriptions**: Focus on WHAT to achieve, not HOW to do it
3. **Concurrent Optimization**: Break independent tasks into parallel execution when possible
4. **Resource Efficiency**: Leverage existing browser tabs when appropriate
5. **Quality Assurance**: Ensure comprehensive data collection and analysis

### Task Completion Requirements (task_done action)
- **Summary Format**: If response is a summary, use markdown format
- **File References**: When showing files, use `[file_name](file_path)` format - especially for report files
- **Complex Tasks**: Provide detailed summaries with comprehensive information

### File Processing
- Support all major file formats (documents, images, spreadsheets, PDFs)
- Use relative file paths within workspace: `data/report.pdf`, `uploads/document.txt`
- Include file references in task descriptions when relevant
- All file operations automatically resolve relative to the workspace directory

## Skills Command Processing
- When users input commands in `/skill_name` format, please use the corresponding skill action:
- **Tab Targeting[Optional]**: Such as `/crawl @1234` â†’ Execute `skill_crawl` with tab_id "1234"
- **Parameter Processing**: Sometimes user provide uncompleted or simple prompt, please convert it to correct and optimized params. Such as convert natural language to valid JavaScript for code skill
- **Special Cases**: `skill_deep_research` only returns guidelines only, then follow guidelines to conduct actual research
- **Execution Policy**: Skill actions execute only once (no need to retry if errors occur), and all results - whether successful or failed - should be presented to users in structured markdown format.
- **Follow-up Operations**: When users input skill operations without specifying additional tasks, do not automatically perform subsequent operations. Only perform additional tool operations when users specifically request actions like saving results to files or writing reports.
- After `/search` completion, NEVER use browser agent to deeply investigate search result (unless explicitly emphasized by the user). The user usually only need the search results. Just return the search results.

## Language Adaptability

**Critical**: Your output language must match the user's request language. If the user communicates in Chinese, respond in Chinese. If in English, respond in English. Maintain consistency throughout the interaction.

## Quality Assurance

Before executing any action:
1. **Analyze Complexity**: Determine if task requires simple response, browser automation, or reporting
2. **Identify Parallelization**: Look for independent subtasks that can run concurrently
3. **Plan Resource Usage**: Consider tab management and session optimization
4. **Validate Completeness**: Ensure all user requirements are addressed


Execute with precision, leverage concurrent capabilities for efficiency, and deliver professional results that exceed expectations.
"""


EXTEND_BU_SYSTEM_PROMPT = """
* Please make sure the language of your output in JSON value should remain the same as the user's request or task. 
* Regarding file operations, please note that you need the full relative path (including subfolders), not just the file name.
* Especially when a file operation reports an error, please reflect whether the file path is not written correctly, such as the subfolder is not written.
* If you are operating on files in the filesystem, be sure to use relative paths (relative to the workspace dir) instead of absolute paths.
* If you are typing in the search box, please use Enter key to search instead of clicking.
"""



================================================
FILE: vibe_surf/backend/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/backend/llm_config.py
================================================
"""
LLM Provider Configuration

Centralized configuration for all supported LLM providers and their models.
"""

# LLM Providers and their supported models
LLM_PROVIDERS = {
    "openai": [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-4.1"
        "gpt-4.1-mini",
        "gpt-5",
        "gpt-5-mini",
    ],
    "anthropic": [
        "claude-opus-4-1-20250805",
        "claude-sonnet-4-20250514",
        "claude-3-7-sonnet-20250219",
        "claude-3-5-haiku-20241022"
    ],
    "google": [
        "gemini-2.5-pro",
        "gemini-2.5-flash",
    ],
    "azure_openai": [
        "gpt-4o",
        "gpt-4o-mini",
    ],
    "groq": [
        "moonshotai/kimi-k2-instruct",
        "deepseek-r1-distill-llama-70b",
        "qwen/qwen3-32b",
    ],
    "ollama": [
        "deepseek-r1:14b",
        "gpt-oss:20b",
        "qwen3:latest",
    ],
    "openrouter": [
        "deepseek/deepseek-chat-v3.1",
        "qwen/qwen3-235b-a22b-thinking-2507",
        "moonshotai/kimi-k2",
        "z-ai/glm-4.5"
    ],
    "deepseek": [
        "deepseek-chat",
        "deepseek-reasoner"
    ],
    "aws_bedrock": [
        "anthropic.claude-opus-4-1-20250805-v1:0",
        "anthropic.claude-opus-4-20250514-v1:0",
        "anthropic.claude-sonnet-4-20250514-v1:0",
        "anthropic.claude-3-7-sonnet-20250219-v1:0",
        "anthropic.claude-3-5-haiku-20241022-v1:0",
    ],
    "anthropic_bedrock": [
    ],
    "openai_compatible": [
    ]
}

# Provider metadata
PROVIDER_METADATA = {
    "openai": {
        "display_name": "OpenAI",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "gpt-4.1-mini"
    },
    "anthropic": {
        "display_name": "Anthropic",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "claude-3-7-sonnet-20250219"
    },
    "google": {
        "display_name": "Google Gemini",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "gemini-2.5-flash"
    },
    "azure_openai": {
        "display_name": "Azure OpenAI",
        "requires_api_key": True,
        "requires_base_url": True,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "gpt-4o-mini"
    },
    "groq": {
        "display_name": "Groq",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "moonshotai/kimi-k2-instruct"
    },
    "ollama": {
        "display_name": "Ollama",
        "requires_api_key": False,
        "requires_base_url": True,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "qwen/qwen3-32b",
        "default_base_url": "http://localhost:11434"
    },
    "openrouter": {
        "display_name": "OpenRouter",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "moonshotai/kimi-k2",
        "base_url": "https://openrouter.ai/api/v1"
    },
    "deepseek": {
        "display_name": "DeepSeek",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "deepseek-chat"
    },
    "aws_bedrock": {
        "display_name": "AWS Bedrock",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": "anthropic.claude-3-7-sonnet-20250219-v1:0"
    },
    "anthropic_bedrock": {
        "display_name": "Anthropic via AWS Bedrock",
        "requires_api_key": True,
        "supports_base_url": False,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": ""
    },
    "openai_compatible": {
        "display_name": "OpenAI Compatible",
        "requires_api_key": True,
        "requires_base_url": True,
        "supports_tools": True,
        "supports_vision": True,
        "default_model": ""
    }
}


def get_supported_providers():
    """Get list of all supported provider names"""
    return list(LLM_PROVIDERS.keys())


def get_provider_models(provider_name: str):
    """Get list of models for a specific provider"""
    return LLM_PROVIDERS.get(provider_name, [])


def get_provider_metadata(provider_name: str):
    """Get metadata for a specific provider"""
    return PROVIDER_METADATA.get(provider_name, {})


def is_provider_supported(provider_name: str):
    """Check if a provider is supported"""
    return provider_name in LLM_PROVIDERS


def get_default_model(provider_name: str):
    """Get default model for a provider"""
    metadata = get_provider_metadata(provider_name)
    return metadata.get("default_model", "")



================================================
FILE: vibe_surf/backend/main.py
================================================
"""
VibeSurf Backend API

uvicorn backend.main:app --host 127.0.0.1 --port 9335

FastAPI application for simplified single-task execution model.
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
import argparse
import os
import asyncio
from datetime import datetime

# Import routers
from .api.task import router as agents_router
from .api.files import router as files_router
from .api.activity import router as activity_router
from .api.config import router as config_router
from .api.browser import router as browser_router
from .api.voices import router as voices_router
from .api.agent import router as agent_router

# Import shared state
from . import shared_state

# Configure logging

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

app = FastAPI(
    title="VibeSurf Backend API",
    description="Simplified single-task execution model for VibeSurf",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(agents_router, prefix="/api", tags=["tasks"])
app.include_router(files_router, prefix="/api", tags=["files"])
app.include_router(activity_router, prefix="/api", tags=["activity"])
app.include_router(config_router, prefix="/api", tags=["config"])
app.include_router(browser_router, prefix="/api", tags=["browser"])
app.include_router(voices_router, prefix="/api", tags=["voices"])
app.include_router(agent_router, prefix="/api", tags=["agent"])

# Global variable to control browser monitoring task
browser_monitor_task = None

async def monitor_browser_connection():
    """Background task to monitor browser connection"""
    while True:
        try:
            await asyncio.sleep(2)  # Check every 1 second
            
            if shared_state.browser_manager:
                is_connected = await shared_state.browser_manager.check_browser_connected()
                if not is_connected:
                    logger.error("No Available Browser, Exiting...")
                    
                    # Schedule a graceful shutdown using os.kill in a separate thread
                    import threading
                    import signal
                    import os
                    
                    def trigger_shutdown():
                        try:
                            # Give a brief moment for any cleanup
                            import time
                            time.sleep(0.5)
                            # Send SIGTERM to current process for graceful shutdown
                            os.kill(os.getpid(), signal.SIGTERM)
                        except Exception as e:
                            logger.error(f"Error during shutdown trigger: {e}")
                            # Fallback to SIGKILL if SIGTERM doesn't work
                            try:
                                os.kill(os.getpid(), signal.SIGKILL)
                            except:
                                pass
                    
                    # Start shutdown in a separate thread to avoid blocking the async loop
                    shutdown_thread = threading.Thread(target=trigger_shutdown)
                    shutdown_thread.daemon = True
                    shutdown_thread.start()
                    
                    # Exit the monitoring loop
                    break
                    
        except asyncio.CancelledError:
            logger.info("Browser monitor task cancelled")
            break
        except Exception as e:
            logger.warning(f"Browser monitor error: {e}")
            # Continue monitoring even if there's an error

@app.on_event("startup")
async def startup_event():
    """Initialize database and VibeSurf components on startup"""
    global browser_monitor_task
    
    # Initialize VibeSurf components and update shared state
    await shared_state.initialize_vibesurf_components()

    # Start browser monitoring task
    browser_monitor_task = asyncio.create_task(monitor_browser_connection())
    logger.info("ğŸ” Started browser connection monitor")

    logger.info("ğŸš€ VibeSurf Backend API started with single-task execution model")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    global browser_monitor_task
    
    logger.info("ğŸ›‘ Starting graceful shutdown...")
    
    # Cancel browser monitor task
    if browser_monitor_task and not browser_monitor_task.done():
        browser_monitor_task.cancel()
        try:
            await asyncio.wait_for(browser_monitor_task, timeout=2.0)
        except (asyncio.CancelledError, asyncio.TimeoutError):
            pass
        logger.info("âœ… Browser monitor task stopped")
    
    # Cleanup VibeSurf components
    if shared_state.browser_manager:
        try:
            await shared_state.browser_manager.close()
            await shared_state.browser_manager.main_browser_session.kill()
            logger.info("âœ… Browser manager closed")
        except Exception as e:
            logger.error(f"âŒ Error closing browser manager: {e}")
    
    # Close database
    if shared_state.db_manager:
        try:
            await shared_state.db_manager.close()
            logger.info("âœ… Database manager closed")
        except Exception as e:
            logger.error(f"âŒ Error closing database manager: {e}")
    
    logger.info("ğŸ›‘ VibeSurf Backend API stopped")

# Health check endpoint
@app.get("/health")
async def health_check():
    """API health check"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "VibeSurf Backend API",
        "model": "single-task-execution",
        "version": "2.0.0"
    }

# Session ID generation endpoint
@app.get("/generate-session-id")
async def generate_session_id(prefix: str = ""):
    """Generate a new session ID using uuid7str"""
    from uuid_extensions import uuid7str
    session_id = f"{uuid7str()}"
    return {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat()
    }

# Simple status endpoint for the active task
@app.get("/api/status")
async def get_system_status():
    """Get current system status"""
    task_info = shared_state.get_active_task_info()
    
    return {
        "system_status": "operational",
        "active_task": task_info,
        "timestamp": datetime.now().isoformat()
    }

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="VibeSurf Backend API")
    parser.add_argument(
        "--vibesurf_port",
        type=int,
        default=9335,
        help="Port for VibeSurf backend (default: 9335)"
    )
    parser.add_argument(
        "--vibesurf_extension",
        type=str,
        help="VibeSurf chrome extension path (optional)"
    )
    return parser.parse_args()

if __name__ == "__main__":
    # Parse command line arguments
    args = parse_args()
    
    # Set environment variables based on arguments
    os.environ["VIBESURF_BACKEND_PORT"] = str(args.vibesurf_port)
    if args.vibesurf_extension:
        os.environ["VIBESURF_EXTENSION"] = args.vibesurf_extension
    
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=args.vibesurf_port)


================================================
FILE: vibe_surf/backend/shared_state.py
================================================
"""
Shared State Module

Contains global state variables shared between main.py and routers
to avoid circular import issues.
"""
import pdb
from typing import Optional, Dict, Any, List
from datetime import datetime
import logging
import os
import json
import platform
from pathlib import Path

# VibeSurf components
from vibe_surf.agents.vibe_surf_agent import VibeSurfAgent
from vibe_surf.tools.browser_use_tools import BrowserUseTools
from vibe_surf.tools.vibesurf_tools import VibeSurfTools
from vibe_surf.browser.browser_manager import BrowserManager
from browser_use.llm.base import BaseChatModel
from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.browser import BrowserProfile
from vibe_surf.llm.openai_compatible import ChatOpenAICompatible
from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.browser.agen_browser_profile import AgentBrowserProfile
from vibe_surf.backend.utils.utils import configure_system_proxies

logger = logging.getLogger(__name__)

# Global VibeSurf components
vibesurf_agent: Optional[VibeSurfAgent] = None
browser_manager: Optional[BrowserManager] = None
vibesurf_tools: Optional[VibeSurfTools] = None
llm: Optional[BaseChatModel] = None
db_manager: Optional['DatabaseManager'] = None
current_llm_profile_name: Optional[str] = None

# Environment variables
workspace_dir: str = ""
browser_execution_path: str = ""
browser_user_data: str = ""

# Global environment variables dictionary
envs: Dict[str, str] = {}

# MCP server management
active_mcp_server: Dict[str, str] = {}  # Dict[mcp_id: mcp_server_name]

# Single task execution tracking
active_task: Optional[Dict[str, Any]] = None


def get_all_components():
    """Get all components as a dictionary"""
    global vibesurf_agent, browser_manager, vibesurf_tools, llm, db_manager, current_llm_profile_name
    global workspace_dir, browser_execution_path, browser_user_data, active_mcp_server, envs

    return {
        "vibesurf_agent": vibesurf_agent,
        "browser_manager": browser_manager,
        "tools": vibesurf_tools,
        "llm": llm,
        "db_manager": db_manager,
        "workspace_dir": workspace_dir,
        "browser_execution_path": browser_execution_path,
        "browser_user_data": browser_user_data,
        "active_mcp_server": active_mcp_server,
        "active_task": active_task,
        "current_llm_profile_name": current_llm_profile_name,
        "envs": envs
    }


def set_components(**kwargs):
    """Update global components"""
    global vibesurf_agent, browser_manager, vibesurf_tools, llm, db_manager, current_llm_profile_name
    global workspace_dir, browser_execution_path, browser_user_data, active_mcp_server, envs

    if "vibesurf_agent" in kwargs:
        vibesurf_agent = kwargs["vibesurf_agent"]
    if "browser_manager" in kwargs:
        browser_manager = kwargs["browser_manager"]
    if "tools" in kwargs:
        vibesurf_tools = kwargs["tools"]
    if "llm" in kwargs:
        llm = kwargs["llm"]
    if "db_manager" in kwargs:
        db_manager = kwargs["db_manager"]
    if "workspace_dir" in kwargs:
        workspace_dir = kwargs["workspace_dir"]
    if "browser_execution_path" in kwargs:
        browser_execution_path = kwargs["browser_execution_path"]
    if "browser_user_data" in kwargs:
        browser_user_data = kwargs["browser_user_data"]
    if "active_mcp_server" in kwargs:
        active_mcp_server = kwargs["active_mcp_server"]
    if "envs" in kwargs:
        envs = kwargs["envs"]
    if "current_llm_profile_name" in kwargs:
        envs = kwargs["current_llm_profile_name"]


async def execute_task_background(
        task_id: str,
        session_id: str,
        task: str,
        llm_profile_name: str,
        upload_files: Optional[List[str]] = None,
        agent_mode: str = "thinking",
        db_session=None
):
    """Background task execution function for single task with LLM profile support"""
    global vibesurf_agent, active_task, current_llm_profile_name

    try:
        current_llm_profile_name = llm_profile_name

        # Check if MCP server configuration needs update
        await _check_and_update_mcp_servers(db_session)

        # Update active task status to running
        active_task = {
            "task_id": task_id,
            "status": "running",
            "session_id": session_id,
            "task": task,
            "llm_profile_name": llm_profile_name,
            "workspace_dir": workspace_dir,
            "upload_files": upload_files or [],
            "active_mcp_servers": list(active_mcp_server.values()),  # List of MCP server names
            "start_time": datetime.now(),
            "agent_id": task_id  # Use task_id as agent_id for tracking
        }

        logger.info(f"Task {task_id} started for session {session_id} with profile {llm_profile_name}")

        # Ensure correct workspace directory is set for this task
        if vibesurf_agent:
            vibesurf_agent.workspace_dir = workspace_dir

        # Execute the task
        result = await vibesurf_agent.run(
            task=task,
            upload_files=upload_files,
            session_id=session_id,
            agent_mode=agent_mode
        )

        # Update task status to completed
        if active_task and active_task.get("status") != "stopped":
            active_task.update({
                "status": "completed",
                "result": result,
                "end_time": datetime.now()
            })

        # Get session directory for report path
        session_dir = os.path.join(workspace_dir, session_id)
        report_path = None

        # Look for generated report
        reports_dir = os.path.join(session_dir, "reports")
        if os.path.exists(reports_dir):
            for file in os.listdir(reports_dir):
                if file.endswith('.html'):
                    report_path = os.path.join(reports_dir, file)
                    break

        # Save task to database
        if db_session:
            try:
                from .database.queries import TaskQueries
                await TaskQueries.update_task_completion(
                    db_session,
                    task_id=task_id,
                    task_result=result,
                    task_status=active_task.get("status", "completed") if active_task else "completed",
                    report_path=report_path
                )
                await db_session.commit()
            except Exception as e:
                logger.error(f"Failed to update task in database: {e}")

        logger.info(f"Task {task_id} completed for session {session_id}")

    except Exception as e:
        logger.error(f"Task execution failed: {e}")
        # Update task status to failed
        if active_task and active_task.get("task_id") == task_id:
            active_task.update({
                "status": "failed",
                "error": str(e),
                "end_time": datetime.now()
            })

        # Save failed task to database
        if db_session:
            try:
                from .database.queries import TaskQueries
                await TaskQueries.update_task_completion(
                    db_session,
                    task_id=task_id,
                    task_result=None,
                    task_status="failed",
                    error_message=str(e)
                )
                await db_session.commit()
            except Exception as e:
                logger.error(f"Failed to save failed task to database: {e}")

        logger.error(f"Task {task_id} failed for session {session_id}: {e}")
    finally:
        # Clear active task when execution is complete (success or failure)
        active_task = None


def is_task_running() -> bool:
    """Quick check if any task is currently running"""
    global active_task
    return active_task is not None and active_task.get("status") not in ["failed",
                                                                         "completed",
                                                                         "stopped"]


def get_active_task_info() -> Optional[Dict[str, Any]]:
    """Get current active task information"""
    global active_task
    return active_task.copy() if active_task else None


def clear_active_task():
    """Clear the active task (used when stopping)"""
    global active_task
    active_task = None


async def _check_and_update_mcp_servers(db_session):
    """Check if MCP server configuration has changed and update tools if needed"""
    global vibesurf_tools, active_mcp_server

    try:
        if not db_session:
            return

        from .database.queries import McpProfileQueries

        # Get current active MCP servers from database
        active_profiles = await McpProfileQueries.get_active_profiles(db_session)
        current_active_servers = {profile.mcp_id: profile.mcp_server_name for profile in active_profiles}

        # Compare with shared state
        if current_active_servers != active_mcp_server:
            logger.info(f"MCP server configuration changed. Updating tools...")
            logger.info(f"Old config: {active_mcp_server}")
            logger.info(f"New config: {current_active_servers}")

            # Update shared state
            active_mcp_server = current_active_servers.copy()

            # Create new MCP server config for tools
            mcp_server_config = await _build_mcp_server_config(active_profiles)

            # Unregister old MCP clients and register new ones
            if vibesurf_tools:
                await vibesurf_tools.unregister_mcp_clients()
                vibesurf_tools.mcp_server_config = mcp_server_config
                await vibesurf_tools.register_mcp_clients()
                logger.info("âœ… Controller MCP configuration updated successfully")

    except Exception as e:
        logger.error(f"Failed to check and update MCP servers: {e}")


async def _build_mcp_server_config(active_profiles) -> Dict[str, Any]:
    """Build MCP server configuration from active profiles"""
    mcp_server_config = {
        "mcpServers": {}
    }

    for profile in active_profiles:
        mcp_server_config["mcpServers"][profile.mcp_server_name] = json.loads(profile.mcp_server_params)

    return mcp_server_config


async def _load_active_mcp_servers():
    """Load active MCP servers from database and return config"""
    global db_manager, active_mcp_server

    try:
        if not db_manager:
            logger.info("Database manager not available, returning empty MCP config")
            return {"mcpServers": {}}

        from .database.queries import McpProfileQueries

        async for db in db_manager.get_session():
            try:
                # Get all active MCP profiles
                active_profiles = await McpProfileQueries.get_active_profiles(db)

                # Update shared state
                active_mcp_server = {profile.mcp_id: profile.mcp_server_name for profile in active_profiles}

                # Build MCP server config
                mcp_server_config = await _build_mcp_server_config(active_profiles)

                logger.info(f"âœ… Loaded {len(active_profiles)} active MCP servers: {list(active_mcp_server.values())}")

                return mcp_server_config

            except Exception as e:
                logger.warning(f"Failed to load MCP servers from database: {e}")
                return {"mcpServers": {}}
            finally:
                break

    except Exception as e:
        logger.warning(f"Database not available for MCP server loading: {e}")
        return {"mcpServers": {}}


async def initialize_vibesurf_components():
    """Initialize VibeSurf components from environment variables and default LLM profile"""
    global vibesurf_agent, browser_manager, vibesurf_tools, llm, db_manager, current_llm_profile_name
    global workspace_dir, browser_execution_path, browser_user_data, envs
    from vibe_surf import common

    try:
        # Load environment variables
        workspace_dir = common.get_workspace_dir()
        logger.info("WorkSpace directory: {}".format(workspace_dir))
        configure_system_proxies()
        # Load environment configuration from envs.json
        envs_file_path = os.path.join(workspace_dir, "envs.json")
        try:
            if os.path.exists(envs_file_path):
                with open(envs_file_path, 'r', encoding='utf-8') as f:
                    envs = json.load(f)
                logger.info(f"âœ… Loaded environment configuration from {envs_file_path}")
                
                # Set loaded environment variables to system environment
                for key, value in envs.items():
                    if value:  # Only set non-empty values
                        os.environ[key] = value
                        logger.info(f"ğŸ”§ Set environment variable: {key}")
            else:
                envs = {}
                logger.info("ğŸ“ No existing envs.json found, initializing empty environment configuration")
        except Exception as e:
            logger.warning(f"Failed to load envs.json: {e}, initializing empty environment configuration")
            envs = {}
        browser_execution_path = os.getenv("BROWSER_EXECUTION_PATH", "")
        assert os.path.exists(browser_execution_path), "Please set the BROWSER_EXECUTION_PATH environment variable"
        browser_user_data = os.getenv("BROWSER_USER_DATA", "")
        if not browser_user_data or not os.path.exists(browser_user_data):
            browser_user_data = os.path.join(workspace_dir, "browser_user_data",
                                             f"{os.path.basename(browser_execution_path)}-profile")

        # Get VibeSurf extension path
        vibesurf_extension = os.getenv("VIBESURF_EXTENSION", "")
        if not vibesurf_extension.strip() or not os.path.exists(vibesurf_extension):
            current_file = Path(__file__)
            project_root = current_file.parent.parent.absolute()
            vibesurf_extension = str(project_root / "chrome_extension")
            assert os.path.exists(vibesurf_extension)

        # Get backend URL
        backend_port = os.getenv("VIBESURF_BACKEND_PORT", "9335")
        if not backend_port or not backend_port.strip():
            backend_port = "9335"
        backend_port = int(backend_port)

        backend_url = f'http://127.0.0.1:{backend_port}'

        # Update envs dictionary with current environment variables
        envs.update({
            "BROWSER_EXECUTION_PATH": browser_execution_path,
            "BROWSER_USER_DATA": browser_user_data,
            "VIBESURF_EXTENSION": vibesurf_extension,
            "VIBESURF_BACKEND_URL": backend_url
        })

        # Create directories if they don't exist
        os.makedirs(workspace_dir, exist_ok=True)

        # Initialize database manager after workspace_dir is set
        from .database.manager import DatabaseManager

        # Debug: Check environment variable value
        env_database_url = os.getenv('VIBESURF_DATABASE_URL')
        logger.info(f"ğŸ” VIBESURF_DATABASE_URL environment variable: '{env_database_url}'")
        logger.info(f"ğŸ” workspace_dir: '{workspace_dir}'")

        # Handle empty string environment variable properly
        if env_database_url and env_database_url.strip():
            database_url = env_database_url
        else:
            database_url = f'sqlite+aiosqlite:///{os.path.join(workspace_dir, "vibe_surf.db")}'

        logger.info(f"ğŸ” Final database_url: '{database_url}'")

        db_manager = DatabaseManager(database_url)

        # Initialize database tables with migration support
        await db_manager.create_tables(use_migrations=True)
        logger.info("âœ… Database manager initialized successfully")

        # Initialize LLM from default profile (if available) or fallback to environment variables
        llm = await _initialize_default_llm()

        # Initialize browser manager
        if browser_manager:
            main_browser_session = browser_manager.main_browser_session
        else:
            from screeninfo import get_monitors
            primary_monitor = get_monitors()[0]
            _update_extension_backend_url(envs["VIBESURF_EXTENSION"], backend_url)
            
            browser_profile = AgentBrowserProfile(
                executable_path=browser_execution_path,
                user_data_dir=browser_user_data,
                headless=False,
                keep_alive=True,
                auto_download_pdfs=False,
                highlight_elements=True,
                custom_extensions=[envs["VIBESURF_EXTENSION"]],
                window_size={"width": primary_monitor.width, "height": primary_monitor.height}
            )

            # Initialize components
            main_browser_session = AgentBrowserSession(browser_profile=browser_profile)
            await main_browser_session.start()
        browser_manager = BrowserManager(
            main_browser_session=main_browser_session
        )

        # Load active MCP servers from database
        mcp_server_config = await _load_active_mcp_servers()

        # Initialize vibesurf tools with MCP server config
        vibesurf_tools = VibeSurfTools(mcp_server_config=mcp_server_config)

        # Register MCP clients if there are any active MCP servers
        if mcp_server_config and mcp_server_config.get("mcpServers"):
            await vibesurf_tools.register_mcp_clients()
            logger.info(f"âœ… Registered {len(mcp_server_config['mcpServers'])} MCP servers")

        # Initialize VibeSurfAgent
        vibesurf_agent = VibeSurfAgent(
            llm=llm,
            browser_manager=browser_manager,
            tools=vibesurf_tools,
            workspace_dir=workspace_dir
        )

        # Save environment configuration to envs.json
        try:
            with open(envs_file_path, 'w', encoding='utf-8') as f:
                json.dump(envs, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… Saved environment configuration to {envs_file_path}")
        except Exception as e:
            logger.warning(f"Failed to save envs.json: {e}")

        logger.info("âœ… VibeSurf components initialized successfully")

    except Exception as e:
        logger.error(f"âŒ Failed to initialize VibeSurf components: {e}")
        raise


async def _initialize_default_llm():
    """Initialize LLM from default profile or fallback to environment variables"""
    global db_manager, current_llm_profile_name

    try:
        # Try to get default LLM profile from database
        from .database.queries import LLMProfileQueries
        from .utils.llm_factory import create_llm_from_profile

        # Get database session from shared state db_manager
        if db_manager:
            async for db in db_manager.get_session():
                try:
                    default_profile = await LLMProfileQueries.get_default_profile(db)
                    if default_profile:
                        # Get profile with decrypted API key
                        profile_with_key = await LLMProfileQueries.get_profile_with_decrypted_key(
                            db, default_profile.profile_name
                        )
                        if profile_with_key:
                            llm_instance = create_llm_from_profile(profile_with_key)
                            current_llm_profile_name = default_profile.profile_name
                            logger.info(f"âœ… LLM initialized from default profile: {default_profile.profile_name}")
                            return llm_instance
                    break
                except Exception as e:
                    logger.warning(f"Failed to load default LLM profile: {e}")
                    break
    except Exception as e:
        logger.warning(f"Database not available for LLM profile loading: {e}")

    # Fallback to environment variables
    logger.info("ğŸ”„ Falling back to environment variable LLM configuration")
    return ChatOpenAI(
        model=os.getenv("LLM_MODEL", "gpt-4.1-mini"),
        base_url=os.getenv("OPENAI_ENDPOINT", "https://api.openai.com/v1"),
        api_key=os.getenv("OPENAI_API_KEY", "")
    )


async def update_llm_from_profile(profile_name: str):
    """Update the global LLM instance from a specific profile"""
    global vibesurf_agent, llm, db_manager

    try:
        from .database.queries import LLMProfileQueries
        from .utils.llm_factory import create_llm_from_profile

        # Get database session from shared state db_manager
        if not db_manager:
            raise ValueError("Database manager not initialized")

        async for db in db_manager.get_session():
            try:
                # Get profile with decrypted API key
                profile_with_key = await LLMProfileQueries.get_profile_with_decrypted_key(db, profile_name)
                if not profile_with_key:
                    raise ValueError(f"LLM profile '{profile_name}' not found")

                # Create new LLM instance
                new_llm = create_llm_from_profile(profile_with_key)

                # Update global state
                llm = new_llm
                if vibesurf_agent and vibesurf_agent.token_cost_service:
                    # FIX: Register new LLM with token cost service to maintain tracking
                    vibesurf_agent.llm = vibesurf_agent.token_cost_service.register_llm(new_llm)

                logger.info(f"âœ… LLM updated to profile: {profile_name}")
                return True

            except Exception as e:
                logger.error(f"Failed to update LLM from profile {profile_name}: {e}")
                raise
            finally:
                break

    except Exception as e:
        logger.error(f"Database error while updating LLM profile: {e}")
        raise


def get_envs() -> Dict[str, str]:
    """Get the current environment variables dictionary"""
    global envs
    return envs.copy()


def update_envs(updates: Dict[str, str]) -> bool:
    """Update environment variables and save to envs.json"""
    global envs, workspace_dir
    
    try:
        # Update the envs dictionary
        envs.update(updates)
        
        # Save to envs.json
        envs_file_path = os.path.join(workspace_dir, "envs.json")
        with open(envs_file_path, 'w', encoding='utf-8') as f:
            json.dump(envs, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Updated and saved environment variables to {envs_file_path}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to update environment variables: {e}")
        return False


def _update_extension_backend_url(extension_path: str, backend_url: str):
    try:
        import re

        config_js_path = os.path.join(extension_path, "config.js")
        if not os.path.exists(config_js_path):
            logger.warning(f"Extension config.js not found at: {config_js_path}")
            return

        with open(config_js_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åŒ¹é… BACKEND_URL: 'xyz' æˆ– BACKEND_URL: "xyz"ï¼Œxyzæ˜¯ä»»æ„å†…å®¹
        pattern = r"BACKEND_URL:\s*(['\"]).*?\1"
        replacement = f"BACKEND_URL: '{backend_url}'"

        updated_content = re.sub(pattern, replacement, content)

        with open(config_js_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)

        logger.info(f"âœ… Updated extension backend URL to: {backend_url}")

    except Exception as e:
        logger.error(f"âŒ Failed to update extension backend URL: {e}")




================================================
FILE: vibe_surf/backend/voice_model_config.py
================================================
"""
Voice Model Configuration

Centralized configuration for all supported voice providers and their models.
"""

# Voice Providers and their supported models
VOICE_MODELS = {
    "qwen-asr": {
        "model_type": "asr",
        "requires_api_key": True,
        "provider": "qwen",
    },
    "openai-asr": {
        "model_type": "asr",
        "requires_api_key": True,
        "provider": "openai",
        "supports_base_url": True,
    },
    "gemini-asr": {
        "model_type": "asr",
        "requires_api_key": True,
        "provider": "gemini",
    }
}


================================================
FILE: vibe_surf/backend/api/__init__.py
================================================
"""
API Models and Request/Response schemas for VibeSurf Backend
"""


================================================
FILE: vibe_surf/backend/api/activity.py
================================================
"""
Activity Logs Router - Simplified

Handles retrieval of activity logs from VibeSurf agents and task history from database.
"""

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional
import logging
from datetime import datetime

from ..database import get_db_session
from ..database.queries import TaskQueries
from .models import ActivityQueryRequest, SessionActivityQueryRequest

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/activity", tags=["activity"])

# Task History Endpoints

@router.get("/tasks")
async def get_recent_tasks(
    limit: int = -1,
    db: AsyncSession = Depends(get_db_session)
):
    """Get recent tasks across all sessions"""
    try:
        # Handle -1 as "get all" and validate other values
        if limit != -1 and (limit < 1 or limit > 1000):
            limit = -1
            
        tasks = await TaskQueries.get_recent_tasks(db, limit)
        
        return {
            "tasks": [
                {
                    "task_id": task.task_id,
                    "session_id": task.session_id,
                    "task_description": task.task_description,
                    "status": task.status.value,
                    "task_result": task.task_result,
                    "error_message": task.error_message,
                    "report_path": task.report_path,
                    "created_at": task.created_at.isoformat(),
                    "started_at": task.started_at.isoformat() if task.started_at else None,
                    "completed_at": task.completed_at.isoformat() if task.completed_at else None
                }
                for task in tasks
            ],
            "total_count": len(tasks),
            "limit": limit
        }
    except Exception as e:
        logger.error(f"Failed to get recent tasks: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get recent tasks: {str(e)}")

@router.get("/sessions")
async def get_all_sessions(
    limit: int = -1,
    offset: int = 0,
    db: AsyncSession = Depends(get_db_session)
):
    """Get all sessions with task counts and metadata"""
    try:
        # Handle -1 as "get all" and validate other values
        if limit != -1 and (limit < 1 or limit > 1000):
            limit = -1
            
        sessions = await TaskQueries.get_all_sessions(db, limit, offset)
        
        return {
            "sessions": sessions,
            "total_count": len(sessions),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        logger.error(f"Failed to get all sessions: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get all sessions: {str(e)}")

@router.get("/sessions/{session_id}/tasks")
async def get_session_tasks(
    session_id: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Get all tasks for a session from database"""
    try:
        tasks = await TaskQueries.get_tasks_by_session(db, session_id)
        
        return {
            "session_id": session_id,
            "tasks": [
                {
                    "task_id": task.task_id,
                    "task_description": task.task_description,
                    "status": task.status.value,
                    "task_result": task.task_result,
                    "llm_profile_name": task.llm_profile_name,
                    "workspace_dir": task.workspace_dir,
                    "mcp_server_config": task.mcp_server_config,
                    "error_message": task.error_message,
                    "report_path": task.report_path,
                    "created_at": task.created_at.isoformat(),
                    "started_at": task.started_at.isoformat() if task.started_at else None,
                    "completed_at": task.completed_at.isoformat() if task.completed_at else None
                }
                for task in tasks
            ],
            "total_count": len(tasks)
        }
    except Exception as e:
        logger.error(f"Failed to get tasks for session {session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get session tasks: {str(e)}")

@router.get("/{task_id}")
async def get_task_info(
    task_id: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Get task information and result from database"""
    try:
        task = await TaskQueries.get_task(db, task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        return {
            "task_id": task.task_id,
            "session_id": task.session_id,
            "task_description": task.task_description,
            "status": task.status.value,
            "upload_files_path": task.upload_files_path,
            "mcp_server_config": task.mcp_server_config,
            "llm_profile_name": task.llm_profile_name,
            "task_result": task.task_result,
            "error_message": task.error_message,
            "report_path": task.report_path,
            "created_at": task.created_at.isoformat(),
            "started_at": task.started_at.isoformat() if task.started_at else None,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
            "metadata": task.task_metadata
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get task info for {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get task info: {str(e)}")

# Real-time VibeSurf Agent Activity Log Endpoints

@router.get("/sessions/{session_id}/activity")
async def get_session_activity_logs(
    session_id: str,
    query: SessionActivityQueryRequest = Depends()
):
    """Get real-time VibeSurf agent activity logs for a specific session"""
    from ..shared_state import vibesurf_agent
    
    if not vibesurf_agent:
        logger.error(f"âŒ VibeSurf agent not initialized")
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")
    
    try:
        # Get activity logs from VibeSurfAgent
        if query.message_index is not None:
            # First get all logs to check the current state
            all_logs = vibesurf_agent.get_activity_logs(session_id)
            
            # Get specific log entry by index
            activity_log = vibesurf_agent.get_activity_logs(session_id, query.message_index)
            
            if activity_log is None:
                return {
                    "session_id": session_id,
                    "activity_log": None,
                    "message_index": query.message_index,
                    "total_available": len(all_logs) if all_logs else 0,
                    "message": f"No activity log found at index {query.message_index}"
                }
            
            return {
                "session_id": session_id,
                "activity_log": activity_log,
                "message_index": query.message_index,
                "total_available": len(all_logs) if all_logs else 0
            }
        else:
            # Get all activity logs for the session
            activity_logs = vibesurf_agent.get_activity_logs(session_id)
            
            if activity_logs is None:
                return {
                    "session_id": session_id,
                    "activity_logs": [],
                    "total_count": 0,
                    "message": "No activity logs found for this session"
                }
            
            # Apply limit (-1 means no limit)
            original_count = len(activity_logs)
            if query.limit != -1 and query.limit > 0 and len(activity_logs) > query.limit:
                activity_logs = activity_logs[-query.limit:]
            
            return {
                "session_id": session_id,
                "activity_logs": activity_logs,
                "total_count": len(activity_logs),
                "original_total": original_count
            }
        
    except Exception as e:
        logger.error(f"âŒ Failed to get VibeSurf activity logs for session {session_id}: {e}")
        logger.exception("Full traceback:")
        raise HTTPException(status_code=500, detail=f"Failed to get activity logs: {str(e)}")


@router.get("/sessions/{session_id}/latest_activity")
async def get_latest_activity(session_id: str):
    """Get the latest activity for a session (both task info and VibeSurf logs)"""
    
    try:
        from ..shared_state import vibesurf_agent
        result = {
            "session_id": session_id,
            "latest_vibesurf_log": None,
            "latest_task": None
        }
        
        # Get latest VibeSurf activity log
        if vibesurf_agent:
            try:
                activity_logs = vibesurf_agent.get_activity_logs(session_id)
                if activity_logs:
                    result["latest_vibesurf_log"] = activity_logs[-1]
            except Exception as e:
                logger.warning(f"Failed to get VibeSurf activity for {session_id}: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"Failed to get latest activity for session {session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get latest activity: {str(e)}")



================================================
FILE: vibe_surf/backend/api/agent.py
================================================
"""
Agent API endpoints
"""
from typing import List
from fastapi import APIRouter, HTTPException

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/agent", tags=["agent"])


@router.get("/get_all_skills", response_model=List[str])
async def get_all_skills():
    """
    Get all available skill names from the VibeSurf tools registry.
    Returns skill names with the 'skill_' prefix removed.
    """
    try:
        from ..shared_state import vibesurf_tools
        if not vibesurf_tools:
            raise HTTPException(status_code=500, detail="VibeSurf tools not initialized")
        
        # Get all action names from the registry
        all_actions = vibesurf_tools.registry.registry.actions.keys()
        
        # Filter for actions that start with 'skill_' and remove the prefix
        skill_names = [
            action_name.replace('skill_', '') 
            for action_name in all_actions 
            if action_name.startswith('skill_')
        ]
        logger.info(skill_names)
        return skill_names
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get skills: {str(e)}")


================================================
FILE: vibe_surf/backend/api/browser.py
================================================
"""
Browser Tabs Router

Handles retrieval of browser tab information including active tab and all tabs.
"""

from fastapi import APIRouter, HTTPException
from typing import Dict, Any, Optional
import logging

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/browser", tags=["browser"])


@router.get("/active-tab")
async def get_active_tab() -> Dict[str, Dict[str, str]]:
    from ..shared_state import browser_manager
    """Get the current active tab information"""
    if not browser_manager:
        raise HTTPException(status_code=503, detail="Browser manager not initialized")

    try:
        # Get active tab info using browser manager
        active_tab_info = await browser_manager.get_activate_tab()

        if not active_tab_info:
            logger.info("No active tab found!")
            return {}

        logger.info(active_tab_info)
        # Return dict format: {tab_id: {url: , title: }}
        return {
            active_tab_info.target_id[:-4]: {
                "url": active_tab_info.url,
                "title": active_tab_info.title
            }
        }

    except Exception as e:
        logger.error(f"Failed to get active tab: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get active tab: {str(e)}")


@router.get("/all-tabs")
async def get_all_tabs() -> Dict[str, Dict[str, str]]:
    """Get all browser tabs information"""
    from ..shared_state import browser_manager

    if not browser_manager:
        raise HTTPException(status_code=503, detail="Browser manager not initialized")

    try:
        all_tab_infos = await browser_manager.get_all_tabs()

        # Filter only page targets and build result dict
        result = {}
        for tab_info in all_tab_infos:
            result[tab_info.target_id[-4:]] = {
                "url": tab_info.url,
                "title": tab_info.title
            }

        return result

    except Exception as e:
        logger.error(f"Failed to get all tabs: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get all tabs: {str(e)}")



================================================
FILE: vibe_surf/backend/api/config.py
================================================
"""
Configuration API endpoints for VibeSurf Backend

Handles LLM Profile and tools configuration management.
"""

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Dict, List
import logging

from ..database.manager import get_db_session
from ..database.queries import LLMProfileQueries, McpProfileQueries
from .models import (
    LLMProfileCreateRequest, LLMProfileUpdateRequest, LLMProfileResponse,
    McpProfileCreateRequest, McpProfileUpdateRequest, McpProfileResponse
)

router = APIRouter(prefix="/config", tags=["config"])

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

def _profile_to_response_dict(profile) -> dict:
    """Convert SQLAlchemy LLMProfile to dict for Pydantic validation - safe extraction"""
    try:
        # Use SQLAlchemy's __dict__ to avoid lazy loading issues
        profile_dict = profile.__dict__.copy()
        
        # Remove SQLAlchemy internal keys
        profile_dict.pop('_sa_instance_state', None)
        
        return {
            "profile_id": profile_dict.get("profile_id"),
            "profile_name": profile_dict.get("profile_name"),
            "provider": profile_dict.get("provider"),
            "model": profile_dict.get("model"),
            "base_url": profile_dict.get("base_url"),
            "temperature": profile_dict.get("temperature"),
            "max_tokens": profile_dict.get("max_tokens"),
            "top_p": profile_dict.get("top_p"),
            "frequency_penalty": profile_dict.get("frequency_penalty"),
            "seed": profile_dict.get("seed"),
            "provider_config": profile_dict.get("provider_config"),
            "description": profile_dict.get("description"),
            "is_active": profile_dict.get("is_active"),
            "is_default": profile_dict.get("is_default"),
            "created_at": profile_dict.get("created_at"),
            "updated_at": profile_dict.get("updated_at"),
            "last_used_at": profile_dict.get("last_used_at")
        }
    except Exception as e:
        # Fallback to direct attribute access if __dict__ approach fails
        return {
            "profile_id": str(profile.profile_id),
            "profile_name": str(profile.profile_name),
            "provider": str(profile.provider),
            "model": str(profile.model),
            "base_url": profile.base_url,
            "temperature": profile.temperature,
            "max_tokens": profile.max_tokens,
            "top_p": profile.top_p,
            "frequency_penalty": profile.frequency_penalty,
            "seed": profile.seed,
            "provider_config": profile.provider_config or {},
            "description": profile.description,
            "is_active": bool(profile.is_active),
            "is_default": bool(profile.is_default),
            "created_at": profile.created_at,
            "updated_at": profile.updated_at,
            "last_used_at": profile.last_used_at
        }

# LLM Profile Management
@router.post("/llm-profiles", response_model=LLMProfileResponse)
async def create_llm_profile(
    profile_request: LLMProfileCreateRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """Create a new LLM profile"""
    try:
        # Check if profile name already exists
        existing_profile = await LLMProfileQueries.get_profile(db, profile_request.profile_name)
        if existing_profile:
            raise HTTPException(
                status_code=400,
                detail=f"Profile with name '{profile_request.profile_name}' already exists"
            )
        
        # Create new profile - now returns dict directly
        profile_data = await LLMProfileQueries.create_profile(
            db=db,
            profile_name=profile_request.profile_name,
            provider=profile_request.provider,
            model=profile_request.model,
            api_key=profile_request.api_key,
            base_url=profile_request.base_url,
            temperature=profile_request.temperature,
            max_tokens=profile_request.max_tokens,
            top_p=profile_request.top_p,
            frequency_penalty=profile_request.frequency_penalty,
            seed=profile_request.seed,
            provider_config=profile_request.provider_config,
            description=profile_request.description,
            is_default=profile_request.is_default
        )
        
        await db.commit()
        
        # If this is set as default, update other profiles
        if profile_request.is_default:
            await LLMProfileQueries.set_default_profile(db, profile_request.profile_name)
            await db.commit()
        
        return LLMProfileResponse(**profile_data)
        
    except Exception as e:
        logger.error(f"Failed to create LLM profile: {e}")
        
        # Handle specific database constraint errors
        error_msg = str(e)
        if "UNIQUE constraint failed: llm_profiles.profile_name" in error_msg:
            raise HTTPException(
                status_code=400,
                detail=f"Profile with name '{profile_request.profile_name}' already exists. Please choose a different name."
            )
        elif "IntegrityError" in error_msg and "profile_name" in error_msg:
            raise HTTPException(
                status_code=400,
                detail=f"Profile name '{profile_request.profile_name}' is already in use. Please choose a different name."
            )
        else:
            raise HTTPException(
                status_code=500,
                detail=f"Failed to create LLM profile: {str(e)}"
            )

@router.get("/llm-profiles", response_model=List[LLMProfileResponse])
async def list_llm_profiles(
    active_only: bool = True,
    limit: int = 50,
    offset: int = 0,
    db: AsyncSession = Depends(get_db_session)
):
    """List LLM profiles"""
    try:
        profiles = await LLMProfileQueries.list_profiles(
            db=db,
            active_only=active_only,
            limit=limit,
            offset=offset
        )
        
        # Use safe extraction to avoid greenlet issues
        return [LLMProfileResponse(**_profile_to_response_dict(profile)) for profile in profiles]
        
    except Exception as e:
        logger.error(f"Failed to list LLM profiles: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to list LLM profiles: {str(e)}"
        )

@router.get("/llm-profiles/{profile_name}", response_model=LLMProfileResponse)
async def get_llm_profile(
    profile_name: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Get specific LLM profile by name"""
    try:
        profile = await LLMProfileQueries.get_profile(db, profile_name)
        if not profile:
            raise HTTPException(
                status_code=404,
                detail=f"LLM profile '{profile_name}' not found"
            )
        
        # Use safe extraction to avoid greenlet issues
        return LLMProfileResponse(**_profile_to_response_dict(profile))
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get LLM profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get LLM profile: {str(e)}"
        )

@router.put("/llm-profiles/{profile_name}", response_model=LLMProfileResponse)
async def update_llm_profile(
    profile_name: str,
    update_request: LLMProfileUpdateRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """Update an existing LLM profile"""
    try:
        # Check if profile exists
        existing_profile = await LLMProfileQueries.get_profile(db, profile_name)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"LLM profile '{profile_name}' not found"
            )
        
        # Prepare update data
        update_data = {}
        for field, value in update_request.model_dump(exclude_unset=True).items():
            if value is not None:
                update_data[field] = value
        
        if not update_data:
            raise HTTPException(
                status_code=400,
                detail="No valid fields provided for update"
            )
        
        # Update profile
        success = await LLMProfileQueries.update_profile(db, profile_name, update_data)
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to update profile"
            )
        
        await db.commit()
        
        # Handle default profile setting
        if update_request.is_default:
            await LLMProfileQueries.set_default_profile(db, profile_name)
            await db.commit()
        
        # Return updated profile
        updated_profile = await LLMProfileQueries.get_profile(db, profile_name)
        from ..shared_state import current_llm_profile_name
        if current_llm_profile_name != profile_name:
            current_llm_profile_name = None
        # Use safe extraction to avoid greenlet issues
        return LLMProfileResponse(**_profile_to_response_dict(updated_profile))
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update LLM profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update LLM profile: {str(e)}"
        )

@router.delete("/llm-profiles/{profile_name}")
async def delete_llm_profile(
    profile_name: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Delete an LLM profile"""
    try:
        # Check if profile exists
        existing_profile = await LLMProfileQueries.get_profile(db, profile_name)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"LLM profile '{profile_name}' not found"
            )
        
        # Don't allow deletion of default profile
        if existing_profile.is_default:
            raise HTTPException(
                status_code=400,
                detail="Cannot delete the default profile. Set another profile as default first."
            )
        
        # TODO: Check if profile is being used by any active tasks
        # This would require checking the tasks table
        
        success = await LLMProfileQueries.delete_profile(db, profile_name)
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to delete profile"
            )
        
        await db.commit()
        
        return JSONResponse(
            content={"message": f"LLM profile '{profile_name}' deleted successfully"},
            status_code=200
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete LLM profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete LLM profile: {str(e)}"
        )

@router.post("/llm-profiles/{profile_name}/set-default")
async def set_default_llm_profile(
    profile_name: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Set an LLM profile as the default"""
    try:
        # Check if profile exists and is active
        profile = await LLMProfileQueries.get_profile(db, profile_name)
        if not profile:
            raise HTTPException(
                status_code=404,
                detail=f"LLM profile '{profile_name}' not found"
            )
        
        if not profile.is_active:
            raise HTTPException(
                status_code=400,
                detail="Cannot set inactive profile as default"
            )
        
        success = await LLMProfileQueries.set_default_profile(db, profile_name)
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to set default profile"
            )
        
        await db.commit()
        
        return JSONResponse(
            content={"message": f"LLM profile '{profile_name}' set as default"},
            status_code=200
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to set default LLM profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to set default LLM profile: {str(e)}"
        )

@router.get("/llm-profiles/default/current", response_model=LLMProfileResponse)
async def get_default_llm_profile(db: AsyncSession = Depends(get_db_session)):
    """Get the current default LLM profile"""
    try:
        profile = await LLMProfileQueries.get_default_profile(db)
        if not profile:
            raise HTTPException(
                status_code=404,
                detail="No default LLM profile found"
            )
        
        # Use safe extraction to avoid greenlet issues
        return LLMProfileResponse(**_profile_to_response_dict(profile))
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get default LLM profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get default LLM profile: {str(e)}"
        )

# MCP Profile Management
def _mcp_profile_to_response_dict(profile) -> dict:
    """Convert SQLAlchemy McpProfile to dict for Pydantic validation - safe extraction"""
    try:
        # Use SQLAlchemy's __dict__ to avoid lazy loading issues
        profile_dict = profile.__dict__.copy()
        
        # Remove SQLAlchemy internal keys
        profile_dict.pop('_sa_instance_state', None)
        
        return {
            "mcp_id": profile_dict.get("mcp_id"),
            "display_name": profile_dict.get("display_name"),
            "mcp_server_name": profile_dict.get("mcp_server_name"),
            "mcp_server_params": profile_dict.get("mcp_server_params"),
            "description": profile_dict.get("description"),
            "is_active": profile_dict.get("is_active"),
            "created_at": profile_dict.get("created_at"),
            "updated_at": profile_dict.get("updated_at"),
            "last_used_at": profile_dict.get("last_used_at")
        }
    except Exception as e:
        # Fallback to direct attribute access if __dict__ approach fails
        return {
            "mcp_id": str(profile.mcp_id),
            "display_name": str(profile.display_name),
            "mcp_server_name": str(profile.mcp_server_name),
            "mcp_server_params": profile.mcp_server_params or {},
            "description": profile.description,
            "is_active": bool(profile.is_active),
            "created_at": profile.created_at,
            "updated_at": profile.updated_at,
            "last_used_at": profile.last_used_at
        }

@router.post("/mcp-profiles", response_model=McpProfileResponse)
async def create_mcp_profile(
    profile_request: McpProfileCreateRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """Create a new MCP profile"""
    try:
        # Check if display name already exists
        existing_profile = await McpProfileQueries.get_profile_by_display_name(db, profile_request.display_name)
        if existing_profile:
            raise HTTPException(
                status_code=400,
                detail=f"MCP Profile with display name '{profile_request.display_name}' already exists"
            )
        
        # Create new profile
        profile_data = await McpProfileQueries.create_profile(
            db=db,
            display_name=profile_request.display_name,
            mcp_server_name=profile_request.mcp_server_name,
            mcp_server_params=profile_request.mcp_server_params,
            description=profile_request.description
        )
        
        await db.commit()
        
        return McpProfileResponse(**profile_data)
        
    except Exception as e:
        logger.error(f"Failed to create MCP profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create MCP profile: {str(e)}"
        )

@router.get("/mcp-profiles", response_model=List[McpProfileResponse])
async def list_mcp_profiles(
    active_only: bool = True,
    limit: int = 50,
    offset: int = 0,
    db: AsyncSession = Depends(get_db_session)
):
    """List MCP profiles"""
    try:
        profiles = await McpProfileQueries.list_profiles(
            db=db,
            active_only=active_only,
            limit=limit,
            offset=offset
        )
        
        return [McpProfileResponse(**_mcp_profile_to_response_dict(profile)) for profile in profiles]
        
    except Exception as e:
        logger.error(f"Failed to list MCP profiles: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to list MCP profiles: {str(e)}"
        )

@router.get("/mcp-profiles/{mcp_id}", response_model=McpProfileResponse)
async def get_mcp_profile(
    mcp_id: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Get specific MCP profile by ID"""
    try:
        profile = await McpProfileQueries.get_profile(db, mcp_id)
        if not profile:
            raise HTTPException(
                status_code=404,
                detail=f"MCP profile '{mcp_id}' not found"
            )
        
        return McpProfileResponse(**_mcp_profile_to_response_dict(profile))
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get MCP profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get MCP profile: {str(e)}"
        )

@router.put("/mcp-profiles/{mcp_id}", response_model=McpProfileResponse)
async def update_mcp_profile(
    mcp_id: str,
    update_request: McpProfileUpdateRequest,
    db: AsyncSession = Depends(get_db_session)
):
    """Update an existing MCP profile"""
    try:
        logger.info(f"Updating MCP profile {mcp_id}")
        
        # Check if profile exists
        existing_profile = await McpProfileQueries.get_profile(db, mcp_id)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"MCP profile '{mcp_id}' not found"
            )
        
        # Prepare update data
        update_data = {}
        for field, value in update_request.model_dump(exclude_unset=True).items():
            if value is not None:
                update_data[field] = value
        
        if not update_data:
            raise HTTPException(
                status_code=400,
                detail="No valid fields provided for update"
            )
        
        # Update profile
        success = await McpProfileQueries.update_profile(db, mcp_id, update_data)
        
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to update profile"
            )
        
        await db.commit()
        
        # Return updated profile
        updated_profile = await McpProfileQueries.get_profile(db, mcp_id)
        response_data = _mcp_profile_to_response_dict(updated_profile)
        
        return McpProfileResponse(**response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update MCP profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update MCP profile: {str(e)}"
        )

@router.delete("/mcp-profiles/{mcp_id}")
async def delete_mcp_profile(
    mcp_id: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Delete an MCP profile"""
    try:
        # Check if profile exists
        existing_profile = await McpProfileQueries.get_profile(db, mcp_id)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"MCP profile '{mcp_id}' not found"
            )
        
        # TODO: Check if profile is being used by any active tasks
        # This would require checking the tasks table
        
        success = await McpProfileQueries.delete_profile(db, mcp_id)
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to delete profile"
            )
        
        await db.commit()
        
        return JSONResponse(
            content={"message": f"MCP profile '{existing_profile.display_name}' deleted successfully"},
            status_code=200
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete MCP profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete MCP profile: {str(e)}"
        )

@router.get("/llm/providers")
async def get_available_providers():
    """Get list of available LLM providers"""
    from ..llm_config import get_supported_providers, get_provider_models, get_provider_metadata
    
    providers = []
    for provider_name in get_supported_providers():
        metadata = get_provider_metadata(provider_name)
        models = get_provider_models(provider_name)
        
        provider_info = {
            "name": provider_name,
            "display_name": metadata.get("display_name", provider_name.title()),
            "models": models,
            "model_count": len(models),
            "requires_api_key": metadata.get("requires_api_key", True),
            "supports_base_url": metadata.get("supports_base_url", False),
            "requires_base_url": metadata.get("requires_base_url", False),
            "supports_tools": metadata.get("supports_tools", False),
            "supports_vision": metadata.get("supports_vision", False),
            "default_model": metadata.get("default_model", "")
        }
        
        # Add default base URL if available
        if "default_base_url" in metadata:
            provider_info["default_base_url"] = metadata["default_base_url"]
        if "base_url" in metadata:
            provider_info["base_url"] = metadata["base_url"]
            
        providers.append(provider_info)
    
    return {
        "providers": providers,
        "total_providers": len(providers)
    }

@router.get("/llm/providers/{provider_name}/models")
async def get_provider_models_endpoint(provider_name: str):
    """Get models for a specific LLM provider"""
    from ..llm_config import get_provider_models as get_models, get_provider_metadata, is_provider_supported
    
    if not is_provider_supported(provider_name):
        raise HTTPException(
            status_code=404,
            detail=f"Provider '{provider_name}' not found or not supported"
        )
    
    models = get_models(provider_name)
    metadata = get_provider_metadata(provider_name)
    
    return {
        "provider": provider_name,
        "display_name": metadata.get("display_name", provider_name.title()),
        "models": models,
        "model_count": len(models),
        "default_model": metadata.get("default_model", ""),
        "metadata": metadata
    }

# Configuration status endpoints
@router.get("/status")
async def get_configuration_status(db: AsyncSession = Depends(get_db_session)):
    """Get overall configuration status including LLM profiles"""
    try:
        from .. import shared_state
        
        # Get LLM profiles info
        total_profiles = len(await LLMProfileQueries.list_profiles(db, active_only=False))
        active_profiles = len(await LLMProfileQueries.list_profiles(db, active_only=True))
        default_profile = await LLMProfileQueries.get_default_profile(db)
        
        status = {
            "llm_profiles": {
                "total_profiles": total_profiles,
                "active_profiles": active_profiles,
                "default_profile": default_profile.profile_name if default_profile else None,
                "has_default": default_profile is not None
            },
            "tools": {
                "initialized": shared_state.vibesurf_tools is not None
            },
            "browser_manager": {
                "initialized": shared_state.browser_manager is not None
            },
            "vibesurf_agent": {
                "initialized": shared_state.vibesurf_agent is not None,
                "workspace_dir": shared_state.workspace_dir
            },
            "overall_status": "ready" if (
                    default_profile and
                    shared_state.vibesurf_tools and
                    shared_state.browser_manager and
                    shared_state.vibesurf_agent
            ) else "partial"
        }
        
        return status
        
    except Exception as e:
        logger.error(f"Failed to get configuration status: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get configuration status: {str(e)}"
        )

# Environment Variables Management
@router.get("/environments")
async def get_environments():
    """Get current environment variables"""
    try:
        from .. import shared_state
        envs = shared_state.get_envs()
        
        return {
            "environments": envs,
            "count": len(envs)
        }
        
    except Exception as e:
        logger.error(f"Failed to get environments: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get environments: {str(e)}"
        )

@router.put("/environments")
async def update_environments(updates: Dict[str, str]):
    """Update environment variables"""
    try:
        from .. import shared_state
        
        # Validate that we only update allowed keys
        allowed_keys = {
            "BROWSER_EXECUTION_PATH",
            "BROWSER_USER_DATA",
            "VIBESURF_EXTENSION",
            "VIBESURF_BACKEND_URL"
        }
        
        # Filter updates to only include allowed keys
        filtered_updates = {
            key: value for key, value in updates.items()
            if key in allowed_keys
        }
        
        if not filtered_updates:
            raise HTTPException(
                status_code=400,
                detail=f"No valid environment variables provided. Allowed keys: {list(allowed_keys)}"
            )
        
        # Update environment variables
        success = shared_state.update_envs(filtered_updates)
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to update environment variables"
            )
        
        # Return updated environments
        updated_envs = shared_state.get_envs()
        
        return {
            "message": "Environment variables updated successfully",
            "updated_keys": list(filtered_updates.keys()),
            "environments": updated_envs
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update environments: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update environments: {str(e)}"
        )


================================================
FILE: vibe_surf/backend/api/files.py
================================================
"""
File Upload and Management Router

Handles file uploads to workspace directories, file retrieval, and listing
of uploaded files for VibeSurf sessions.
"""

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from fastapi.responses import FileResponse
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional, Dict, Any
import os
import shutil
import logging
from datetime import datetime
from uuid_extensions import uuid7str
import mimetypes
from pathlib import Path

from ..database import get_db_session
from ..database.queries import UploadedFileQueries
from .models import FileListQueryRequest, SessionFilesQueryRequest

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/files", tags=["files"])


def get_upload_directory(session_id: Optional[str] = None) -> str:
    from ..shared_state import workspace_dir
    """Get the upload directory path for a session or global uploads"""
    if session_id:
        upload_dir = os.path.join(workspace_dir, "sessions", session_id, "upload_files")
    else:
        upload_dir = os.path.join(workspace_dir, "sessions", "upload_files")

    # Create directory if it doesn't exist
    os.makedirs(upload_dir, exist_ok=True)
    return upload_dir


def is_safe_path(basedir: str, path: str) -> bool:
    """Check if the path is safe (within basedir)"""
    try:
        # Resolve both paths to absolute paths
        basedir = os.path.abspath(basedir)
        path = os.path.abspath(path)

        # Check if path starts with basedir
        return path.startswith(basedir)
    except:
        return False


@router.post("/upload")
async def upload_files(
        files: List[UploadFile] = File(...),
        session_id: Optional[str] = Form(None),
        db: AsyncSession = Depends(get_db_session)
):
    """Upload files to workspace/upload_files folder or session-specific folder"""
    try:
        from ..shared_state import workspace_dir

        upload_dir = get_upload_directory(session_id)
        uploaded_file_info = []

        for file in files:
            if not file.filename:
                continue

            # Generate unique file ID
            file_id = uuid7str()

            # Create safe filename
            filename = file.filename
            file_path = os.path.join(upload_dir, filename)

            # Handle duplicate filenames by adding suffix
            counter = 1
            base_name, ext = os.path.splitext(filename)
            while os.path.exists(file_path):
                new_filename = f"{base_name}_{counter}{ext}"
                file_path = os.path.join(upload_dir, new_filename)
                filename = new_filename
                counter += 1

            # Ensure path is safe
            if not is_safe_path(upload_dir, file_path):
                raise HTTPException(status_code=400, detail=f"Invalid file path: {filename}")

            # Save file
            try:
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)

                # Get file info
                file_size = os.path.getsize(file_path)
                mime_type, _ = mimetypes.guess_type(file_path)
                relative_path = os.path.relpath(file_path, workspace_dir)

                # Store file metadata in database
                uploaded_file = await UploadedFileQueries.create_file_record(
                    db=db,
                    file_id=file_id,
                    original_filename=file.filename,
                    stored_filename=filename,
                    file_path=file_path,
                    session_id=session_id,
                    file_size=file_size,
                    mime_type=mime_type or "application/octet-stream",
                    relative_path=relative_path
                )

                # Create response metadata
                file_metadata = {
                    "file_id": uploaded_file.file_id,
                    "original_filename": uploaded_file.original_filename,
                    "stored_filename": uploaded_file.stored_filename,
                    "session_id": uploaded_file.session_id,
                    "file_size": uploaded_file.file_size,
                    "mime_type": uploaded_file.mime_type,
                    "upload_time": uploaded_file.upload_time.isoformat(),
                    "file_path": file_path
                }

                uploaded_file_info.append(file_metadata)

                logger.info(f"File uploaded: {filename} (ID: {file_id}) to {upload_dir}")

            except Exception as e:
                logger.error(f"Failed to save file {filename}: {e}")
                # If database record was created but file save failed, clean up
                try:
                    await UploadedFileQueries.hard_delete_file(db, file_id)
                except:
                    pass
                raise HTTPException(status_code=500, detail=f"Failed to save file {filename}: {str(e)}")

        # Commit all database changes
        await db.commit()

        return {
            "message": f"Successfully uploaded {len(uploaded_file_info)} files",
            "files": uploaded_file_info,
            "upload_directory": upload_dir
        }

    except Exception as e:
        logger.error(f"File upload failed: {e}")
        raise HTTPException(status_code=500, detail=f"File upload failed: {str(e)}")


@router.get("/{file_id}")
async def download_file(file_id: str, db: AsyncSession = Depends(get_db_session)):
    """Download file by file ID"""
    from ..shared_state import workspace_dir

    uploaded_file = await UploadedFileQueries.get_file(db, file_id)
    if not uploaded_file:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = uploaded_file.file_path

    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found on disk")

    # Ensure path is safe
    if not is_safe_path(workspace_dir, file_path):
        raise HTTPException(status_code=403, detail="Access denied")

    return FileResponse(
        path=file_path,
        filename=uploaded_file.original_filename,
        media_type=uploaded_file.mime_type
    )


@router.get("")
async def list_uploaded_files(
        query: FileListQueryRequest = Depends(),
        db: AsyncSession = Depends(get_db_session)
):
    """List uploaded files, optionally filtered by session"""
    try:
        # Get files from database
        uploaded_files = await UploadedFileQueries.list_files(
            db=db,
            session_id=query.session_id,
            limit=query.limit,
            offset=query.offset,
            active_only=True
        )

        # Get total count
        total_count = await UploadedFileQueries.count_files(
            db=db,
            session_id=query.session_id,
            active_only=True
        )

        # Convert to response format (exclude file_path for security)
        files_response = []
        for file_record in uploaded_files:
            files_response.append({
                "file_id": file_record.file_id,
                "original_filename": file_record.original_filename,
                "stored_filename": file_record.stored_filename,
                "session_id": file_record.session_id,
                "file_size": file_record.file_size,
                "mime_type": file_record.mime_type,
                "upload_time": file_record.upload_time.isoformat(),
                "file_path": file_record.file_path
            })

        return {
            "files": files_response,
            "total_count": total_count,
            "limit": query.limit,
            "offset": query.offset,
            "has_more": query.limit != -1 and (query.offset + query.limit < total_count),
            "session_id": query.session_id
        }

    except Exception as e:
        logger.error(f"Failed to list files: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list files: {str(e)}")


@router.delete("/{file_id}")
async def delete_file(file_id: str, db: AsyncSession = Depends(get_db_session)):
    """Delete uploaded file by file ID"""
    # Get file record
    uploaded_file = await UploadedFileQueries.get_file(db, file_id)
    if not uploaded_file:
        raise HTTPException(status_code=404, detail="File not found")

    try:
        # Remove file from disk
        if os.path.exists(uploaded_file.file_path):
            os.remove(uploaded_file.file_path)

        # Soft delete from database
        success = await UploadedFileQueries.delete_file(db, file_id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to delete file record")

        await db.commit()

        return {
            "message": f"File {uploaded_file.original_filename} deleted successfully",
            "file_id": file_id
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete file {file_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")


@router.get("/session/{session_id}")
async def list_session_files(
        session_id: str,
        query: SessionFilesQueryRequest = Depends()
):
    """List all files in a session directory"""
    try:
        from ..shared_state import workspace_dir
        session_dir = os.path.join(workspace_dir, session_id)

        if not os.path.exists(session_dir):
            return {
                "session_id": session_id,
                "files": [],
                "directories": [],
                "message": "Session directory not found"
            }

        files = []
        directories = []

        for root, dirs, filenames in os.walk(session_dir):
            # Calculate relative path from session directory
            rel_root = os.path.relpath(root, session_dir)
            if rel_root == ".":
                rel_root = ""

            # Add directories if requested
            if query.include_directories:
                for dirname in dirs:
                    dir_path = os.path.join(rel_root, dirname) if rel_root else dirname
                    directories.append({
                        "name": dirname,
                        "path": dir_path,
                        "type": "directory"
                    })

            # Add files
            for filename in filenames:
                file_path = os.path.join(root, filename)
                rel_path = os.path.join(rel_root, filename) if rel_root else filename

                try:
                    stat = os.stat(file_path)
                    mime_type, _ = mimetypes.guess_type(file_path)

                    files.append({
                        "name": filename,
                        "path": rel_path,
                        "size": stat.st_size,
                        "mime_type": mime_type or "application/octet-stream",
                        "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "type": "file"
                    })
                except Exception as e:
                    logger.warning(f"Could not get stats for file {file_path}: {e}")

        return {
            "session_id": session_id,
            "files": files,
            "directories": directories if query.include_directories else [],
            "total_files": len(files),
            "total_directories": len(directories) if query.include_directories else 0
        }

    except Exception as e:
        logger.error(f"Failed to list session files for {session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list session files: {str(e)}")



================================================
FILE: vibe_surf/backend/api/models.py
================================================
"""
API Request/Response Models for VibeSurf Backend

Pydantic models for API serialization and validation.
With LLM Profile management support.
"""

from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

# LLM Profile Models
class LLMProfileCreateRequest(BaseModel):
    """Request model for creating a new LLM profile"""
    profile_name: str = Field(description="Unique profile name", min_length=1, max_length=100)
    provider: str = Field(description="LLM provider (openai, anthropic, google, azure_openai)")
    model: str = Field(description="Model name")
    api_key: Optional[str] = Field(default=None, description="API key (will be encrypted)")
    base_url: Optional[str] = Field(default=None, description="Custom base URL")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, gt=0)
    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0)
    frequency_penalty: Optional[float] = Field(default=None, ge=-2.0, le=2.0)
    seed: Optional[int] = Field(default=None)
    provider_config: Optional[Dict[str, Any]] = Field(default=None, description="Provider-specific config")
    description: Optional[str] = Field(default=None, description="Profile description")
    is_default: bool = Field(default=False, description="Set as default profile")

class LLMProfileUpdateRequest(BaseModel):
    """Request model for updating an LLM profile"""
    provider: Optional[str] = None
    model: Optional[str] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, gt=0)
    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0)
    frequency_penalty: Optional[float] = Field(default=None, ge=-2.0, le=2.0)
    seed: Optional[int] = None
    provider_config: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    is_active: Optional[bool] = None
    is_default: Optional[bool] = None

class LLMProfileResponse(BaseModel):
    """Response model for LLM profile data (without API key)"""
    profile_id: str
    profile_name: str
    provider: str
    model: str
    base_url: Optional[str] = None
    # Note: API key is intentionally excluded from response
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    seed: Optional[int] = None
    provider_config: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    is_active: bool
    is_default: bool
    created_at: datetime
    updated_at: datetime
    last_used_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# MCP Profile Models
class McpProfileCreateRequest(BaseModel):
    """Request model for creating a new MCP profile"""
    display_name: str = Field(description="Display name for MCP profile", min_length=1, max_length=100)
    mcp_server_name: str = Field(description="MCP server name/identifier", min_length=1, max_length=100)
    mcp_server_params: Dict[str, Any] = Field(description="MCP server parameters (command, args, etc.)")
    description: Optional[str] = Field(default=None, description="Profile description")

class McpProfileUpdateRequest(BaseModel):
    """Request model for updating an MCP profile"""
    display_name: Optional[str] = Field(default=None, min_length=1, max_length=100)
    mcp_server_name: Optional[str] = Field(default=None, min_length=1, max_length=100)
    mcp_server_params: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    is_active: Optional[bool] = None

class McpProfileResponse(BaseModel):
    """Response model for MCP profile data"""
    mcp_id: str
    display_name: str
    mcp_server_name: str
    mcp_server_params: Dict[str, Any]
    description: Optional[str] = None
    is_active: bool
    created_at: datetime
    updated_at: datetime
    last_used_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# Task Models
class TaskCreateRequest(BaseModel):
    """Request model for creating a new task"""
    session_id: str = Field(description="Session identifier")
    task_description: str = Field(description="The task description")
    llm_profile_name: str = Field(description="LLM profile name to use")
    upload_files_path: Optional[str] = Field(default=None, description="Path to uploaded files")
    mcp_server_config: Optional[Dict[str, Any]] = Field(default=None, description="MCP server configuration")
    agent_mode: str = Field(default="thinking", description="Agent mode: 'thinking', 'no-thinking', or 'flash'")

class TaskControlRequest(BaseModel):
    """Request model for task control operations (pause/resume/stop)"""
    reason: Optional[str] = Field(default=None, description="Reason for the operation")

class TaskResponse(BaseModel):
    """Response model for task data"""
    task_id: str
    session_id: str
    task_description: str
    status: str
    llm_profile_name: str
    upload_files_path: Optional[str] = None
    workspace_dir: Optional[str] = None
    mcp_server_config: Optional[Dict[str, Any]] = None
    agent_mode: str = "thinking"
    task_result: Optional[str] = None
    error_message: Optional[str] = None
    report_path: Optional[str] = None
    created_at: datetime
    updated_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    task_metadata: Optional[Dict[str, Any]] = None
    
    class Config:
        from_attributes = True
    
    @classmethod
    def from_orm(cls, task):
        """Create response from SQLAlchemy Task model"""
        return cls(
            task_id=task.task_id,
            session_id=task.session_id,
            task_description=task.task_description,
            status=task.status.value,
            llm_profile_name=task.llm_profile_name,
            upload_files_path=task.upload_files_path,
            workspace_dir=task.workspace_dir,
            mcp_server_config=task.mcp_server_config,
            agent_mode=task.agent_mode,
            task_result=task.task_result,
            error_message=task.error_message,
            report_path=task.report_path,
            created_at=task.created_at,
            updated_at=task.updated_at,
            started_at=task.started_at,
            completed_at=task.completed_at,
            task_metadata=task.task_metadata
        )

class TaskStatusResponse(BaseModel):
    """Response model for task status information"""
    task_id: Optional[str] = None
    session_id: Optional[str] = None
    status: Optional[str] = None
    task_description: Optional[str] = None
    created_at: Optional[datetime] = None
    started_at: Optional[datetime] = None
    error_message: Optional[str] = None
    is_running: bool = False

class TaskListResponse(BaseModel):
    """Response model for task list"""
    tasks: List[TaskResponse]
    total_count: int
    session_id: Optional[str] = None

class ErrorResponse(BaseModel):
    """Standard error response model"""
    error: str
    detail: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.now)

class ControlOperationResponse(BaseModel):
    """Response model for control operations"""
    success: bool
    message: str
    operation: str
    timestamp: datetime
    details: Optional[Dict[str, Any]] = None

# Activity Log Models (for VibeSurf agent activity)
class ActivityLogEntry(BaseModel):
    """Model for VibeSurf agent activity log entry"""
    timestamp: datetime
    level: str
    message: str
    metadata: Optional[Dict[str, Any]] = None

class ActivityLogResponse(BaseModel):
    """Response model for activity logs"""
    logs: List[ActivityLogEntry]
    total_count: int
    session_id: Optional[str] = None
    task_id: Optional[str] = None

# Activity API Request Models
class ActivityQueryRequest(BaseModel):
    """Request model for getting recent tasks"""
    limit: int = Field(default=-1, ge=-1, le=1000, description="Number of recent tasks to retrieve (-1 for all)")

class SessionActivityQueryRequest(BaseModel):
    """Request model for getting session activity logs"""
    limit: int = Field(default=-1, ge=-1, le=1000, description="Number of activity logs to retrieve (-1 for all)")
    message_index: Optional[int] = Field(default=None, ge=0, description="Specific message index to retrieve")

# File API Request Models
class FileUploadRequest(BaseModel):
    """Request model for file upload (for form validation)"""
    session_id: Optional[str] = Field(default=None, description="Session ID for file association")

class FileListQueryRequest(BaseModel):
    """Request model for listing uploaded files"""
    session_id: Optional[str] = Field(default=None, description="Filter by session ID")
    limit: int = Field(default=-1, ge=-1, le=1000, description="Number of files to retrieve (-1 for all)")
    offset: int = Field(default=0, ge=0, description="Number of files to skip")

class SessionFilesQueryRequest(BaseModel):
    """Request model for listing session files"""
    include_directories: bool = Field(default=False, description="Whether to include directories in the response")

# File Upload Models
class UploadedFileResponse(BaseModel):
    """Response model for uploaded file information"""
    filename: str
    file_path: str
    file_size: Optional[int] = None
    mime_type: Optional[str] = None
    uploaded_at: datetime

# Configuration Models (for config endpoints)
class LLMConfigResponse(BaseModel):
    """Response model for LLM configuration"""
    provider: str
    model: str
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    available_providers: List[str] = []

class ControllerConfigRequest(BaseModel):
    """Request model for updating tools configuration"""
    exclude_actions: Optional[List[str]] = Field(default=None, description="Actions to exclude from execution")
    max_actions_per_task: Optional[int] = Field(default=None, gt=0, description="Maximum actions per task")
    display_files_in_done_text: Optional[bool] = Field(default=None, description="Whether to display files in done text")

class ControllerConfigResponse(BaseModel):
    """Response model for tools configuration"""
    exclude_actions: List[str] = []
    max_actions_per_task: int = 100
    display_files_in_done_text: bool = True


================================================
FILE: vibe_surf/backend/api/task.py
================================================
"""
VibeSurf Agent Execution Router

Handles task submission, execution control (pause/resume/stop), and status monitoring
for VibeSurf agents.
"""

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List, Optional, Dict, Any
import logging
import os
from datetime import datetime
from uuid_extensions import uuid7str

from ..database import get_db_session
from .models import TaskCreateRequest, TaskControlRequest

# Import global variables and functions from shared_state
from ..shared_state import (
    execute_task_background,
    is_task_running,
    get_active_task_info,
    clear_active_task,
)

from vibe_surf.logger import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/tasks", tags=["tasks"])


@router.get("/status")
async def check_task_status():
    """Quick check if a task is currently running"""
    return {
        "has_active_task": is_task_running(),
        "active_task": get_active_task_info()
    }


@router.post("/submit")
async def submit_task(
        task_request: "TaskCreateRequest",
        background_tasks: BackgroundTasks,
        db: AsyncSession = Depends(get_db_session)
):
    """Submit new task for execution (single task mode)"""
    from ..database.queries import LLMProfileQueries
    from ..shared_state import workspace_dir, active_task, llm, current_llm_profile_name

    # Check if task is already running
    if is_task_running():
        current_task = get_active_task_info()
        return {
            "success": False,
            "message": "Cannot submit task: another task is currently running",
            "active_task": {
                "task_id": current_task.get("task_id"),
                "status": current_task.get("status"),
                "session_id": current_task.get("session_id"),
                "start_time": current_task.get("start_time").isoformat() if current_task.get("start_time") else None
            }
        }

    try:
        # Get LLM profile from database
        llm_profile = await LLMProfileQueries.get_profile_with_decrypted_key(db, task_request.llm_profile_name)
        if not llm_profile:
            active_task = None
            return {
                "success": False,
                "error": "llm_connection_failed",
                "message": f"Failed to get LLM profile with decrypted key {task_request.llm_profile_name}",
                "llm_profile": task_request.llm_profile_name
            }

        # Initialize LLM for this task if needed
        if not current_llm_profile_name or current_llm_profile_name != task_request.llm_profile_name:
            current_llm_profile_name = task_request.llm_profile_name
            success, message = await _ensure_llm_initialized(llm_profile)
            logger.info("Test LLM Connection!")
            if not success:
                active_task = None
                return {
                    "success": False,
                    "error": "llm_connection_failed",
                    "message": f"Cannot connect to LLM API: {message}",
                    "llm_profile": task_request.llm_profile_name
                }
        # Generate task ID
        task_id = uuid7str()

        # Get MCP server config for saving
        from ..shared_state import vibesurf_tools, active_mcp_server
        mcp_server_config = task_request.mcp_server_config
        if not mcp_server_config and vibesurf_tools and hasattr(vibesurf_tools, 'mcp_server_config'):
            mcp_server_config = vibesurf_tools.mcp_server_config

        # Ensure we have a valid MCP server config (never None)
        if mcp_server_config is None:
            mcp_server_config = {"mcpServers": {}}
            logger.info("Using default empty MCP server configuration")

        # DEBUG: Log the type and content of mcp_server_config
        logger.debug(f"mcp_server_config type: {type(mcp_server_config)}, value: {mcp_server_config}")

        # Create initial task record in database
        from ..database.queries import TaskQueries
        await TaskQueries.save_task(
            db,
            task_id=task_id,
            session_id=task_request.session_id,
            task_description=task_request.task_description,
            upload_files_path=task_request.upload_files_path,
            mcp_server_config=mcp_server_config,
            llm_profile_name=task_request.llm_profile_name,
            workspace_dir=workspace_dir,
            task_status="pending",
            agent_mode=task_request.agent_mode
        )
        await db.commit()

        # Add background task
        background_tasks.add_task(
            execute_task_background,
            task_id=task_id,
            session_id=task_request.session_id,
            task=task_request.task_description,
            llm_profile_name=task_request.llm_profile_name,
            upload_files=task_request.upload_files_path,
            agent_mode=task_request.agent_mode,
            db_session=db
        )

        return {
            "success": True,
            "task_id": task_id,
            "session_id": task_request.session_id,
            "status": "submitted",
            "message": "Task submitted for execution",
            "llm_profile": task_request.llm_profile_name,
            "workspace_dir": workspace_dir
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to submit task: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to submit task: {str(e)}")


async def _ensure_llm_initialized(llm_profile):
    """Ensure LLM is initialized with the specified profile and test connectivity"""
    from ..utils.llm_factory import create_llm_from_profile
    from ..shared_state import vibesurf_agent
    from browser_use.llm import UserMessage

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    try:
        # Always create new LLM instance to ensure we're using the right profile
        new_llm = create_llm_from_profile(llm_profile)

        # Test LLM connectivity with a simple question
        test_message = UserMessage(content='What is the capital of France? Answer in one word.')
        
        logger.info(f"Testing LLM connectivity for profile: {llm_profile['profile_name']}")
        response = await new_llm.ainvoke([test_message])
        
        # Check if response contains expected answer
        if not response or not hasattr(response, 'completion'):
            return False, f"LLM response validation failed: No completion content received"
        
        completion = response.completion.lower() if response.completion else ""
        if 'paris' not in completion:
            logger.warning(f"LLM connectivity test returned unexpected answer: {response.completion}")
            # Still continue if we got some response, just log the warning
        
        logger.info(f"LLM connectivity test successful for profile: {llm_profile['profile_name']}")

        # Update vibesurf agent's LLM and register with token cost service
        if vibesurf_agent and vibesurf_agent.token_cost_service:
            vibesurf_agent.llm = vibesurf_agent.token_cost_service.register_llm(new_llm)
            logger.info(f"LLM updated and registered for token tracking for profile: {llm_profile['profile_name']}")
        
        return True, "LLM initialized and tested successfully"
        
    except Exception as e:
        error_msg = f"LLM connectivity test failed: {str(e)}"
        logger.error(error_msg)
        return False, error_msg


@router.post("/pause")
async def pause_task(control_request: TaskControlRequest):
    """Pause current task execution"""
    from ..shared_state import vibesurf_agent

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    if not is_task_running():
        raise HTTPException(status_code=400, detail="No active task to pause")

    try:
        result = await vibesurf_agent.pause(control_request.reason)

        if result.success:
            # Update active task status
            current_task = get_active_task_info()
            if current_task:
                from ..shared_state import active_task
                active_task["status"] = "paused"
                active_task["pause_reason"] = control_request.reason

            return {
                "success": True,
                "message": result.message,
                "operation": "pause"
            }
        else:
            raise HTTPException(status_code=500, detail=result.message)

    except Exception as e:
        logger.error(f"Failed to pause task: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to pause task: {str(e)}")


@router.post("/resume")
async def resume_task(control_request: TaskControlRequest):
    """Resume current task execution"""
    from ..shared_state import vibesurf_agent

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    current_task = get_active_task_info()
    if not current_task or current_task.get("status") != "paused":
        raise HTTPException(status_code=400, detail="No paused task to resume")

    try:
        result = await vibesurf_agent.resume(control_request.reason)

        if result.success:
            # Update active task status
            from ..shared_state import active_task
            active_task["status"] = "running"
            active_task["resume_reason"] = control_request.reason

            return {
                "success": True,
                "message": result.message,
                "operation": "resume"
            }
        else:
            raise HTTPException(status_code=500, detail=result.message)

    except Exception as e:
        logger.error(f"Failed to resume task: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to resume task: {str(e)}")


@router.post("/stop")
async def stop_task(control_request: TaskControlRequest):
    """Stop current task execution"""
    from ..shared_state import vibesurf_agent

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    if not is_task_running():
        raise HTTPException(status_code=400, detail="No active task to stop")

    try:
        result = await vibesurf_agent.stop(control_request.reason)

        if result.success:
            # Update active task status and clear it
            current_task = get_active_task_info()
            if current_task:
                from ..shared_state import active_task
                active_task["status"] = "stopped"
                active_task["stop_reason"] = control_request.reason
                active_task["end_time"] = datetime.now()

            # Clear active task
            clear_active_task()

            return {
                "success": True,
                "message": result.message,
                "operation": "stop"
            }
        else:
            raise HTTPException(status_code=500, detail=result.message)

    except Exception as e:
        logger.error(f"Failed to stop task: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to stop task: {str(e)}")


@router.post("/add-new-task")
async def add_new_task(control_request: TaskControlRequest):
    """Add a new task or follow-up instruction during execution"""
    from ..shared_state import vibesurf_agent

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    if not is_task_running():
        raise HTTPException(status_code=400, detail="No active task to add new instruction to")

    try:
        # Use the reason field as the new task content
        new_task = control_request.reason or "No additional task provided"
        
        # Add the new task to the running agent
        await vibesurf_agent.add_new_task(new_task)

        return {
            "success": True,
            "message": "New task added successfully",
            "operation": "add_new_task",
            "new_task": new_task
        }

    except Exception as e:
        logger.error(f"Failed to add new task: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to add new task: {str(e)}")


@router.get("/detailed-status")
async def get_detailed_task_status():
    """Get detailed task execution status with vibesurf information"""
    from ..shared_state import vibesurf_agent

    if not vibesurf_agent:
        raise HTTPException(status_code=503, detail="VibeSurf agent not initialized")

    try:
        current_task = get_active_task_info()

        if current_task:
            # Get detailed vibesurf status
            vibesurf_status = vibesurf_agent.get_status()

            return {
                "has_active_task": True,
                "task_id": current_task["task_id"],
                "status": current_task["status"],
                "session_id": current_task["session_id"],
                "task": current_task["task"],
                "start_time": current_task["start_time"].isoformat() if current_task.get("start_time") else None,
                "end_time": current_task.get("end_time").isoformat() if current_task.get("end_time") else None,
                "result": current_task.get("result"),
                "error": current_task.get("error"),
                "pause_reason": current_task.get("pause_reason"),
                "stop_reason": current_task.get("stop_reason"),
                "vibesurf_status": {
                    "overall_status": vibesurf_status.overall_status,
                    "active_step": vibesurf_status.active_step,
                    "agent_statuses": {k: v.dict() for k, v in vibesurf_status.agent_statuses.items()},
                    "progress": vibesurf_status.progress,
                    "last_update": vibesurf_status.last_update.isoformat()
                }
            }
        else:
            return {
                "has_active_task": False,
                "message": "No active task"
            }

    except Exception as e:
        logger.error(f"Failed to get task status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get task status: {str(e)}")



================================================
FILE: vibe_surf/backend/api/voices.py
================================================
"""
Tools API endpoints for VibeSurf Backend

Handles voice recognition and other tool-related operations.
"""
import pdb

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from fastapi.responses import JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Dict, List, Optional, Any
from pydantic import BaseModel
import os
import logging
from datetime import datetime

from vibe_surf.tools.voice_asr import QwenASR, OpenAIASR, GeminiASR

from ..database.manager import get_db_session
from ..database.queries import VoiceProfileQueries
from ..voice_model_config import VOICE_MODELS


router = APIRouter(prefix="/voices", tags=["voices"])

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


# Pydantic models for request validation
class VoiceProfileCreate(BaseModel):
    voice_profile_name: str
    voice_model_type: str  # "asr" or "tts"
    voice_model_name: str
    api_key: Optional[str] = None
    voice_meta_params: Optional[Dict[str, Any]] = None
    description: Optional[str] = None

class VoiceProfileUpdate(BaseModel):
    voice_model_type: Optional[str] = None
    voice_model_name: Optional[str] = None
    api_key: Optional[str] = None
    voice_meta_params: Optional[Dict[str, Any]] = None
    description: Optional[str] = None
    is_active: Optional[bool] = None


@router.post("/voice-profiles")
async def create_voice_profile(
    profile_data: VoiceProfileCreate,
    db: AsyncSession = Depends(get_db_session)
):
    """Create a new voice profile"""
    try:
        # Validate voice_model_type
        if profile_data.voice_model_type not in ["asr", "tts"]:
            raise HTTPException(
                status_code=400,
                detail="voice_model_type must be 'asr' or 'tts'"
            )
        
        # Check if profile name already exists
        existing_profile = await VoiceProfileQueries.get_profile(db, profile_data.voice_profile_name)
        if existing_profile:
            raise HTTPException(
                status_code=400,
                detail=f"Voice profile '{profile_data.voice_profile_name}' already exists"
            )
        
        # Create the profile
        created_profile = await VoiceProfileQueries.create_profile(
            db=db,
            voice_profile_name=profile_data.voice_profile_name,
            voice_model_type=profile_data.voice_model_type,
            voice_model_name=profile_data.voice_model_name,
            api_key=profile_data.api_key,
            voice_meta_params=profile_data.voice_meta_params,
            description=profile_data.description
        )
        
        await db.commit()
        
        return {
            "success": True,
            "message": f"Voice profile '{profile_data.voice_profile_name}' created successfully",
            "profile": created_profile
        }
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to create voice profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create voice profile: {str(e)}"
        )


@router.put("/voice-profiles/{voice_profile_name}")
async def update_voice_profile(
    voice_profile_name: str,
    profile_data: VoiceProfileUpdate,
    db: AsyncSession = Depends(get_db_session)
):
    """Update an existing voice profile"""
    try:
        # Check if profile exists
        existing_profile = await VoiceProfileQueries.get_profile(db, voice_profile_name)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"Voice profile '{voice_profile_name}' not found"
            )
        
        # Validate voice_model_type if provided
        if profile_data.voice_model_type and profile_data.voice_model_type not in ["asr", "tts"]:
            raise HTTPException(
                status_code=400,
                detail="voice_model_type must be 'asr' or 'tts'"
            )
        
        # Prepare update data (exclude None values)
        update_data = {}
        for field, value in profile_data.dict(exclude_unset=True).items():
            if value is not None:
                update_data[field] = value
        
        if not update_data:
            raise HTTPException(
                status_code=400,
                detail="No valid fields provided for update"
            )
        
        # Update the profile
        success = await VoiceProfileQueries.update_profile(
            db=db,
            voice_profile_name=voice_profile_name,
            updates=update_data
        )
        
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to update voice profile"
            )
        
        await db.commit()
        
        # Get updated profile
        updated_profile = await VoiceProfileQueries.get_profile(db, voice_profile_name)
        
        return {
            "success": True,
            "message": f"Voice profile '{voice_profile_name}' updated successfully",
            "profile": {
                "profile_id": updated_profile.profile_id,
                "voice_profile_name": updated_profile.voice_profile_name,
                "voice_model_type": updated_profile.voice_model_type.value,
                "voice_model_name": updated_profile.voice_model_name,
                "voice_meta_params": updated_profile.voice_meta_params,
                "description": updated_profile.description,
                "is_active": updated_profile.is_active,
                "created_at": updated_profile.created_at,
                "updated_at": updated_profile.updated_at,
                "last_used_at": updated_profile.last_used_at
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to update voice profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update voice profile: {str(e)}"
        )


@router.delete("/voice-profiles/{voice_profile_name}")
async def delete_voice_profile(
    voice_profile_name: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Delete a voice profile"""
    try:
        # Check if profile exists
        existing_profile = await VoiceProfileQueries.get_profile(db, voice_profile_name)
        if not existing_profile:
            raise HTTPException(
                status_code=404,
                detail=f"Voice profile '{voice_profile_name}' not found"
            )
        
        # Delete the profile
        success = await VoiceProfileQueries.delete_profile(db, voice_profile_name)
        
        if not success:
            raise HTTPException(
                status_code=500,
                detail="Failed to delete voice profile"
            )
        
        await db.commit()
        
        return {
            "success": True,
            "message": f"Voice profile '{voice_profile_name}' deleted successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to delete voice profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete voice profile: {str(e)}"
        )


@router.post("/asr")
async def voice_recognition(
    audio_file: UploadFile = File(...),
    voice_profile_name: str = None,
    db: AsyncSession = Depends(get_db_session)
):
    """
    Voice recognition using specified voice profile
    
    Args:
        audio_file: Audio file to transcribe
        voice_profile_name: Name of the voice profile to use (required)
        db: Database session
    
    Returns:
        Dict with recognized text
    """
    from .. import shared_state
    try:
        # Validate required parameters
        if not voice_profile_name:
            raise HTTPException(
                status_code=400,
                detail="voice_profile_name parameter is required"
            )
        
        if not audio_file or not audio_file.filename:
            raise HTTPException(
                status_code=400,
                detail="audio_file is required and must have a filename"
            )
        
        # Log the incoming request for debugging
        logger.info(f"ASR request: voice_profile_name='{voice_profile_name}', audio_file='{audio_file.filename}', size={audio_file.size if hasattr(audio_file, 'size') else 'unknown'}")
        
        # Get voice profile with decrypted API key
        profile_data = await VoiceProfileQueries.get_profile_with_decrypted_key(db, voice_profile_name)
        if not profile_data:
            raise HTTPException(
                status_code=404,
                detail=f"Voice profile '{voice_profile_name}' not found"
            )
        
        # Check if profile is active
        if not profile_data.get("is_active"):
            raise HTTPException(
                status_code=400,
                detail=f"Voice profile '{voice_profile_name}' is inactive"
            )
        
        # Check if profile is for ASR
        if profile_data.get("voice_model_type") != "asr":
            raise HTTPException(
                status_code=400,
                detail=f"Voice profile '{voice_profile_name}' is not an ASR profile"
            )
        
        # Get model configuration
        voice_model_name = profile_data.get("voice_model_name")
        model_config = VOICE_MODELS.get(voice_model_name)
        if not model_config:
            raise HTTPException(
                status_code=400,
                detail=f"Voice model '{voice_model_name}' is not supported"
            )
        
        # Save uploaded file permanently in workspace_dir/audios/
        saved_file_path = None
        try:
            # Get workspace directory
            workspace_dir = shared_state.workspace_dir
            if not workspace_dir:
                raise HTTPException(
                    status_code=500,
                    detail="Workspace directory not configured"
                )
            
            # Create audios directory if it doesn't exist
            audios_dir = os.path.join(workspace_dir, "audios")
            os.makedirs(audios_dir, exist_ok=True)
            
            # Generate timestamp-based filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # microseconds to milliseconds
            file_extension = ".wav"  # Default to wav
            if audio_file.filename:
                original_ext = os.path.splitext(audio_file.filename)[1]
                if original_ext:
                    file_extension = original_ext
            
            saved_filename = f"asr-{timestamp}{file_extension}"
            saved_file_path = os.path.join(audios_dir, saved_filename)
            
            # Save the audio file
            content = await audio_file.read()
            with open(saved_file_path, "wb") as f:
                f.write(content)
            
            # Initialize ASR
            api_key = profile_data.get("api_key")
            voice_meta_params = profile_data.get("voice_meta_params", {})
            asr_model_name = voice_meta_params.get("asr_model_name", "")
            recognized_text = ""
            if voice_model_name == "qwen-asr":
                asr = QwenASR(model=asr_model_name, api_key=api_key)
                recognized_text = asr.asr(wav_url=saved_file_path)
            elif voice_model_name == "openai-asr":
                # Support custom base_url for OpenAI
                base_url = voice_meta_params.get("base_url")
                asr = OpenAIASR(model=asr_model_name, api_key=api_key, base_url=base_url)
                recognized_text = asr.asr(wav_url=saved_file_path)
            elif voice_model_name == "gemini-asr":
                asr = GeminiASR(model=asr_model_name, api_key=api_key)
                recognized_text = asr.asr(wav_url=saved_file_path)
            else:
                raise HTTPException(
                    status_code=400,
                    detail=f"Voice model '{voice_model_name}' is not supported"
                )
            logger.debug(f"Recognized text: {recognized_text}")
            # Update last used timestamp
            await VoiceProfileQueries.update_last_used(db, voice_profile_name)
            await db.commit()
            
            return {
                "success": True,
                "voice_profile_name": voice_profile_name,
                "voice_model_name": voice_model_name,
                "recognized_text": recognized_text,
                "filename": audio_file.filename,
                "saved_audio_path": saved_file_path
            }
            
        except Exception as e:
            # If there's an error, we might want to clean up the saved file
            if saved_file_path and os.path.exists(saved_file_path):
                try:
                    os.unlink(saved_file_path)
                except:
                    pass  # Ignore cleanup errors
            raise e
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to perform voice recognition: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Voice recognition failed: {str(e)}"
        )


@router.get("/voice-profiles")
async def list_voice_profiles(
    voice_model_type: Optional[str] = None,
    active_only: bool = True,
    limit: int = 50,
    offset: int = 0,
    db: AsyncSession = Depends(get_db_session)
):
    """List voice profiles"""
    try:
        profiles = await VoiceProfileQueries.list_profiles(
            db=db,
            voice_model_type=voice_model_type,
            active_only=active_only,
            limit=limit,
            offset=offset
        )
        
        profile_list = []
        for profile in profiles:
            profile_data = {
                "profile_id": profile.profile_id,
                "voice_profile_name": profile.voice_profile_name,
                "voice_model_type": profile.voice_model_type.value,
                "voice_model_name": profile.voice_model_name,
                "voice_meta_params": profile.voice_meta_params,
                "description": profile.description,
                "is_active": profile.is_active,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }
            profile_list.append(profile_data)
        
        return {
            "profiles": profile_list,
            "total": len(profile_list),
            "voice_model_type": voice_model_type,
            "active_only": active_only
        }
        
    except Exception as e:
        logger.error(f"Failed to list voice profiles: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to list voice profiles: {str(e)}"
        )


@router.get("/models")
async def get_available_voice_models(model_type: Optional[str] = None):
    """Get list of all available voice models"""
    models = []
    for model_name, config in VOICE_MODELS.items():
        # Filter by model_type if provided
        config_model_type = config.get("model_type", "asr")
        if model_type and config_model_type != model_type:
            continue
            
        model_info = {
            "model_name": model_name,
            "model_type": config_model_type,
            "requires_api_key": config.get("requires_api_key", True)
        }
        models.append(model_info)
    
    return {
        "models": models,
        "total_models": len(models)
    }


@router.get("/{voice_profile_name}")
async def get_voice_profile(
    voice_profile_name: str,
    db: AsyncSession = Depends(get_db_session)
):
    """Get specific voice profile by name (without API key)"""
    try:
        profile = await VoiceProfileQueries.get_profile(db, voice_profile_name)
        if not profile:
            raise HTTPException(
                status_code=404,
                detail=f"Voice profile '{voice_profile_name}' not found"
            )
        
        return {
            "profile_id": profile.profile_id,
            "voice_profile_name": profile.voice_profile_name,
            "voice_model_type": profile.voice_model_type.value,
            "voice_model_name": profile.voice_model_name,
            "voice_meta_params": profile.voice_meta_params,
            "description": profile.description,
            "is_active": profile.is_active,
            "created_at": profile.created_at,
            "updated_at": profile.updated_at,
            "last_used_at": profile.last_used_at
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get voice profile: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get voice profile: {str(e)}"
        )


================================================
FILE: vibe_surf/backend/database/__init__.py
================================================
"""
VibeSurf Backend Database Package - Simplified

Single table design for task tracking and execution.
"""

from .manager import get_db_session
from .models import (
    Base,
    Task,
    TaskStatus,
)
from .queries import TaskQueries



================================================
FILE: vibe_surf/backend/database/manager.py
================================================
"""
Database Manager for VibeSurf Session Management

Handles database connections, session management, and initialization
with optimized configuration for real-time operations.
"""

import asyncio
import os
import glob
import pdb
import re
from pathlib import Path
from typing import AsyncGenerator, List, Tuple, Optional
import logging
import aiosqlite
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool
from .models import Base

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class DBMigrationManager:
    """Simplified database migration manager."""
    
    def __init__(self, db_path: str = None):
        """Initialize migration manager
        
        Args:
            db_path: Database file path. Will be extracted from database_url if not provided.
        """
        if db_path is None:
            # Extract database path from shared_state
            from .. import shared_state
            database_url = os.getenv(
                'VIBESURF_DATABASE_URL',
                f'sqlite+aiosqlite:///{os.path.join(shared_state.workspace_dir, "vibe_surf.db")}'
            )
            self.db_path = database_url
        else:
            self.db_path = db_path
            
        # Extract path from sqlite URL
        if self.db_path.startswith('sqlite+aiosqlite:///'):
            self.db_path = self.db_path[20:]  # Remove 'sqlite+aiosqlite:///' prefix
        else:
            raise ValueError(f"Migration manager only supports SQLite databases, got: {self.db_path}")
            
        self.migrations_dir = Path(__file__).parent / "migrations"
    
    async def get_db_version(self) -> int:
        """Get current database version."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("PRAGMA user_version;")
                result = await cursor.fetchone()
                return result[0] if result else 0
        except Exception:
            return 0
    
    async def set_db_version(self, version: int) -> None:
        """Set database version."""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(f"PRAGMA user_version = {version};")
            await db.commit()
    
    def get_migration_files(self) -> List[Tuple[int, str]]:
        """Get migration files sorted by version. Returns (version, filepath) tuples."""
        migrations = []
        pattern = str(self.migrations_dir / "v*.sql")
        
        for filepath in glob.glob(pattern):
            filename = os.path.basename(filepath)
            match = re.match(r'v(\d+)_.*\.sql$', filename)
            if match:
                version = int(match.group(1))
                migrations.append((version, filepath))
            else:
                logger.warning(f"Migration file {filename} doesn't match pattern v000_description.sql")
        
        return sorted(migrations, key=lambda x: x[0])
    
    async def init_database(self) -> None:
        """Initialize database directory if needed."""
        db_dir = os.path.dirname(self.db_path)
        os.makedirs(db_dir, exist_ok=True)
        
        current_version = await self.get_db_version()
        
        if current_version == 0:
            logger.info("Database version is 0, will be initialized via migrations...")
        else:
            logger.info(f"Database already exists (version {current_version})")
    
    async def apply_migrations(self, target_version: Optional[int] = None) -> int:
        """Apply pending migrations. Returns final version."""
        await self.init_database()  # Ensure database exists
        
        current_version = await self.get_db_version()
        migrations = self.get_migration_files()
        
        if not migrations:
            logger.info("No migration files found")
            return current_version
        
        if target_version is None:
            target_version = max(m[0] for m in migrations)
        
        logger.info(f"Current version: {current_version}, Target: {target_version}")
        
        if current_version >= target_version:
            logger.info("Database is up to date")
            return current_version
        
        applied = 0
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("PRAGMA foreign_keys = ON;")
            
            for version, filepath in migrations:
                if version <= current_version or version > target_version:
                    continue
                
                try:
                    logger.info(f"Applying migration v{version:03d}: {os.path.basename(filepath)}")
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        sql_content = f.read()
                    
                    await db.executescript(sql_content)
                    await self.set_db_version(version)
                    applied += 1
                    
                    logger.info(f"Successfully applied migration v{version:03d}")
                    
                except Exception as e:
                    logger.error(f"Migration v{version:03d} failed: {e}")
                    raise RuntimeError(f"Migration failed at version {version}: {e}")
        
        final_version = await self.get_db_version()
        logger.info(f"Applied {applied} migrations. Final version: {final_version}")
        return final_version


class DatabaseManager:
    """Database connection and session management"""

    def __init__(self, database_url: str = None):
        """Initialize database manager
        
        Args:
            database_url: Database connection URL. Defaults to SQLite if not provided.
        """
        from .. import shared_state
        self.database_url = database_url or os.getenv(
            'VIBESURF_DATABASE_URL',
            f'sqlite+aiosqlite:///{os.path.join(shared_state.workspace_dir, "vibe_surf.db")}'
        )

        # Configure engine based on database type
        if self.database_url.startswith('sqlite'):
            # SQLite configuration for development
            self.engine = create_async_engine(
                self.database_url,
                poolclass=StaticPool,
                connect_args={
                    "check_same_thread": False,
                    "timeout": 30
                },
                echo=False  # Set to True for SQL debugging
            )
        else:
            # PostgreSQL/MySQL configuration for production
            self.engine = create_async_engine(
                self.database_url,
                pool_size=20,
                max_overflow=30,
                pool_pre_ping=True,
                pool_recycle=3600,
                echo=False
            )

        self.async_session_factory = sessionmaker(
            self.engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        # Initialize migration manager for SQLite databases
        if self.database_url.startswith('sqlite'):
            try:
                self.migration_manager = DBMigrationManager(db_path=self.database_url)
            except Exception as e:
                logger.warning(f"Failed to initialize migration manager: {e}")
                self.migration_manager = None
        else:
            self.migration_manager = None
            logger.info("Migration manager is only supported for SQLite databases")

    async def create_tables(self, use_migrations: bool = True):
        """Create all database tables
        
        Args:
            use_migrations: If True, use migration system. If False, use direct table creation.
        """
        if use_migrations and self.migration_manager:
            logger.info("ğŸ”„ Using migration system to initialize database...")
            try:
                await self.migration_manager.apply_migrations()
                logger.info("âœ… Database initialized via migrations")
                return
            except Exception as e:
                logger.warning(f"Migration failed, falling back to direct table creation: {e}")
        
        # Fallback to direct table creation
        logger.info("ğŸ”„ Using direct table creation...")
        async with self.engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        logger.info("âœ… Database initialized via direct table creation")

    async def drop_tables(self):
        """Drop all database tables"""
        async with self.engine.begin() as conn:
            await conn.run_sync(Base.metadata.drop_all)

    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """Get async database session"""
        async with self.async_session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()

    async def apply_migrations(self, target_version: Optional[int] = None) -> int:
        """Apply database migrations
        
        Args:
            target_version: Target migration version. If None, applies all available migrations.
            
        Returns:
            Final database version after applying migrations.
            
        Raises:
            RuntimeError: If migration manager is not available or migration fails.
        """
        if not self.migration_manager:
            raise RuntimeError("Migration manager is not available. Only SQLite databases support migrations.")
        
        return await self.migration_manager.apply_migrations(target_version)
    
    async def get_db_version(self) -> int:
        """Get current database version
        
        Returns:
            Current database version, or 0 if not available.
        """
        if not self.migration_manager:
            logger.warning("Migration manager not available, cannot get database version")
            return 0
        
        return await self.migration_manager.get_db_version()

    async def close(self):
        """Close database connections"""
        await self.engine.dispose()


# Dependency for FastAPI
async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    """FastAPI dependency for database sessions"""
    from .. import shared_state

    if not shared_state.db_manager:
        raise RuntimeError("Database manager not initialized. Call initialize_vibesurf_components() first.")

    async for session in shared_state.db_manager.get_session():
        yield session


# Database initialization script
async def init_database():
    """Initialize database with tables"""
    from .. import shared_state

    logger.info("ğŸ—„ï¸ Initializing VibeSurf database...")

    try:
        if not shared_state.db_manager:
            raise RuntimeError("Database manager not initialized. Call initialize_vibesurf_components() first.")

        await shared_state.db_manager.create_tables()
        logger.info("âœ… Database tables created successfully")
        logger.info("âœ… VibeSurf database ready for single-task execution")

    except Exception as e:
        logger.error(f"âŒ Database initialization failed: {e}")
        raise


if __name__ == "__main__":
    # For standalone execution, initialize a temporary db_manager
    import os
    from .. import shared_state

    workspace_dir = os.getenv("VIBESURF_WORKSPACE", os.path.join(os.path.dirname(__file__), "../vibesurf_workspace"))
    database_url = os.getenv(
        'VIBESURF_DATABASE_URL',
        f'sqlite+aiosqlite:///{os.path.join(workspace_dir, "vibe_surf.db")}'
    )
    shared_state.db_manager = DatabaseManager(database_url)
    asyncio.run(init_database())



================================================
FILE: vibe_surf/backend/database/models.py
================================================
"""
Database Models for VibeSurf Backend - With LLM Profile Management

SQLAlchemy models for task execution system with LLM profile management.
"""

from sqlalchemy import Column, String, Text, DateTime, Enum, JSON, Boolean, Index, BigInteger
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from datetime import datetime
import enum
from uuid import uuid4

Base = declarative_base()

# Enums for type safety
class TaskStatus(enum.Enum):
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    STOPPED = "stopped"

class VoiceModelType(enum.Enum):
    ASR = "asr"
    TTS = "tts"

class VoiceProfile(Base):
    """Voice Profile model for managing voice model configurations with encrypted API keys"""
    __tablename__ = 'voice_profiles'
    
    # Primary identifier
    profile_id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))
    voice_profile_name = Column(String(100), nullable=False, unique=True)  # User-defined unique name
    
    # Voice Model Configuration
    voice_model_type = Column(Enum(VoiceModelType, values_callable=lambda obj: [e.value for e in obj]), nullable=False)  # asr or tts
    voice_model_name = Column(String(100), nullable=False)
    encrypted_api_key = Column(Text, nullable=True)  # Encrypted API key using MAC address
    
    # Voice model parameters (stored as JSON to allow flexibility)
    voice_meta_params = Column(JSON, nullable=True)  # Model-specific parameters
    
    # Profile metadata
    description = Column(Text, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, nullable=False, default=func.now())
    updated_at = Column(DateTime, nullable=False, default=func.now(), onupdate=func.now())
    last_used_at = Column(DateTime, nullable=True)
    
    def __repr__(self):
        return f"<VoiceProfile(voice_profile_name={self.voice_profile_name}, voice_model_name={self.voice_model_name}, type={self.voice_model_type.value})>"

class LLMProfile(Base):
    """LLM Profile model for managing LLM configurations with encrypted API keys"""
    __tablename__ = 'llm_profiles'
    
    # Primary identifier
    profile_id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))
    profile_name = Column(String(100), nullable=False, unique=True)  # User-defined unique name
    
    # LLM Configuration
    provider = Column(String(50), nullable=False)  # openai, anthropic, google, azure_openai, etc.
    model = Column(String(100), nullable=False)
    base_url = Column(String(500), nullable=True)
    encrypted_api_key = Column(Text, nullable=True)  # Encrypted API key using MAC address
    
    # LLM Parameters (stored as JSON to allow null values)
    temperature = Column(JSON, nullable=True)  # Allow float or null
    max_tokens = Column(JSON, nullable=True)   # Allow int or null
    top_p = Column(JSON, nullable=True)
    frequency_penalty = Column(JSON, nullable=True)
    seed = Column(JSON, nullable=True)
    
    # Provider-specific configuration
    provider_config = Column(JSON, nullable=True)
    
    # Profile metadata
    description = Column(Text, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    is_default = Column(Boolean, default=False, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, nullable=False, default=func.now())
    updated_at = Column(DateTime, nullable=False, default=func.now(), onupdate=func.now())
    last_used_at = Column(DateTime, nullable=True)
    
    def __repr__(self):
        return f"<LLMProfile(profile_name={self.profile_name}, provider={self.provider}, model={self.model})>"

class Task(Base):
    """Task model with LLM profile reference and workspace directory"""
    __tablename__ = 'tasks'
    
    # Primary identifier
    task_id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))
    
    # Session tracking
    session_id = Column(String(36), nullable=False)
    
    # Task definition
    task_description = Column(Text, nullable=False)
    status = Column(Enum(TaskStatus, values_callable=lambda obj: [e.value for e in obj]), nullable=False, default=TaskStatus.PENDING)
    
    # LLM Profile reference (instead of storing LLM config directly)
    llm_profile_name = Column(String(100), nullable=False)  # Reference to LLMProfile.profile_name
    
    # File uploads and workspace
    upload_files_path = Column(String(500), nullable=True)  # Path to uploaded files
    workspace_dir = Column(String(500), nullable=True)     # Workspace directory for this task
    
    # Configuration (JSON strings without API keys)
    mcp_server_config = Column(Text, nullable=True)  # MCP server config as JSON string
    
    # Agent execution mode
    agent_mode = Column(String(50), nullable=False, default='thinking')  # Agent mode: 'thinking' or 'direct'
    
    # Results
    task_result = Column(Text, nullable=True)  # Final markdown result
    error_message = Column(Text, nullable=True)
    report_path = Column(String(500), nullable=True)  # Generated report file path
    
    # Timestamps
    created_at = Column(DateTime, nullable=False, default=func.now())
    updated_at = Column(DateTime, nullable=False, default=func.now(), onupdate=func.now())
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    
    # Additional metadata
    task_metadata = Column(JSON, nullable=True)  # Additional context
    
    def __repr__(self):
        return f"<Task(task_id={self.task_id}, status={self.status.value}, llm_profile={self.llm_profile_name})>"

class UploadedFile(Base):
    """Model for tracking uploaded files"""
    __tablename__ = "uploaded_files"
    
    file_id = Column(String(36), primary_key=True)  # UUID7 string
    original_filename = Column(String(255), nullable=False, index=True)
    stored_filename = Column(String(255), nullable=False)
    file_path = Column(Text, nullable=False)
    session_id = Column(String(255), nullable=True, index=True)
    file_size = Column(BigInteger, nullable=False)
    mime_type = Column(String(100), nullable=False)
    upload_time = Column(DateTime, default=func.now(), nullable=False, index=True)
    relative_path = Column(Text, nullable=False)  # Relative to workspace_dir
    is_deleted = Column(Boolean, default=False, nullable=False, index=True)
    deleted_at = Column(DateTime, nullable=True)
    
    def __repr__(self):
        return f"<UploadedFile(file_id={self.file_id}, filename={self.original_filename}, session={self.session_id})>"

# Create useful indexes for performance
Index('idx_llm_profiles_name', LLMProfile.profile_name)
Index('idx_llm_profiles_active', LLMProfile.is_active)
Index('idx_llm_profiles_default', LLMProfile.is_default)
Index('idx_llm_profiles_provider', LLMProfile.provider)

Index('idx_tasks_status', Task.status)
Index('idx_tasks_session', Task.session_id)
Index('idx_tasks_llm_profile', Task.llm_profile_name)
Index('idx_tasks_created', Task.created_at)

class McpProfile(Base):
    """MCP Profile model for managing MCP server configurations"""
    __tablename__ = 'mcp_profiles'
    
    # Primary identifier
    mcp_id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))
    display_name = Column(String(100), nullable=False, unique=True)  # User-friendly name
    mcp_server_name = Column(String(100), nullable=False, unique=True)  # Server identifier (e.g., "filesystem", "markitdown")
    
    # MCP Server Configuration
    mcp_server_params = Column(JSON, nullable=False)  # {"command": "npx", "args": [...]}
    
    # Profile metadata
    description = Column(Text, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, nullable=False, default=func.now())
    updated_at = Column(DateTime, nullable=False, default=func.now(), onupdate=func.now())
    last_used_at = Column(DateTime, nullable=True)
    
    def __repr__(self):
        return f"<McpProfile(display_name={self.display_name}, server_name={self.mcp_server_name}, active={self.is_active})>"

Index('idx_uploaded_files_session_time', UploadedFile.session_id, UploadedFile.upload_time)
Index('idx_uploaded_files_active', UploadedFile.is_deleted, UploadedFile.upload_time)
Index('idx_uploaded_files_filename', UploadedFile.original_filename)

# MCP Profile indexes
Index('idx_mcp_profiles_display_name', McpProfile.display_name)
Index('idx_mcp_profiles_server_name', McpProfile.mcp_server_name)
Index('idx_mcp_profiles_active', McpProfile.is_active)

# Voice Profile indexes
Index('idx_voice_profiles_name', VoiceProfile.voice_profile_name)
Index('idx_voice_profiles_type', VoiceProfile.voice_model_type)
Index('idx_voice_profiles_active', VoiceProfile.is_active)


================================================
FILE: vibe_surf/backend/database/queries.py
================================================
"""
Database Query Operations for VibeSurf Backend - With LLM Profile Management

Centralized database operations for Task and LLMProfile tables.
"""

from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update, delete, func, desc, and_, or_
from sqlalchemy.orm import selectinload
from .models import Task, TaskStatus, LLMProfile, UploadedFile, McpProfile, VoiceProfile, VoiceModelType
from ..utils.encryption import encrypt_api_key, decrypt_api_key
import logging
import json

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class LLMProfileQueries:
    """Query operations for LLMProfile model"""

    @staticmethod
    async def create_profile(
            db: AsyncSession,
            profile_name: str,
            provider: str,
            model: str,
            api_key: Optional[str] = None,
            base_url: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            top_p: Optional[float] = None,
            frequency_penalty: Optional[float] = None,
            seed: Optional[int] = None,
            provider_config: Optional[Dict[str, Any]] = None,
            description: Optional[str] = None,
            is_default: bool = False
    ) -> Dict[str, Any]:
        """Create a new LLM profile with encrypted API key"""
        try:
            # Encrypt API key if provided
            encrypted_api_key = encrypt_api_key(api_key) if api_key else None

            profile = LLMProfile(
                profile_name=profile_name,
                provider=provider,
                model=model,
                base_url=base_url,
                encrypted_api_key=encrypted_api_key,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                seed=seed,
                provider_config=provider_config or {},
                description=description,
                is_default=is_default
            )

            db.add(profile)
            await db.flush()
            await db.refresh(profile)

            # Extract data immediately to avoid greenlet issues
            profile_data = {
                "profile_id": profile.profile_id,
                "profile_name": profile.profile_name,
                "provider": profile.provider,
                "model": profile.model,
                "base_url": profile.base_url,
                "temperature": profile.temperature,
                "max_tokens": profile.max_tokens,
                "top_p": profile.top_p,
                "frequency_penalty": profile.frequency_penalty,
                "seed": profile.seed,
                "provider_config": profile.provider_config,
                "description": profile.description,
                "is_active": profile.is_active,
                "is_default": profile.is_default,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }

            return profile_data
        except Exception as e:
            logger.error(f"Failed to create LLM profile {profile_name}: {e}")
            raise

    @staticmethod
    async def get_profile(db: AsyncSession, profile_name: str) -> Optional[LLMProfile]:
        """Get LLM profile by name"""
        try:
            result = await db.execute(
                select(LLMProfile).where(LLMProfile.profile_name == profile_name)
            )
            profile = result.scalar_one_or_none()
            if profile:
                # Ensure all attributes are loaded by accessing them
                _ = (profile.profile_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active, profile.is_default)
            return profile
        except Exception as e:
            logger.error(f"Failed to get LLM profile {profile_name}: {e}")
            raise

    @staticmethod
    async def get_profile_with_decrypted_key(db: AsyncSession, profile_name: str) -> Optional[Dict[str, Any]]:
        """Get LLM profile with decrypted API key"""
        try:
            profile = await LLMProfileQueries.get_profile(db, profile_name)
            if not profile:
                return None

            # Decrypt API key
            decrypted_api_key = decrypt_api_key(profile.encrypted_api_key) if profile.encrypted_api_key else None

            return {
                "profile_id": profile.profile_id,
                "profile_name": profile.profile_name,
                "provider": profile.provider,
                "model": profile.model,
                "base_url": profile.base_url,
                "api_key": decrypted_api_key,  # Decrypted for use
                "temperature": profile.temperature,
                "max_tokens": profile.max_tokens,
                "top_p": profile.top_p,
                "frequency_penalty": profile.frequency_penalty,
                "seed": profile.seed,
                "provider_config": profile.provider_config,
                "description": profile.description,
                "is_active": profile.is_active,
                "is_default": profile.is_default,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }
        except Exception as e:
            logger.error(f"Failed to get LLM profile with decrypted key {profile_name}: {e}")
            raise

    @staticmethod
    async def list_profiles(
            db: AsyncSession,
            active_only: bool = True,
            limit: int = 50,
            offset: int = 0
    ) -> List[LLMProfile]:
        """List LLM profiles"""
        try:
            query = select(LLMProfile)

            if active_only:
                query = query.where(LLMProfile.is_active == True)

            query = query.order_by(desc(LLMProfile.last_used_at), desc(LLMProfile.created_at))
            query = query.limit(limit).offset(offset)

            result = await db.execute(query)
            profiles = result.scalars().all()

            # Ensure all attributes are loaded for each profile
            for profile in profiles:
                _ = (profile.profile_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active, profile.is_default)

            return profiles
        except Exception as e:
            logger.error(f"Failed to list LLM profiles: {e}")
            raise

    @staticmethod
    async def update_profile(
            db: AsyncSession,
            profile_name: str,
            updates: Dict[str, Any]
    ) -> bool:
        """Update LLM profile"""
        try:
            # Handle API key encryption if present
            if "api_key" in updates:
                api_key = updates.pop("api_key")
                if api_key:
                    updates["encrypted_api_key"] = encrypt_api_key(api_key)
                else:
                    updates["encrypted_api_key"] = None

            result = await db.execute(
                update(LLMProfile)
                .where(LLMProfile.profile_name == profile_name)
                .values(**updates)
            )

            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update LLM profile {profile_name}: {e}")
            raise

    @staticmethod
    async def delete_profile(db: AsyncSession, profile_name: str) -> bool:
        """Delete LLM profile"""
        try:
            result = await db.execute(
                delete(LLMProfile).where(LLMProfile.profile_name == profile_name)
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to delete LLM profile {profile_name}: {e}")
            raise

    @staticmethod
    async def get_default_profile(db: AsyncSession) -> Optional[LLMProfile]:
        """Get the default LLM profile"""
        try:
            result = await db.execute(
                select(LLMProfile).where(LLMProfile.is_default == True)
            )
            profile = result.scalar_one_or_none()
            if profile:
                # Ensure all attributes are loaded by accessing them
                _ = (profile.profile_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active, profile.is_default)
            return profile
        except Exception as e:
            logger.error(f"Failed to get default LLM profile: {e}")
            raise

    @staticmethod
    async def set_default_profile(db: AsyncSession, profile_name: str) -> bool:
        """Set a profile as default (and unset others)"""
        try:
            # First, unset all defaults
            await db.execute(
                update(LLMProfile).values(is_default=False)
            )

            # Then set the specified profile as default
            result = await db.execute(
                update(LLMProfile)
                .where(LLMProfile.profile_name == profile_name)
                .values(is_default=True)
            )

            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to set default LLM profile {profile_name}: {e}")
            raise

    @staticmethod
    async def update_last_used(db: AsyncSession, profile_name: str) -> bool:
        """Update the last_used_at timestamp for a profile"""
        try:
            result = await db.execute(
                update(LLMProfile)
                .where(LLMProfile.profile_name == profile_name)
                .values(last_used_at=func.now())
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update last_used for LLM profile {profile_name}: {e}")
            raise


class McpProfileQueries:
    """Query operations for McpProfile model"""

    @staticmethod
    async def create_profile(
            db: AsyncSession,
            display_name: str,
            mcp_server_name: str,
            mcp_server_params: Dict[str, Any],
            description: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create a new MCP profile"""
        try:
            profile = McpProfile(
                display_name=display_name,
                mcp_server_name=mcp_server_name,
                mcp_server_params=mcp_server_params,
                description=description
            )

            db.add(profile)
            await db.flush()
            await db.refresh(profile)

            # Extract data immediately to avoid greenlet issues
            profile_data = {
                "mcp_id": profile.mcp_id,
                "display_name": profile.display_name,
                "mcp_server_name": profile.mcp_server_name,
                "mcp_server_params": profile.mcp_server_params,
                "description": profile.description,
                "is_active": profile.is_active,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }

            return profile_data
        except Exception as e:
            logger.error(f"Failed to create MCP profile {display_name}: {e}")
            raise

    @staticmethod
    async def get_profile(db: AsyncSession, mcp_id: str) -> Optional[McpProfile]:
        """Get MCP profile by ID"""
        try:
            result = await db.execute(
                select(McpProfile).where(McpProfile.mcp_id == mcp_id)
            )
            profile = result.scalar_one_or_none()
            if profile:
                # Ensure all attributes are loaded by accessing them
                _ = (profile.mcp_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)
            return profile
        except Exception as e:
            logger.error(f"Failed to get MCP profile {mcp_id}: {e}")
            raise

    @staticmethod
    async def get_profile_by_display_name(db: AsyncSession, display_name: str) -> Optional[McpProfile]:
        """Get MCP profile by display name"""
        try:
            result = await db.execute(
                select(McpProfile).where(McpProfile.display_name == display_name)
            )
            profile = result.scalar_one_or_none()
            if profile:
                _ = (profile.mcp_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)
            return profile
        except Exception as e:
            logger.error(f"Failed to get MCP profile by display name {display_name}: {e}")
            raise

    @staticmethod
    async def list_profiles(
            db: AsyncSession,
            active_only: bool = True,
            limit: int = 50,
            offset: int = 0
    ) -> List[McpProfile]:
        """List MCP profiles"""
        try:
            query = select(McpProfile)

            if active_only:
                query = query.where(McpProfile.is_active == True)

            query = query.order_by(desc(McpProfile.last_used_at), desc(McpProfile.created_at))
            query = query.limit(limit).offset(offset)

            result = await db.execute(query)
            profiles = result.scalars().all()

            # Ensure all attributes are loaded for each profile
            for profile in profiles:
                _ = (profile.mcp_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)

            return profiles
        except Exception as e:
            logger.error(f"Failed to list MCP profiles: {e}")
            raise

    @staticmethod
    async def get_active_profiles(db: AsyncSession) -> List[McpProfile]:
        """Get all active MCP profiles"""
        try:
            result = await db.execute(
                select(McpProfile).where(McpProfile.is_active == True)
            )
            profiles = result.scalars().all()

            # Ensure all attributes are loaded for each profile
            for profile in profiles:
                _ = (profile.mcp_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)

            return profiles
        except Exception as e:
            logger.error(f"Failed to get active MCP profiles: {e}")
            raise

    @staticmethod
    async def update_profile(
            db: AsyncSession,
            mcp_id: str,
            updates: Dict[str, Any]
    ) -> bool:
        """Update MCP profile"""
        try:
            logger.info(f"Updating profile {mcp_id}")

            result = await db.execute(
                update(McpProfile)
                .where(McpProfile.mcp_id == mcp_id)
                .values(**updates)
            )

            rows_affected = result.rowcount
            logger.info(f"Update query affected {rows_affected} rows")

            return rows_affected > 0
        except Exception as e:
            logger.error(f"Failed to update MCP profile {mcp_id}: {e}")
            raise

    @staticmethod
    async def delete_profile(db: AsyncSession, mcp_id: str) -> bool:
        """Delete MCP profile"""
        try:
            result = await db.execute(
                delete(McpProfile).where(McpProfile.mcp_id == mcp_id)
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to delete MCP profile {mcp_id}: {e}")
            raise

    @staticmethod
    async def update_last_used(db: AsyncSession, mcp_id: str) -> bool:
        """Update the last_used_at timestamp for a profile"""
        try:
            result = await db.execute(
                update(McpProfile)
                .where(McpProfile.mcp_id == mcp_id)
                .values(last_used_at=func.now())
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update last_used for MCP profile {mcp_id}: {e}")
            raise


class TaskQueries:
    """Database queries for task management with LLM Profile support"""

    @staticmethod
    async def save_task(
            db: AsyncSession,
            task_id: str,
            session_id: str,
            task_description: str,
            llm_profile_name: str,
            upload_files_path: Optional[str] = None,
            workspace_dir: Optional[str] = None,
            mcp_server_config: Optional[str] = None,  # JSON string
            task_result: Optional[str] = None,
            task_status: str = "pending",
            error_message: Optional[str] = None,
            report_path: Optional[str] = None,
            agent_mode: str = "thinking"
    ) -> Task:
        """Create or update a task record"""
        try:
            # Check if task exists
            result = await db.execute(select(Task).where(Task.task_id == task_id))
            existing_task = result.scalar_one_or_none()

            if existing_task:
                # Update existing task
                update_data = {}
                if task_result is not None:
                    update_data['task_result'] = task_result
                if task_status:
                    update_data['status'] = TaskStatus(task_status)
                if error_message is not None:
                    update_data['error_message'] = error_message
                if report_path is not None:
                    update_data['report_path'] = report_path
                if task_status == "running" and not existing_task.started_at:
                    update_data['started_at'] = func.now()
                if task_status in ["completed", "failed", "stopped"]:
                    update_data['completed_at'] = func.now()

                await db.execute(
                    update(Task).where(Task.task_id == task_id).values(**update_data)
                )
                await db.refresh(existing_task)
                return existing_task
            else:
                # DEBUG: Log the type and content of mcp_server_config before saving
                logger.debug(
                    f"Creating task with mcp_server_config type: {type(mcp_server_config)}, value: {mcp_server_config}")

                # Serialize mcp_server_config to JSON string if it's a dict
                if isinstance(mcp_server_config, dict):
                    mcp_server_config_json = json.dumps(mcp_server_config)
                    logger.debug(f"Converted dict to JSON string: {mcp_server_config_json}")
                else:
                    mcp_server_config_json = mcp_server_config

                # Create new task
                task = Task(
                    task_id=task_id,
                    session_id=session_id,
                    task_description=task_description,
                    status=TaskStatus(task_status),
                    llm_profile_name=llm_profile_name,
                    upload_files_path=upload_files_path,
                    workspace_dir=workspace_dir,
                    mcp_server_config=mcp_server_config_json,
                    task_result=task_result,
                    error_message=error_message,
                    report_path=report_path,
                    agent_mode=agent_mode
                )

                db.add(task)
                await db.flush()
                await db.refresh(task)
                return task

        except Exception as e:
            logger.error(f"Failed to save task {task_id}: {e}")
            raise

    @staticmethod
    async def get_task(db: AsyncSession, task_id: str) -> Optional[Task]:
        """Get task by ID"""
        try:
            result = await db.execute(select(Task).where(Task.task_id == task_id))
            return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"Failed to get task {task_id}: {e}")
            raise

    @staticmethod
    async def get_tasks_by_session(
            db: AsyncSession,
            session_id: str,
            limit: int = 50,
            offset: int = 0
    ) -> List[Task]:
        """Get all tasks for a session"""
        try:
            result = await db.execute(
                select(Task)
                .where(Task.session_id == session_id)
                .order_by(desc(Task.created_at))
                .limit(limit)
                .offset(offset)
            )
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get tasks for session {session_id}: {e}")
            raise

    @staticmethod
    async def get_recent_tasks(db: AsyncSession, limit: int = -1) -> List[Task]:
        """Get recent tasks"""
        try:
            query = select(Task).order_by(desc(Task.created_at))

            # Handle -1 as "get all records"
            if limit != -1:
                query = query.limit(limit)

            result = await db.execute(query)
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get recent tasks: {e}")
            raise

    @staticmethod
    async def get_all_sessions(
            db: AsyncSession,
            limit: int = -1,
            offset: int = 0
    ) -> List[Dict[str, Any]]:
        """Get all unique sessions with task counts and metadata"""
        try:
            # Get distinct session_ids with aggregated data
            query = select(
                Task.session_id,
                func.count(Task.task_id).label('task_count'),
                func.min(Task.created_at).label('created_at'),
                func.max(Task.created_at).label('last_activity'),
                func.max(Task.status).label('latest_status')
            ).group_by(Task.session_id).order_by(desc(func.max(Task.created_at)))

            # Handle -1 as "get all records"
            if limit != -1:
                query = query.limit(limit)

            # Always apply offset if provided
            if offset > 0:
                query = query.offset(offset)

            result = await db.execute(query)

            sessions = []
            for row in result.all():
                sessions.append({
                    'session_id': row.session_id,
                    'task_count': row.task_count,
                    'created_at': row.created_at.isoformat() if row.created_at else None,
                    'last_activity': row.last_activity.isoformat() if row.last_activity else None,
                    'status': row.latest_status.value if row.latest_status else 'unknown'
                })

            return sessions
        except Exception as e:
            logger.error(f"Failed to get all sessions: {e}")
            raise

    @staticmethod
    async def update_task_status(
            db: AsyncSession,
            task_id: str,
            status: str,
            error_message: Optional[str] = None,
            task_result: Optional[str] = None,
            report_path: Optional[str] = None
    ) -> bool:
        """Update task status"""
        try:
            update_data = {
                'status': TaskStatus(status)
            }

            if status == "running":
                update_data['started_at'] = func.now()
            elif status in ["completed", "failed", "stopped"]:
                update_data['completed_at'] = func.now()

            if error_message:
                update_data['error_message'] = error_message
            if task_result:
                update_data['task_result'] = task_result
            if report_path:
                update_data['report_path'] = report_path

            result = await db.execute(
                update(Task).where(Task.task_id == task_id).values(**update_data)
            )

            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update task status {task_id}: {e}")
            raise

    @staticmethod
    async def delete_task(db: AsyncSession, task_id: str) -> bool:
        """Delete a task"""
        try:
            result = await db.execute(delete(Task).where(Task.task_id == task_id))
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to delete task {task_id}: {e}")
            raise

    @staticmethod
    async def get_running_tasks(db: AsyncSession) -> List[Task]:
        """Get all currently running tasks"""
        try:
            result = await db.execute(
                select(Task).where(Task.status.in_([TaskStatus.RUNNING, TaskStatus.PAUSED]))
            )
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get running tasks: {e}")
            raise

    @staticmethod
    async def get_active_task(db: AsyncSession) -> Optional[Task]:
        """Get currently running task (for single-task model)"""
        try:
            result = await db.execute(
                select(Task).where(Task.status == TaskStatus.RUNNING)
            )
            return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"Failed to get active task: {e}")
            raise

    @staticmethod
    async def get_tasks_by_llm_profile(
            db: AsyncSession,
            llm_profile_name: str,
            limit: int = 50,
            offset: int = 0
    ) -> List[Task]:
        """Get tasks that used a specific LLM profile"""
        try:
            result = await db.execute(
                select(Task)
                .where(Task.llm_profile_name == llm_profile_name)
                .order_by(desc(Task.created_at))
                .limit(limit)
                .offset(offset)
            )
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get tasks for LLM profile {llm_profile_name}: {e}")
            raise

    @staticmethod
    async def update_task_completion(
            db: AsyncSession,
            task_id: str,
            task_result: Optional[str] = None,
            task_status: str = "completed",
            error_message: Optional[str] = None,
            report_path: Optional[str] = None
    ) -> bool:
        """Update task completion status and results"""
        try:
            update_data = {
                'status': TaskStatus(task_status),
                'completed_at': func.now()
            }

            if task_result is not None:
                update_data['task_result'] = task_result
            if error_message is not None:
                update_data['error_message'] = error_message
            if report_path is not None:
                update_data['report_path'] = report_path

            result = await db.execute(
                update(Task).where(Task.task_id == task_id).values(**update_data)
            )

            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update task completion {task_id}: {e}")
            raise

    @staticmethod
    async def get_task_counts_by_status(db: AsyncSession) -> Dict[str, int]:
        """Get count of tasks by status"""
        try:
            result = await db.execute(
                select(Task.status, func.count(Task.task_id))
                .group_by(Task.status)
            )

            counts = {}
            for status, count in result.all():
                counts[status.value] = count

            return counts
        except Exception as e:
            logger.error(f"Failed to get task counts by status: {e}")
            raise


class UploadedFileQueries:
    """Query operations for UploadedFile model"""

    @staticmethod
    async def create_file_record(
            db: AsyncSession,
            file_id: str,
            original_filename: str,
            stored_filename: str,
            file_path: str,
            session_id: Optional[str],
            file_size: int,
            mime_type: str,
            relative_path: str
    ) -> UploadedFile:
        """Create a new uploaded file record"""
        try:
            uploaded_file = UploadedFile(
                file_id=file_id,
                original_filename=original_filename,
                stored_filename=stored_filename,
                file_path=file_path,
                session_id=session_id,
                file_size=file_size,
                mime_type=mime_type,
                relative_path=relative_path
            )

            db.add(uploaded_file)
            await db.flush()
            await db.refresh(uploaded_file)
            return uploaded_file
        except Exception as e:
            logger.error(f"Failed to create file record {file_id}: {e}")
            raise

    @staticmethod
    async def get_file(db: AsyncSession, file_id: str) -> Optional[UploadedFile]:
        """Get uploaded file by ID"""
        try:
            result = await db.execute(
                select(UploadedFile).where(
                    and_(UploadedFile.file_id == file_id, UploadedFile.is_deleted == False)
                )
            )
            return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"Failed to get file {file_id}: {e}")
            raise

    @staticmethod
    async def list_files(
            db: AsyncSession,
            session_id: Optional[str] = None,
            limit: int = -1,
            offset: int = 0,
            active_only: bool = True
    ) -> List[UploadedFile]:
        """List uploaded files with optional filtering"""
        try:
            query = select(UploadedFile)

            if active_only:
                query = query.where(UploadedFile.is_deleted == False)

            if session_id is not None:
                query = query.where(UploadedFile.session_id == session_id)

            query = query.order_by(desc(UploadedFile.upload_time))

            # Handle -1 as "get all records"
            if limit != -1:
                query = query.limit(limit)

            # Always apply offset if provided
            if offset > 0:
                query = query.offset(offset)

            result = await db.execute(query)
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to list files: {e}")
            raise

    @staticmethod
    async def count_files(
            db: AsyncSession,
            session_id: Optional[str] = None,
            active_only: bool = True
    ) -> int:
        """Count uploaded files with optional filtering"""
        try:
            query = select(func.count(UploadedFile.file_id))

            if active_only:
                query = query.where(UploadedFile.is_deleted == False)

            if session_id is not None:
                query = query.where(UploadedFile.session_id == session_id)

            result = await db.execute(query)
            return result.scalar() or 0
        except Exception as e:
            logger.error(f"Failed to count files: {e}")
            raise

    @staticmethod
    async def delete_file(db: AsyncSession, file_id: str) -> bool:
        """Soft delete uploaded file by marking as deleted"""
        try:
            result = await db.execute(
                update(UploadedFile)
                .where(UploadedFile.file_id == file_id)
                .values(is_deleted=True, deleted_at=func.now())
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to delete file {file_id}: {e}")
            raise

    @staticmethod
    async def hard_delete_file(db: AsyncSession, file_id: str) -> bool:
        """Permanently delete uploaded file record"""
        try:
            result = await db.execute(
                delete(UploadedFile).where(UploadedFile.file_id == file_id)
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to hard delete file {file_id}: {e}")
            raise

    @staticmethod
    async def get_files_by_session(
            db: AsyncSession,
            session_id: str,
            limit: int = -1,
            offset: int = 0
    ) -> List[UploadedFile]:
        """Get all uploaded files for a specific session"""
        try:
            query = select(UploadedFile).where(and_(
                UploadedFile.session_id == session_id,
                UploadedFile.is_deleted == False
            )).order_by(desc(UploadedFile.upload_time))

            # Handle -1 as "get all records"
            if limit != -1:
                query = query.limit(limit)

            # Always apply offset if provided
            if offset > 0:
                query = query.offset(offset)

            result = await db.execute(query)
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get files for session {session_id}: {e}")
            raise

    @staticmethod
    async def cleanup_deleted_files(db: AsyncSession, days_old: int = 30) -> int:
        """Clean up files marked as deleted for more than specified days"""
        try:
            cutoff_date = func.now() - func.make_interval(days=days_old)

            result = await db.execute(
                delete(UploadedFile)
                .where(and_(
                    UploadedFile.is_deleted == True,
                    UploadedFile.deleted_at < cutoff_date
                ))
            )
            return result.rowcount
        except Exception as e:
            logger.error(f"Failed to cleanup deleted files: {e}")
            raise


class VoiceProfileQueries:
    """Query operations for VoiceProfile model"""

    @staticmethod
    async def create_profile(
            db: AsyncSession,
            voice_profile_name: str,
            voice_model_type: str,
            voice_model_name: str,
            api_key: Optional[str] = None,
            voice_meta_params: Optional[Dict[str, Any]] = None,
            description: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create a new Voice profile with encrypted API key"""
        try:
            # Encrypt API key if provided
            encrypted_api_key = encrypt_api_key(api_key) if api_key else None

            profile = VoiceProfile(
                voice_profile_name=voice_profile_name,
                voice_model_type=VoiceModelType(voice_model_type),
                voice_model_name=voice_model_name,
                encrypted_api_key=encrypted_api_key,
                voice_meta_params=voice_meta_params or {},
                description=description
            )

            db.add(profile)
            await db.flush()
            await db.refresh(profile)

            # Extract data immediately to avoid greenlet issues
            profile_data = {
                "profile_id": profile.profile_id,
                "voice_profile_name": profile.voice_profile_name,
                "voice_model_type": profile.voice_model_type.value,
                "voice_model_name": profile.voice_model_name,
                "voice_meta_params": profile.voice_meta_params,
                "description": profile.description,
                "is_active": profile.is_active,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }

            return profile_data
        except Exception as e:
            logger.error(f"Failed to create Voice profile {voice_profile_name}: {e}")
            raise

    @staticmethod
    async def get_profile(db: AsyncSession, voice_profile_name: str) -> Optional[VoiceProfile]:
        """Get Voice profile by name"""
        try:
            result = await db.execute(
                select(VoiceProfile).where(VoiceProfile.voice_profile_name == voice_profile_name)
            )
            profile = result.scalar_one_or_none()
            if profile:
                # Ensure all attributes are loaded by accessing them
                _ = (profile.profile_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)
            return profile
        except Exception as e:
            logger.error(f"Failed to get Voice profile {voice_profile_name}: {e}")
            raise

    @staticmethod
    async def get_profile_with_decrypted_key(db: AsyncSession, voice_profile_name: str) -> Optional[Dict[str, Any]]:
        """Get Voice profile with decrypted API key"""
        try:
            profile = await VoiceProfileQueries.get_profile(db, voice_profile_name)
            if not profile:
                return None

            # Decrypt API key
            decrypted_api_key = decrypt_api_key(profile.encrypted_api_key) if profile.encrypted_api_key else None

            return {
                "profile_id": profile.profile_id,
                "voice_profile_name": profile.voice_profile_name,
                "voice_model_type": profile.voice_model_type.value,
                "voice_model_name": profile.voice_model_name,
                "api_key": decrypted_api_key,  # Decrypted for use
                "voice_meta_params": profile.voice_meta_params,
                "description": profile.description,
                "is_active": profile.is_active,
                "created_at": profile.created_at,
                "updated_at": profile.updated_at,
                "last_used_at": profile.last_used_at
            }
        except Exception as e:
            logger.error(f"Failed to get Voice profile with decrypted key {voice_profile_name}: {e}")
            raise

    @staticmethod
    async def list_profiles(
            db: AsyncSession,
            voice_model_type: Optional[str] = None,
            active_only: bool = True,
            limit: int = 50,
            offset: int = 0
    ) -> List[VoiceProfile]:
        """List Voice profiles"""
        try:
            query = select(VoiceProfile)

            if active_only:
                query = query.where(VoiceProfile.is_active == True)
            
            if voice_model_type:
                query = query.where(VoiceProfile.voice_model_type == VoiceModelType(voice_model_type))

            query = query.order_by(desc(VoiceProfile.last_used_at), desc(VoiceProfile.created_at))
            query = query.limit(limit).offset(offset)

            result = await db.execute(query)
            profiles = result.scalars().all()

            # Ensure all attributes are loaded for each profile
            for profile in profiles:
                _ = (profile.profile_id, profile.created_at, profile.updated_at,
                     profile.last_used_at, profile.is_active)

            return profiles
        except Exception as e:
            logger.error(f"Failed to list Voice profiles: {e}")
            raise

    @staticmethod
    async def update_profile(
            db: AsyncSession,
            voice_profile_name: str,
            updates: Dict[str, Any]
    ) -> bool:
        """Update Voice profile"""
        try:
            # Handle API key encryption if present
            if "api_key" in updates:
                api_key = updates.pop("api_key")
                if api_key:
                    updates["encrypted_api_key"] = encrypt_api_key(api_key)
                else:
                    updates["encrypted_api_key"] = None

            # Handle voice_model_type enum conversion
            if "voice_model_type" in updates:
                updates["voice_model_type"] = VoiceModelType(updates["voice_model_type"])

            result = await db.execute(
                update(VoiceProfile)
                .where(VoiceProfile.voice_profile_name == voice_profile_name)
                .values(**updates)
            )

            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update Voice profile {voice_profile_name}: {e}")
            raise

    @staticmethod
    async def delete_profile(db: AsyncSession, voice_profile_name: str) -> bool:
        """Delete Voice profile"""
        try:
            result = await db.execute(
                delete(VoiceProfile).where(VoiceProfile.voice_profile_name == voice_profile_name)
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to delete Voice profile {voice_profile_name}: {e}")
            raise


    @staticmethod
    async def update_last_used(db: AsyncSession, voice_profile_name: str) -> bool:
        """Update the last_used_at timestamp for a profile"""
        try:
            result = await db.execute(
                update(VoiceProfile)
                .where(VoiceProfile.voice_profile_name == voice_profile_name)
                .values(last_used_at=func.now())
            )
            return result.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to update last_used for Voice profile {voice_profile_name}: {e}")
            raise



================================================
FILE: vibe_surf/backend/database/schemas.py
================================================
"""
JSON Schema Definitions for VibeSurf Database - Simplified Single Task Model

Pydantic models for validating JSON fields in the simplified Task table.
"""

from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from datetime import datetime

class TaskMetadata(BaseModel):
    """Schema for Task.task_metadata JSON field"""
    
    # Execution summary
    execution_duration_seconds: Optional[float] = None
    total_actions: Optional[int] = None
    
    # Results summary
    generated_report_path: Optional[str] = None
    final_summary: Optional[str] = None
    
    # Control state history
    control_history: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Error context
    last_error: Optional[str] = None
    error_recovery_attempts: int = 0
    
    # User context
    created_via: str = "api"

class LLMConfiguration(BaseModel):
    """Schema for Task.llm_config JSON field"""
    
    model: str
    base_url: Optional[str] = None
    # Note: API key is intentionally excluded from stored config
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    seed: Optional[int] = None
    
    # Provider-specific settings
    provider: str = "openai"  # 'openai', 'azure', 'anthropic', etc.
    provider_config: Dict[str, Any] = Field(default_factory=dict)

class McpServerParams(BaseModel):
    """Schema for MCP server parameters configuration"""
    command: str  # e.g., "npx", "docker"
    args: List[str]  # command arguments
    env: Optional[Dict[str, str]] = None  # environment variables
    cwd: Optional[str] = None  # working directory
    timeout: Optional[int] = None  # timeout in seconds

class McpServerConfig(BaseModel):
    """Schema for MCP server configuration in Task.mcp_server_config"""
    mcpServers: Dict[str, McpServerParams] = Field(default_factory=dict)

class ControllerConfiguration(BaseModel):
    """Schema for Task.mcp_server_config JSON field (legacy tools config)"""
    
    # Action control
    exclude_actions: List[str] = Field(default_factory=list)
    max_actions_per_task: Optional[int] = 100
    
    # Output configuration
    display_files_in_done_text: bool = True
    save_screenshots: bool = True
    
    # Error handling
    continue_on_action_error: bool = False
    max_retries_per_action: int = 3

# Schema validation utilities
JSON_SCHEMAS = {
    'task_metadata': TaskMetadata,
    'llm_configuration': LLMConfiguration,
    'controller_configuration': ControllerConfiguration,
    'mcp_server_config': McpServerConfig,
    'mcp_server_params': McpServerParams,
}

def validate_json_field(schema_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
    """Validate and normalize JSON field data using appropriate schema"""
    if schema_name not in JSON_SCHEMAS:
        return data
    
    schema_class = JSON_SCHEMAS[schema_name]
    validated = schema_class(**data)
    return validated.model_dump(exclude_none=True)

def get_schema_for_config_type(config_type: str) -> Optional[BaseModel]:
    """Get appropriate schema based on configuration type"""
    schema_mapping = {
        'llm_config': LLMConfiguration,
        'controller_config': ControllerConfiguration,  # Legacy support
        'mcp_server_config': McpServerConfig,
    }
    return schema_mapping.get(config_type)


================================================
FILE: vibe_surf/backend/database/migrations/v001_initial_schema.sql
================================================
-- Migration: v001_initial_schema.sql
-- Description: Initial database schema creation
-- Version: 0.0.1

-- Enable foreign keys
PRAGMA foreign_keys = ON;

-- Create LLM Profiles table
CREATE TABLE IF NOT EXISTS llm_profiles (
    profile_id VARCHAR(36) NOT NULL PRIMARY KEY,
    profile_name VARCHAR(100) NOT NULL UNIQUE,
    provider VARCHAR(50) NOT NULL,
    model VARCHAR(100) NOT NULL,
    base_url VARCHAR(500),
    encrypted_api_key TEXT,
    temperature JSON,
    max_tokens JSON,
    top_p JSON,
    frequency_penalty JSON,
    seed JSON,
    provider_config JSON,
    description TEXT,
    is_active BOOLEAN NOT NULL DEFAULT 1,
    is_default BOOLEAN NOT NULL DEFAULT 0,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    last_used_at DATETIME
);

-- Create Tasks table
CREATE TABLE IF NOT EXISTS tasks (
    task_id VARCHAR(36) NOT NULL PRIMARY KEY,
    session_id VARCHAR(36) NOT NULL,
    task_description TEXT NOT NULL,
    status VARCHAR(9) NOT NULL DEFAULT 'pending',
    llm_profile_name VARCHAR(100) NOT NULL,
    upload_files_path VARCHAR(500),
    workspace_dir VARCHAR(500),
    mcp_server_config TEXT,
    task_result TEXT,
    error_message TEXT,
    report_path VARCHAR(500),
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    started_at DATETIME,
    completed_at DATETIME,
    task_metadata JSON,
    CHECK (status IN ('pending', 'running', 'paused', 'completed', 'failed', 'stopped'))
);

-- Create Uploaded Files table
CREATE TABLE IF NOT EXISTS uploaded_files (
    file_id VARCHAR(36) NOT NULL PRIMARY KEY,
    original_filename VARCHAR(255) NOT NULL,
    stored_filename VARCHAR(255) NOT NULL,
    file_path TEXT NOT NULL,
    session_id VARCHAR(255),
    file_size BIGINT NOT NULL,
    mime_type VARCHAR(100) NOT NULL,
    upload_time DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    relative_path TEXT NOT NULL,
    is_deleted BOOLEAN NOT NULL DEFAULT 0,
    deleted_at DATETIME
);

-- Create MCP Profiles table
CREATE TABLE IF NOT EXISTS mcp_profiles (
    mcp_id VARCHAR(36) NOT NULL PRIMARY KEY,
    display_name VARCHAR(100) NOT NULL UNIQUE,
    mcp_server_name VARCHAR(100) NOT NULL UNIQUE,
    mcp_server_params JSON NOT NULL,
    description TEXT,
    is_active BOOLEAN NOT NULL DEFAULT 1,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    last_used_at DATETIME
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_llm_profiles_name ON llm_profiles(profile_name);
CREATE INDEX IF NOT EXISTS idx_llm_profiles_active ON llm_profiles(is_active);
CREATE INDEX IF NOT EXISTS idx_llm_profiles_default ON llm_profiles(is_default);
CREATE INDEX IF NOT EXISTS idx_llm_profiles_provider ON llm_profiles(provider);

CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);
CREATE INDEX IF NOT EXISTS idx_tasks_session ON tasks(session_id);
CREATE INDEX IF NOT EXISTS idx_tasks_llm_profile ON tasks(llm_profile_name);
CREATE INDEX IF NOT EXISTS idx_tasks_created ON tasks(created_at);

CREATE INDEX IF NOT EXISTS idx_uploaded_files_session_time ON uploaded_files(session_id, upload_time);
CREATE INDEX IF NOT EXISTS idx_uploaded_files_active ON uploaded_files(is_deleted, upload_time);
CREATE INDEX IF NOT EXISTS idx_uploaded_files_filename ON uploaded_files(original_filename);

CREATE INDEX IF NOT EXISTS idx_mcp_profiles_display_name ON mcp_profiles(display_name);
CREATE INDEX IF NOT EXISTS idx_mcp_profiles_server_name ON mcp_profiles(mcp_server_name);
CREATE INDEX IF NOT EXISTS idx_mcp_profiles_active ON mcp_profiles(is_active);

-- Create triggers for automatic timestamp updates
CREATE TRIGGER IF NOT EXISTS update_llm_profiles_updated_at
AFTER UPDATE ON llm_profiles
FOR EACH ROW
BEGIN
    UPDATE llm_profiles SET updated_at = CURRENT_TIMESTAMP WHERE profile_id = OLD.profile_id;
END;

CREATE TRIGGER IF NOT EXISTS update_tasks_updated_at
AFTER UPDATE ON tasks
FOR EACH ROW
BEGIN
    UPDATE tasks SET updated_at = CURRENT_TIMESTAMP WHERE task_id = OLD.task_id;
END;

CREATE TRIGGER IF NOT EXISTS update_mcp_profiles_updated_at
AFTER UPDATE ON mcp_profiles
FOR EACH ROW
BEGIN
    UPDATE mcp_profiles SET updated_at = CURRENT_TIMESTAMP WHERE mcp_id = OLD.mcp_id;
END;


================================================
FILE: vibe_surf/backend/database/migrations/v002_add_agent_mode.sql
================================================
-- Migration: v002_add_agent_mode.sql
-- Description: Add agent_mode column to tasks table
-- Version: 0.0.2

-- Add agent_mode column to tasks table with default value 'thinking'
ALTER TABLE tasks ADD COLUMN agent_mode VARCHAR(50) DEFAULT 'thinking';


================================================
FILE: vibe_surf/backend/database/migrations/v003_fix_task_status_case.sql
================================================
-- Migration: v003_fix_task_status_case.sql
-- Description: Fix task status values to use lowercase (enum values)
-- Version: 0.0.3

-- Update any uppercase status values to lowercase to match TaskStatus enum
UPDATE tasks SET status = 'pending' WHERE status = 'PENDING';
UPDATE tasks SET status = 'running' WHERE status = 'RUNNING';
UPDATE tasks SET status = 'paused' WHERE status = 'PAUSED';
UPDATE tasks SET status = 'completed' WHERE status = 'COMPLETED';
UPDATE tasks SET status = 'failed' WHERE status = 'FAILED';
UPDATE tasks SET status = 'stopped' WHERE status = 'STOPPED';


================================================
FILE: vibe_surf/backend/database/migrations/v004_add_voice_profiles.sql
================================================
-- Migration: v004_add_voice_profiles.sql
-- Description: Add voice_profiles table for voice model management
-- Version: 0.0.4

-- Enable foreign keys
PRAGMA foreign_keys = ON;

-- Create Voice Profiles table
CREATE TABLE IF NOT EXISTS voice_profiles (
    profile_id VARCHAR(36) NOT NULL PRIMARY KEY,
    voice_profile_name VARCHAR(100) NOT NULL UNIQUE,
    voice_model_type VARCHAR(3) NOT NULL,
    voice_model_name VARCHAR(100) NOT NULL,
    encrypted_api_key TEXT,
    voice_meta_params JSON,
    description TEXT,
    is_active BOOLEAN NOT NULL DEFAULT 1,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    last_used_at DATETIME,
    CHECK (voice_model_type IN ('asr', 'tts'))
);

-- Create indexes for voice profiles
CREATE INDEX IF NOT EXISTS idx_voice_profiles_name ON voice_profiles(voice_profile_name);
CREATE INDEX IF NOT EXISTS idx_voice_profiles_type ON voice_profiles(voice_model_type);
CREATE INDEX IF NOT EXISTS idx_voice_profiles_active ON voice_profiles(is_active);

-- Create trigger for automatic timestamp updates
CREATE TRIGGER IF NOT EXISTS update_voice_profiles_updated_at
AFTER UPDATE ON voice_profiles
FOR EACH ROW
BEGIN
    UPDATE voice_profiles SET updated_at = CURRENT_TIMESTAMP WHERE profile_id = OLD.profile_id;
END;


================================================
FILE: vibe_surf/backend/utils/__init__.py
================================================
"""
Utilities package for VibeSurf Backend
"""

from .encryption import encrypt_api_key, decrypt_api_key, is_encrypted

__all__ = ['encrypt_api_key', 'decrypt_api_key', 'is_encrypted']


================================================
FILE: vibe_surf/backend/utils/encryption.py
================================================
"""
Encryption utilities for VibeSurf Backend

Uses machine MAC address for key derivation to encrypt sensitive data like API keys.
"""

import hashlib
import pdb
import uuid
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from getmac import get_mac_address

import logging

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


def derive_key(machine_id: str, salt: bytes = None) -> bytes:
    """Derive encryption key from machine ID."""
    if salt is None:
        # Use a fixed salt for consistency across sessions
        salt = b'vibesurf_warmshao_2025'
    
    # Convert machine_id to bytes
    password = machine_id.encode('utf-8')
    
    # Derive key using PBKDF2
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
    )
    key = base64.urlsafe_b64encode(kdf.derive(password))
    return key

def get_encryption_key() -> bytes:
    """Get the encryption key for this machine."""
    machine_id1 = get_mac_address()
    return derive_key(machine_id1)

def encrypt_api_key(api_key: str) -> str:
    """
    Encrypt API key using machine-specific key.
    
    Args:
        api_key: Plain text API key
        
    Returns:
        str: Base64 encoded encrypted API key
    """
    if not api_key or api_key.strip() == "":
        return ""
    
    try:
        key = get_encryption_key()
        fernet = Fernet(key)
        encrypted_data = fernet.encrypt(api_key.encode('utf-8'))
        return base64.urlsafe_b64encode(encrypted_data).decode('utf-8')
    except Exception as e:
        logger.error(f"Failed to encrypt API key: {e}")
        raise ValueError("Encryption failed")

def decrypt_api_key(encrypted_api_key: str) -> str:
    """
    Decrypt API key using machine-specific key.
    
    Args:
        encrypted_api_key: Base64 encoded encrypted API key
        
    Returns:
        str: Decrypted API key
    """
    if not encrypted_api_key or encrypted_api_key.strip() == "":
        return ""
    
    try:
        key = get_encryption_key()
        fernet = Fernet(key)
        encrypted_data = base64.urlsafe_b64decode(encrypted_api_key.encode('utf-8'))
        decrypted_data = fernet.decrypt(encrypted_data)
        return decrypted_data.decode('utf-8')
    except Exception as e:
        logger.error(f"Failed to decrypt API key: {e}")
        raise ValueError("Decryption failed")

def is_encrypted(value: str) -> bool:
    """
    Check if a value appears to be encrypted.
    
    Args:
        value: String to check
        
    Returns:
        bool: True if value appears encrypted
    """
    if not value:
        return False
    
    try:
        # Try to decode as base64
        base64.urlsafe_b64decode(value.encode('utf-8'))
        # If it's base64 and contains the Fernet token prefix, likely encrypted
        return len(value) > 50 and '=' in value
    except:
        return False

# Test functions
def test_encryption():
    """Test encryption/decryption functionality."""
    test_api_key = "sk-test123456789"
    
    try:
        # Test encryption
        encrypted = encrypt_api_key(test_api_key)
        print(f"Original: {test_api_key}")
        print(f"Encrypted: {encrypted}")
        
        # Test decryption
        decrypted = decrypt_api_key(encrypted)
        print(f"Decrypted: {decrypted}")
        
        # Verify
        assert test_api_key == decrypted, "Encryption/decryption failed"
        print("âœ… Encryption test passed")
        
    except Exception as e:
        print(f"âŒ Encryption test failed: {e}")

if __name__ == "__main__":
    test_encryption()


================================================
FILE: vibe_surf/backend/utils/llm_factory.py
================================================
"""
LLM Factory utilities for creating LLM instances from profiles
"""

from typing import Optional
import logging
# Import LLM classes from browser_use and vibe_surf
from browser_use.llm import (
    BaseChatModel,
    ChatOpenAI, ChatAnthropic, ChatGoogle, ChatAzureOpenAI,
    ChatGroq, ChatOllama, ChatOpenRouter, ChatDeepSeek,
    ChatAWSBedrock, ChatAnthropicBedrock
)
from vibe_surf.llm import ChatOpenAICompatible

from ..llm_config import get_supported_providers, is_provider_supported

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


def create_llm_from_profile(llm_profile) -> BaseChatModel:
    """Create LLM instance from LLMProfile database record (dict or object)"""
    try:

        # Handle both dict and object access patterns
        def get_attr(obj, key, default=None):
            if isinstance(obj, dict):
                return obj.get(key, default)
            else:
                return getattr(obj, key, default)

        provider = get_attr(llm_profile, 'provider')
        model = get_attr(llm_profile, 'model')
        api_key = get_attr(llm_profile, 'api_key')  # Should already be decrypted by queries
        base_url = get_attr(llm_profile, 'base_url')
        temperature = get_attr(llm_profile, 'temperature') or 0.7
        max_tokens = get_attr(llm_profile, 'max_tokens')
        top_p = get_attr(llm_profile, 'top_p')
        frequency_penalty = get_attr(llm_profile, 'frequency_penalty')
        seed = get_attr(llm_profile, 'seed')
        provider_config = get_attr(llm_profile, 'provider_config', {})

        # Validate provider
        if not is_provider_supported(provider):
            raise ValueError(f"Unsupported provider: {provider}. Supported: {get_supported_providers()}")

        # Define provider-specific parameter support
        provider_param_support = {
            "openai": ["temperature"],
            "anthropic": ["temperature"],
            "google": ["temperature"],
            "azure_openai": ["temperature"],
            "groq": ["temperature"],
            "ollama": [],
            "openrouter": ["temperature"],  # OpenRouter doesn't support max_tokens
            "deepseek": ["temperature"],
            "aws_bedrock": ["temperature"],
            "anthropic_bedrock": ["temperature"],
            "openai_compatible": ["temperature", "max_tokens"]
        }

        # Build common parameters based on provider support
        supported_params = provider_param_support.get(provider, [])
        common_params = {}

        if temperature is not None and "temperature" in supported_params:
            common_params["temperature"] = temperature
        if max_tokens is not None and "max_tokens" in supported_params:
            common_params["max_tokens"] = max_tokens
        if top_p is not None and "top_p" in supported_params:
            common_params["top_p"] = top_p
        if frequency_penalty is not None and "frequency_penalty" in supported_params:
            common_params["frequency_penalty"] = frequency_penalty
        if seed is not None and "seed" in supported_params:
            common_params["seed"] = seed

        # Add provider-specific config if available
        if provider_config:
            common_params.update(provider_config)

        # Create LLM instance based on provider
        if provider == "openai":
            params = {
                "model": model,
                "api_key": api_key,
                **common_params
            }
            if base_url:
                params["base_url"] = base_url
            return ChatOpenAI(**params)

        elif provider == "anthropic":
            return ChatAnthropic(
                model=model,
                api_key=api_key,
                **common_params
            )

        elif provider == "google":
            return ChatGoogle(
                model=model,
                api_key=api_key,
                **common_params
            )

        elif provider == "azure_openai":
            if not base_url:
                raise ValueError("Azure OpenAI requires base_url (azure_endpoint)")
            return ChatAzureOpenAI(
                model=model,
                api_version="2025-01-01-preview",
                api_key=api_key,
                azure_endpoint=base_url,
                **common_params
            )

        elif provider == "groq":
            return ChatGroq(
                model=model,
                api_key=api_key,
                **common_params
            )

        elif provider == "ollama":
            params = {
                "model": model,
                **common_params
            }
            if base_url:
                params["host"] = base_url
            else:
                params["host"] = "http://localhost:11434"  # Default Ollama URL
            return ChatOllama(**params)

        elif provider == "openrouter":
            return ChatOpenRouter(
                model=model,
                api_key=api_key,
                **common_params
            )

        elif provider == "deepseek":
            return ChatOpenAICompatible(
                model=model,
                base_url="https://api.deepseek.com",
                api_key=api_key,
                **common_params
            )

        elif provider == "aws_bedrock":
            params = {
                "model": model,
                "aws_access_key_id": api_key,  # AWS uses different auth
                **common_params
            }
            # Add AWS-specific parameters from provider_config
            if "aws_secret_access_key" in provider_config:
                params["aws_secret_access_key"] = provider_config["aws_secret_access_key"]
            if "aws_region" in provider_config:
                params["aws_region"] = provider_config["aws_region"]
            if 'aws_region' not in params:
                params["aws_region"] = "us-east-1"
            return ChatAWSBedrock(**params)

        elif provider == "anthropic_bedrock":
            params = {
                "model": model,
                "aws_access_key_id": api_key,  # AWS uses different auth
                **common_params
            }
            # Add AWS-specific parameters from provider_config
            if "aws_secret_access_key" in provider_config:
                params["aws_secret_access_key"] = provider_config["aws_secret_access_key"]
            if "region_name" in provider_config:
                params["region_name"] = provider_config["region_name"]
            return ChatAnthropicBedrock(**params)

        elif provider == "openai_compatible":
            if not base_url:
                raise ValueError("OpenAI Compatible provider requires base_url")
            return ChatOpenAICompatible(
                model=model,
                api_key=api_key,
                base_url=base_url,
                **common_params
            )

        else:
            raise ValueError(f"Unsupported provider: {provider}")

    except Exception as e:
        logger.error(f"Failed to create LLM from profile: {e}")
        raise RuntimeError(f"Failed to create LLM from profile: {str(e)}")


def validate_llm_configuration(provider: str, model: str, api_key: str, base_url: Optional[str] = None):
    """Validate LLM configuration parameters"""
    if not provider:
        raise ValueError("Provider is required")

    if not model:
        raise ValueError("Model is required")

    if not is_provider_supported(provider):
        raise ValueError(f"Unsupported provider: {provider}. Supported: {get_supported_providers()}")

    # Provider-specific validation
    from ..llm_config import get_provider_metadata
    metadata = get_provider_metadata(provider)

    if metadata.get("requires_api_key", True) and not api_key:
        raise ValueError(f"API key is required for provider: {provider}")

    if metadata.get("requires_base_url", False) and not base_url:
        raise ValueError(f"Base URL is required for provider: {provider}")

    return True


def get_llm_creation_parameters(provider: str):
    """Get the required and optional parameters for creating an LLM instance"""
    from ..llm_config import get_provider_metadata

    if not is_provider_supported(provider):
        raise ValueError(f"Unsupported provider: {provider}")

    metadata = get_provider_metadata(provider)

    required_params = ["model"]
    optional_params = ["temperature", "max_tokens", "top_p", "frequency_penalty", "seed"]

    if metadata.get("requires_api_key", True):
        required_params.append("api_key")

    if metadata.get("requires_base_url", False):
        required_params.append("base_url")
    elif metadata.get("supports_base_url", False):
        optional_params.append("base_url")

    # Special cases for AWS Bedrock
    if provider in ["aws_bedrock", "anthropic_bedrock"]:
        required_params.extend(["aws_secret_access_key", "region_name"])

    return {
        "required": required_params,
        "optional": optional_params,
        "metadata": metadata
    }



================================================
FILE: vibe_surf/backend/utils/utils.py
================================================
import pdb
import urllib.request
import os
from vibe_surf.logger import get_logger

logger = get_logger(__name__)

def configure_system_proxies():
    """
    Get system proxy settings using urllib.request.getproxies()
    and set them as HTTP_PROXY and HTTPS_PROXY environment variables.
    """

    # 1. Get system proxy setting
    try:
        proxies = urllib.request.getproxies()
        logger.info(proxies)
    except Exception as e:
        # Simple error handling
        logger.error(e)
        return

    if not proxies:
        logger.info("No system proxies detected.")
        return

    logger.debug(f"Detected system proxies: {proxies}")

    # 2. Configure HTTP_PROXY
    http_proxy = proxies.get('http')
    if http_proxy:
        os.environ['HTTP_PROXY'] = http_proxy
        logger.info(f"Set HTTP_PROXY to: {http_proxy}")

    # 3. Configure HTTPS_PROXY
    https_proxy = proxies.get('https')
    if https_proxy:
        os.environ['HTTPS_PROXY'] = https_proxy
        logger.info(f"Set HTTPS_PROXY to: {https_proxy}")

    if http_proxy or https_proxy:
        os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'


================================================
FILE: vibe_surf/browser/__init__.py
================================================
from typing import Optional, TYPE_CHECKING

from vibe_surf.browser.browser_manager import BrowserManager
from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.browser.agen_browser_profile import AgentBrowserProfile


__all__ = [ "AgentBrowserSession", "AgentBrowserProfile", "BrowserManager"]


================================================
FILE: vibe_surf/browser/agen_browser_profile.py
================================================
import pdb
import sys
import tempfile
from collections.abc import Iterable
from enum import Enum
from functools import cache
from pathlib import Path
from re import Pattern
from typing import Annotated, Any, Literal, Self
from urllib.parse import urlparse

from pydantic import AfterValidator, AliasChoices, BaseModel, ConfigDict, Field, field_validator, model_validator
from uuid_extensions import uuid7str

from browser_use.config import CONFIG
from browser_use.observability import observe_debug
from browser_use.utils import _log_pretty_path, logger

from browser_use.browser import BrowserProfile
from browser_use.browser.profile import CHROME_DEFAULT_ARGS, CHROME_DOCKER_ARGS, CHROME_HEADLESS_ARGS, \
    CHROME_DETERMINISTIC_RENDERING_ARGS, CHROME_DISABLE_SECURITY_ARGS, BrowserLaunchArgs


class AgentBrowserProfile(BrowserProfile):
    custom_extensions: list[str] | None = Field(
        default=None,
        description="Enable Custom Extensions.",
    )

    def _ensure_default_extensions_downloaded(self) -> list[str]:
        """
        Ensure default extensions are downloaded and cached locally.
        Returns list of paths to extension directories.
        """

        # Extension definitions - optimized for automation and content extraction
        extensions = [
            # {
            #     'name': 'uBlock Origin',
            #     'id': 'cjpalhdlnbpafiamejdnhcphjbkeiagm',
            #     'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dcjpalhdlnbpafiamejdnhcphjbkeiagm%26uc',
            # },
            {
                'name': "I still don't care about cookies",
                'id': 'edibdbjcniadpccecjdfdjjppcpchdlm',
                'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dedibdbjcniadpccecjdfdjjppcpchdlm%26uc',
            },
            # {
            #     'name': 'ClearURLs',
            #     'id': 'lckanjgmijmafbedllaakclkaicjfmnk',
            #     'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dlckanjgmijmafbedllaakclkaicjfmnk%26uc',
            # },
            # {
            # 	'name': 'Captcha Solver: Auto captcha solving service',
            # 	'id': 'pgojnojmmhpofjgdmaebadhbocahppod',
            # 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dpgojnojmmhpofjgdmaebadhbocahppod%26uc',
            # },
            # {
            # 	'name': 'Consent-O-Matic',
            # 	'id': 'mdjildafknihdffpkfmmpnpoiajfjnjd',
            # 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dmdjildafknihdffpkfmmpnpoiajfjnjd%26uc',
            # },
            # {
            # 	'name': 'Privacy | Protect Your Payments',
            # 	'id': 'hmgpakheknboplhmlicfkkgjipfabmhp',
            # 	'url': 'https://clients2.google.com/service/update2/crx?response=redirect&prodversion=130&acceptformat=crx3&x=id%3Dhmgpakheknboplhmlicfkkgjipfabmhp%26uc',
            # },
        ]

        # Create extensions cache directory
        cache_dir = CONFIG.BROWSER_USE_EXTENSIONS_DIR
        cache_dir.mkdir(parents=True, exist_ok=True)
        # logger.debug(f'ğŸ“ Extensions cache directory: {_log_pretty_path(cache_dir)}')

        extension_paths = []
        loaded_extension_names = []

        for ext in extensions:
            ext_dir = cache_dir / ext['id']
            crx_file = cache_dir / f'{ext["id"]}.crx'

            # Check if extension is already extracted
            if ext_dir.exists() and (ext_dir / 'manifest.json').exists():
                # logger.debug(f'âœ… Using cached {ext["name"]} extension from {_log_pretty_path(ext_dir)}')
                extension_paths.append(str(ext_dir))
                loaded_extension_names.append(ext['name'])
                continue

            try:
                # Download extension if not cached
                if not crx_file.exists():
                    logger.info(f'ğŸ“¦ Downloading {ext["name"]} extension...')
                    self._download_extension(ext['url'], crx_file)
                else:
                    logger.debug(f'ğŸ“¦ Found cached {ext["name"]} .crx file')

                # Extract extension
                logger.info(f'ğŸ“‚ Extracting {ext["name"]} extension...')
                self._extract_extension(crx_file, ext_dir)
                extension_paths.append(str(ext_dir))
                loaded_extension_names.append(ext['name'])

            except Exception as e:
                logger.warning(f'âš ï¸ Failed to setup {ext["name"]} extension: {e}')
                continue

        if extension_paths:
            logger.debug(
                f'[BrowserProfile] ğŸ§© Extensions loaded ({len(extension_paths)}): [{", ".join(loaded_extension_names)}]')
        else:
            logger.warning('[BrowserProfile] âš ï¸ No default extensions could be loaded')

        return extension_paths

    def _get_extension_args(self) -> list[str]:
        """Get Chrome args for enabling default extensions (ad blocker and cookie handler)."""
        extension_paths = self._ensure_default_extensions_downloaded()

        args = [
            '--enable-extensions',
            '--disable-extensions-file-access-check',
            '--disable-extensions-http-throttling',
            '--enable-extension-activity-logging',
            '--disable-features=DisableLoadExtensionCommandLineSwitch'
        ]
        if self.custom_extensions:
            extension_paths.extend(self.custom_extensions)
        if extension_paths:
            args.append(f'--load-extension={",".join(extension_paths)}')
        logger.info(f"Extension infos: {args}")
        return args



================================================
FILE: vibe_surf/browser/agent_browser_session.py
================================================
from __future__ import annotations

import asyncio
import os
import pdb
from pathlib import Path
from typing import Any, List, Optional

from browser_use.browser.session import BrowserSession, CDPSession
from pydantic import Field
from browser_use.browser.events import (
    NavigationCompleteEvent,
)
from browser_use.utils import _log_pretty_url, is_new_tab_page, time_execution_async
import time
from browser_use.browser.profile import BrowserProfile
from browser_use.browser.views import BrowserStateSummary
from browser_use.dom.views import TargetInfo
from vibe_surf.browser.agen_browser_profile import AgentBrowserProfile
from typing import Self
from uuid_extensions import uuid7str
import httpx
from browser_use.browser.views import BrowserStateSummary, TabInfo
from browser_use.dom.views import EnhancedDOMTreeNode, TargetInfo
from browser_use.observability import observe_debug
from cdp_use import CDPClient
from browser_use.browser.events import (
    AgentFocusChangedEvent,
    BrowserConnectedEvent,
    BrowserErrorEvent,
    BrowserLaunchEvent,
    BrowserLaunchResult,
    BrowserStartEvent,
    BrowserStateRequestEvent,
    BrowserStopEvent,
    BrowserStoppedEvent,
    CloseTabEvent,
    FileDownloadedEvent,
    NavigateToUrlEvent,
    NavigationCompleteEvent,
    NavigationStartedEvent,
    SwitchTabEvent,
    TabClosedEvent,
    TabCreatedEvent,
)
from browser_use.browser.profile import BrowserProfile, ProxySettings

DEFAULT_BROWSER_PROFILE = AgentBrowserProfile()


class AgentBrowserSession(BrowserSession):
    """Isolated browser session for a specific agent."""

    def __init__(
            self,
            # Core configuration
            id: str | None = None,
            cdp_url: str | None = None,
            is_local: bool = False,
            browser_profile: AgentBrowserProfile | None = None,
            # Custom AgentBrowserSession fields
            main_browser_session: BrowserSession | None = None,
            # BrowserProfile fields that can be passed directly
            # From BrowserConnectArgs
            headers: dict[str, str] | None = None,
            # From BrowserLaunchArgs
            env: dict[str, str | float | bool] | None = None,
            executable_path: str | Path | None = None,
            headless: bool | None = None,
            args: list[str] | None = None,
            ignore_default_args: list[str] | list[bool] | None = None,
            channel: str | None = None,
            chromium_sandbox: bool | None = None,
            devtools: bool | None = None,
            downloads_path: str | Path | None = None,
            traces_dir: str | Path | None = None,
            # From BrowserContextArgs
            accept_downloads: bool | None = None,
            permissions: list[str] | None = None,
            user_agent: str | None = None,
            screen: dict | None = None,
            viewport: dict | None = None,
            no_viewport: bool | None = None,
            device_scale_factor: float | None = None,
            record_har_content: str | None = None,
            record_har_mode: str | None = None,
            record_har_path: str | Path | None = None,
            record_video_dir: str | Path | None = None,
            # From BrowserLaunchPersistentContextArgs
            user_data_dir: str | Path | None = None,
            # From BrowserNewContextArgs
            storage_state: str | Path | dict[str, Any] | None = None,
            # BrowserProfile specific fields
            disable_security: bool | None = None,
            deterministic_rendering: bool | None = None,
            allowed_domains: list[str] | None = None,
            keep_alive: bool | None = None,
            proxy: ProxySettings | None = None,
            enable_default_extensions: bool | None = None,
            window_size: dict | None = None,
            window_position: dict | None = None,
            cross_origin_iframes: bool | None = None,
            minimum_wait_page_load_time: float | None = None,
            wait_for_network_idle_page_load_time: float | None = None,
            wait_between_actions: float | None = None,
            highlight_elements: bool | None = None,
            filter_highlight_ids: bool | None = None,
            auto_download_pdfs: bool | None = None,
            profile_directory: str | None = None,
            cookie_whitelist_domains: list[str] | None = None,
            # AgentBrowserProfile specific fields
            custom_extensions: list[str] | None = None,
    ):
        # Filter out AgentBrowserSession specific parameters
        agent_session_params = {
            'main_browser_session': main_browser_session,
        }

        # Get all browser profile parameters
        profile_kwargs = {k: v for k, v in locals().items()
                          if k not in ['self', 'browser_profile', 'id', 'main_browser_session']
                          and v is not None}

        # Apply BrowserSession's is_local logic first
        effective_is_local = is_local
        if is_local is False and executable_path is not None:
            effective_is_local = True
        if not cdp_url:
            effective_is_local = True

        # Always include is_local in profile_kwargs to ensure it's properly set
        profile_kwargs['is_local'] = effective_is_local

        # Create AgentBrowserProfile from direct parameters or use provided one
        if browser_profile is not None:
            # Always merge to ensure is_local logic is applied
            merged_kwargs = {**browser_profile.model_dump(), **profile_kwargs}
            resolved_browser_profile = AgentBrowserProfile(**merged_kwargs)
        else:
            resolved_browser_profile = AgentBrowserProfile(**profile_kwargs)

        # Initialize the Pydantic model directly (like BrowserSession does)
        # Don't call BrowserSession.__init__ as it would recreate BrowserProfile and lose custom_extensions
        from pydantic import BaseModel
        BaseModel.__init__(
            self,
            id=id or str(uuid7str()),
            browser_profile=resolved_browser_profile,
        )

        # Set AgentBrowserSession specific fields
        self.main_browser_session = main_browser_session

    # Override browser_profile field to ensure it's always AgentBrowserProfile
    browser_profile: AgentBrowserProfile = Field(
        default_factory=lambda: DEFAULT_BROWSER_PROFILE,
        description='AgentBrowserProfile() options to use for the session',
    )
    main_browser_session: BrowserSession | None = Field(default=None)

    async def connect(self, cdp_url: str | None = None) -> Self:
        """Connect to a remote chromium-based browser via CDP using cdp-use.

        This MUST succeed or the browser is unusable. Fails hard on any error.
        """

        self.browser_profile.cdp_url = cdp_url or self.cdp_url
        if not self.cdp_url:
            raise RuntimeError('Cannot setup CDP connection without CDP URL')

        if not self.cdp_url.startswith('ws'):
            # If it's an HTTP URL, fetch the WebSocket URL from /json/version endpoint
            url = self.cdp_url.rstrip('/')
            if not url.endswith('/json/version'):
                url = url + '/json/version'

            # Run a tiny HTTP client to query for the WebSocket URL from the /json/version endpoint
            async with httpx.AsyncClient() as client:
                headers = self.browser_profile.headers or {}
                version_info = await client.get(url, headers=headers)
                self.browser_profile.cdp_url = version_info.json()['webSocketDebuggerUrl']

        assert self.cdp_url is not None

        browser_location = 'local browser' if self.is_local else 'remote browser'
        self.logger.debug(
            f'ğŸŒ Connecting to existing chromium-based browser via CDP: {self.cdp_url} -> ({browser_location})')

        try:
            # Import cdp-use client

            # Convert HTTP URL to WebSocket URL if needed

            # Create and store the CDP client for direct CDP communication
            self._cdp_client_root = CDPClient(self.cdp_url)
            assert self._cdp_client_root is not None
            await self._cdp_client_root.start()
            await self._cdp_client_root.send.Target.setAutoAttach(
                params={'autoAttach': True, 'waitForDebuggerOnStart': False, 'flatten': True}
            )
            self.logger.debug('CDP client connected successfully')

            # Get browser targets to find available contexts/pages
            targets = await self._cdp_client_root.send.Target.getTargets()

            # Find main browser pages (avoiding iframes, workers, extensions, etc.)
            page_targets: list[TargetInfo] = [
                t
                for t in targets['targetInfos']
                if self._is_valid_target(
                    t, include_http=True, include_about=True, include_pages=True, include_iframes=False,
                    include_workers=False
                )
            ]

            # Check for chrome://newtab pages and immediately redirect them
            # to about:blank to avoid JS issues from CDP on chrome://* urls
            from browser_use.utils import is_new_tab_page

            # Collect all targets that need redirection
            redirected_targets = []
            redirect_sessions = {}  # Store sessions created for redirection to potentially reuse
            for target in page_targets:
                target_url = target.get('url', '')
                if is_new_tab_page(target_url) and target_url != '':
                    # Redirect chrome://newtab to about:blank to avoid JS issues preventing driving chrome://newtab
                    target_id = target['targetId']
                    self.logger.debug(f'ğŸ”„ Redirecting {target_url} to about:blank for target {target_id}')
                    try:
                        # Create a CDP session for redirection (minimal domains to avoid duplicate event handlers)
                        # Only enable Page domain for navigation, avoid duplicate event handlers
                        redirect_session = await CDPSession.for_target(self._cdp_client_root, target_id,
                                                                       domains=['Page'])
                        # Navigate to about:blank
                        await redirect_session.cdp_client.send.Page.navigate(
                            params={'url': ''}, session_id=redirect_session.session_id
                        )
                        redirected_targets.append(target_id)
                        redirect_sessions[target_id] = redirect_session  # Store for potential reuse
                        # Update the target's URL in our list for later use
                        target['url'] = ''
                        # Small delay to ensure navigation completes
                        await asyncio.sleep(0.1)
                    except Exception as e:
                        self.logger.warning(f'Failed to redirect {target_url} to about:blank: {e}')

            # Log summary of redirections
            if redirected_targets:
                self.logger.debug(f'Redirected {len(redirected_targets)} chrome://newtab pages to about:blank')

            if not page_targets:
                # No pages found, create a new one
                new_target = await self._cdp_client_root.send.Target.createTarget(params={'url': ''})
                target_id = new_target['targetId']
                self.logger.debug(f'ğŸ“„ Created new blank page with target ID: {target_id}')
            else:
                # Use the first available page
                target_id = [page for page in page_targets if page.get('type') == 'page'][0]['targetId']
                self.logger.debug(f'ğŸ“„ Using existing page with target ID: {target_id}')

            # Store the current page target ID and add to pool
            # Reuse redirect session if available, otherwise create new one
            if target_id in redirect_sessions:
                self.logger.debug(f'Reusing redirect session for target {target_id}')
                self.agent_focus = redirect_sessions[target_id]
            else:
                # For the initial connection, we'll use the shared root WebSocket
                self.agent_focus = await CDPSession.for_target(self._cdp_client_root, target_id, new_socket=False)
            if self.agent_focus:
                self._cdp_session_pool[target_id] = self.agent_focus

            # Enable proxy authentication handling if configured
            await self._setup_proxy_auth()

            # Verify the session is working
            try:
                if self.agent_focus:
                    assert self.agent_focus.title != 'Unknown title'
                else:
                    raise RuntimeError('Failed to create CDP session')
            except Exception as e:
                self.logger.warning(f'Failed to create CDP session: {e}')
                raise

            # Dispatch TabCreatedEvent for all initial tabs (so watchdogs can initialize)
            # This replaces the duplicated logic from navigation_watchdog's _initialize_agent_focus
            for idx, target in enumerate(page_targets):
                target_url = target.get('url', '')
                self.logger.debug(f'Dispatching TabCreatedEvent for initial tab {idx}: {target_url}')
                await self.event_bus.dispatch(TabCreatedEvent(url=target_url, target_id=target['targetId']))

            # Dispatch initial focus event
            if page_targets:
                initial_url = page_targets[0].get('url', '')
                await self.event_bus.dispatch(
                    AgentFocusChangedEvent(target_id=page_targets[0]['targetId'], url=initial_url))
                self.logger.debug(f'Initial agent focus set to tab 0: {initial_url}')

        except Exception as e:
            # Fatal error - browser is not usable without CDP connection
            self.logger.error(f'âŒ FATAL: Failed to setup CDP connection: {e}')
            self.logger.error('âŒ Browser cannot continue without CDP connection')
            # Clean up any partial state
            self._cdp_client_root = None
            self.agent_focus = None
            # Re-raise as a fatal error
            raise RuntimeError(f'Failed to establish CDP connection to browser: {e}') from e

        return self

    async def connect_agent(self, target_id: str) -> Self:
        """Register agent to browser with optional target assignment."""
        # First ensure the parent BrowserSession is properly connected
        if not hasattr(self, '_cdp_client_root') or self._cdp_client_root is None:
            await self.connect()

        assigned_target_ids = self._cdp_session_pool.keys()
        if target_id not in assigned_target_ids:
            self.logger.info(f"Agent {self.id}: Assigned target {target_id}")
            self.agent_focus = await CDPSession.for_target(self._cdp_client_root, target_id, new_socket=True,
                                                           cdp_url=self.cdp_url)
            await self.agent_focus.cdp_client.send.Target.activateTarget(
                params={'targetId': target_id})
            await self.agent_focus.cdp_client.send.Runtime.runIfWaitingForDebugger(
                session_id=self.agent_focus.session_id)
            self._cdp_session_pool[target_id] = self.agent_focus
        return self

    async def disconnect_agent(self) -> None:
        """Disconnect all agent-specific CDP sessions and cleanup security context."""
        for session in self._cdp_session_pool.values():
            await session.disconnect()
        self._cdp_session_pool.clear()
        self.main_browser_session = None

    async def _cdp_get_all_pages(
            self,
            include_http: bool = True,
            include_about: bool = True,
            include_pages: bool = True,
            include_iframes: bool = False,
            include_workers: bool = False,
            include_chrome: bool = False,
            include_chrome_extensions: bool = False,
            include_chrome_error: bool = False,
    ) -> list[TargetInfo]:
        """Get all browser pages/tabs using CDP Target.getTargets."""
        # Safety check - return empty list if browser not connected yet
        if not self._cdp_client_root:
            return []
        targets = await self.cdp_client.send.Target.getTargets()
        if self.main_browser_session is not None:
            assigned_target_ids = self._cdp_session_pool.keys()
            return [
                t
                for t in targets.get('targetInfos', [])
                if self._is_valid_target(
                    t,
                    include_http=include_http,
                    include_about=include_about,
                    include_pages=include_pages,
                    include_iframes=include_iframes,
                    include_workers=include_workers,
                    include_chrome=include_chrome,
                    include_chrome_extensions=include_chrome_extensions,
                    include_chrome_error=include_chrome_error,
                ) and t.get('targetId') in assigned_target_ids
            ]
        else:
            # Filter for valid page/tab targets only
            return [
                t
                for t in targets.get('targetInfos', [])
                if self._is_valid_target(
                    t,
                    include_http=include_http,
                    include_about=include_about,
                    include_pages=include_pages,
                    include_iframes=include_iframes,
                    include_workers=include_workers,
                    include_chrome=include_chrome,
                    include_chrome_extensions=include_chrome_extensions,
                    include_chrome_error=include_chrome_error,
                )
            ]

    def model_post_init(self, __context) -> None:
        """Register event handlers after model initialization."""
        # Check if handlers are already registered to prevent duplicates

        from browser_use.browser.watchdog_base import BaseWatchdog

        start_handlers = self.event_bus.handlers.get('BrowserStartEvent', [])
        start_handler_names = [getattr(h, '__name__', str(h)) for h in start_handlers]

        if any('on_BrowserStartEvent' in name for name in start_handler_names):
            raise RuntimeError(
                '[BrowserSession] Duplicate handler registration attempted! '
                'on_BrowserStartEvent is already registered. '
                'This likely means BrowserSession was initialized multiple times with the same EventBus.'
            )

        BaseWatchdog.attach_handler_to_session(self, BrowserStartEvent, self.on_BrowserStartEvent)
        BaseWatchdog.attach_handler_to_session(self, BrowserStopEvent, self.on_BrowserStopEvent)
        BaseWatchdog.attach_handler_to_session(self, NavigateToUrlEvent, self.on_NavigateToUrlEvent)
        BaseWatchdog.attach_handler_to_session(self, SwitchTabEvent, self.on_SwitchTabEvent)
        BaseWatchdog.attach_handler_to_session(self, TabCreatedEvent, self.on_TabCreatedEvent)
        BaseWatchdog.attach_handler_to_session(self, TabClosedEvent, self.on_TabClosedEvent)
        BaseWatchdog.attach_handler_to_session(self, AgentFocusChangedEvent, self.on_AgentFocusChangedEvent)
        # BaseWatchdog.attach_handler_to_session(self, FileDownloadedEvent, self.on_FileDownloadedEvent)
        BaseWatchdog.attach_handler_to_session(self, CloseTabEvent, self.on_CloseTabEvent)

    async def attach_all_watchdogs(self) -> None:
        """Initialize and attach all watchdogs EXCEPT AboutBlankWatchdog to disable DVD animation."""
        # Prevent duplicate watchdog attachment
        if hasattr(self, '_watchdogs_attached') and self._watchdogs_attached:
            self.logger.debug('Watchdogs already attached, skipping duplicate attachment')
            return

        # Import all watchdogs except AboutBlankWatchdog
        from vibe_surf.browser.watchdogs.action_watchdog import CustomActionWatchdog
        from vibe_surf.browser.watchdogs.dom_watchdog import CustomDOMWatchdog

        from browser_use.browser.watchdogs.downloads_watchdog import DownloadsWatchdog
        from browser_use.browser.watchdogs.local_browser_watchdog import LocalBrowserWatchdog
        from browser_use.browser.watchdogs.permissions_watchdog import PermissionsWatchdog
        from browser_use.browser.watchdogs.popups_watchdog import PopupsWatchdog
        from browser_use.browser.watchdogs.screenshot_watchdog import ScreenshotWatchdog
        from browser_use.browser.watchdogs.security_watchdog import SecurityWatchdog

        # NOTE: AboutBlankWatchdog is deliberately excluded to disable DVD animation

        self.logger.info('ğŸš« VibeSurfBrowserSession: AboutBlankWatchdog disabled - no DVD animation will be shown')

        # Initialize DownloadsWatchdog
        # DownloadsWatchdog.model_rebuild()
        # self._downloads_watchdog = DownloadsWatchdog(event_bus=self.event_bus, browser_session=self)
        # self._downloads_watchdog.attach_to_session()
        # if self.browser_profile.auto_download_pdfs:
        #     self.logger.info('ğŸ“„ PDF auto-download enabled for this session')

        # Initialize LocalBrowserWatchdog
        LocalBrowserWatchdog.model_rebuild()
        self._local_browser_watchdog = LocalBrowserWatchdog(event_bus=self.event_bus, browser_session=self)
        self._local_browser_watchdog.attach_to_session()

        # Initialize SecurityWatchdog (hooks NavigationWatchdog and implements allowed_domains restriction)
        SecurityWatchdog.model_rebuild()
        self._security_watchdog = SecurityWatchdog(event_bus=self.event_bus, browser_session=self)
        self._security_watchdog.attach_to_session()

        # Initialize PopupsWatchdog (handles accepting and dismissing JS dialogs, alerts, confirm, onbeforeunload, etc.)
        PopupsWatchdog.model_rebuild()
        self._popups_watchdog = PopupsWatchdog(event_bus=self.event_bus, browser_session=self)
        self._popups_watchdog.attach_to_session()

        # Initialize PermissionsWatchdog (handles granting and revoking browser permissions like clipboard, microphone, camera, etc.)
        # PermissionsWatchdog.model_rebuild()
        # self._permissions_watchdog = PermissionsWatchdog(event_bus=self.event_bus, browser_session=self)
        # self._permissions_watchdog.attach_to_session()

        # Initialize DefaultActionWatchdog (handles all default actions like click, type, scroll, go back, go forward, refresh, wait, send keys, upload file, scroll to text, etc.)
        CustomActionWatchdog.model_rebuild()
        self._default_action_watchdog = CustomActionWatchdog(event_bus=self.event_bus, browser_session=self)
        self._default_action_watchdog.attach_to_session()

        # Initialize ScreenshotWatchdog (handles taking screenshots of the browser)
        ScreenshotWatchdog.model_rebuild()
        self._screenshot_watchdog = ScreenshotWatchdog(event_bus=self.event_bus, browser_session=self)
        self._screenshot_watchdog.attach_to_session()

        # Initialize DOMWatchdog (handles building the DOM tree and detecting interactive elements, depends on ScreenshotWatchdog)
        CustomDOMWatchdog.model_rebuild()
        self._dom_watchdog = CustomDOMWatchdog(event_bus=self.event_bus, browser_session=self)
        self._dom_watchdog.attach_to_session()

        # Mark watchdogs as attached to prevent duplicate attachment
        self._watchdogs_attached = True

        self.logger.info('âœ… VibeSurfBrowserSession: All watchdogs attached (AboutBlankWatchdog excluded)')

    def get_cdp_session_pool(self):
        return self._cdp_session_pool

    async def active_focus_page(self):
        if self.agent_focus is None:
            self.logger.info('No active focus page found, cannot active!')
            return
        await self.get_or_create_cdp_session(self.agent_focus.target_id, focus=True)

    async def navigate_to_url(self, url: str, new_tab: bool = False) -> Optional[str]:
        """
        Concurrent navigation method that bypasses serial bottlenecks in on_NavigateToUrlEvent.
        
        This method performs minimal event dispatching and direct CDP calls for maximum concurrency.
        """
        if not self.agent_focus:
            self.logger.warning('Cannot navigate - browser not connected')
            return None

        target_id = None

        try:
            # Minimal target handling - avoid expensive _cdp_get_all_pages() call
            if new_tab:
                # Create new tab directly via CDP - no event system overhead
                result = await self._cdp_client_root.send.Target.createTarget(
                    params={'url': 'about:blank', 'newWindow': False, 'background': False}
                )
                target_id = result['targetId']

                # Create CDP session with dedicated WebSocket for this target
                session = await self.get_or_create_cdp_session(target_id, focus=True)
                self.agent_focus = session

                # Activate target without events
                await session.cdp_client.send.Target.activateTarget(params={'targetId': target_id})
                await session.cdp_client.send.Runtime.runIfWaitingForDebugger(session_id=session.session_id)
            else:
                # Use current tab - no tab switching events
                target_id = self.agent_focus.target_id

            # Direct CDP navigation - bypasses all event system overhead
            await self.agent_focus.cdp_client.send.Page.navigate(
                params={
                    'url': url,
                    'transitionType': 'address_bar',
                },
                session_id=self.agent_focus.session_id,
            )

            # Minimal delay for navigation to start
            await asyncio.sleep(0.2)

            # Optional: Dispatch only essential completion event (non-blocking)
            self.event_bus.dispatch(
                NavigationCompleteEvent(
                    target_id=target_id,
                    url=url,
                    status=None,
                )
            )

        except Exception as e:
            self.logger.error(f'Concurrent navigation failed: {type(e).__name__}: {e}')
            if target_id:
                # Non-blocking error event
                self.event_bus.dispatch(
                    NavigationCompleteEvent(
                        target_id=target_id,
                        url=url,
                        error_message=f'{type(e).__name__}: {e}',
                    )
                )
            raise
        finally:
            return target_id

    async def _wait_for_stable_network(self):
        """Wait for page stability - simplified for CDP-only branch."""
        start_time = time.time()
        page_url = await self.get_current_page_url()
        not_a_meaningful_website = page_url.lower().split(':', 1)[0] not in ('http', 'https')

        # Wait for page stability using browser profile settings (main branch pattern)
        if not not_a_meaningful_website:
            self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: â³ Waiting for page stability...')
            try:
                # Apply minimum wait time first (let page settle)
                min_wait = self.browser_profile.minimum_wait_page_load_time
                if min_wait > 0:
                    self.logger.debug(f'â³ Minimum wait: {min_wait}s')
                    await asyncio.sleep(min_wait)

                # Apply network idle wait time (for dynamic content like iframes)
                network_idle_wait = self.browser_profile.wait_for_network_idle_page_load_time
                if network_idle_wait > 0:
                    self.logger.debug(f'â³ Network idle wait: {network_idle_wait}s')
                    await asyncio.sleep(network_idle_wait)

                elapsed = time.time() - start_time
                self.logger.debug(f'âœ… Page stability wait completed in {elapsed:.2f}s')
            except Exception as e:
                self.logger.warning(
                    f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Network waiting failed: {e}, continuing anyway...'
                )

    async def take_screenshot(self, target_id: Optional[str] = None, format: str = 'png') -> str:
        """
        Concurrent screenshot method that bypasses serial bottlenecks in ScreenshotWatchdog.
        
        This method performs direct CDP calls for maximum concurrency.
        """
        if target_id is None:
            if not self.agent_focus:
                self.logger.warning('No page focus to get html, please specify a target id.')
                return ''
            target_id = self.agent_focus.target_id
        cdp_session = await self.get_or_create_cdp_session(target_id, focus=False)
        await self._wait_for_stable_network()

        try:
            ready_state = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={'expression': 'document.readyState'}, session_id=cdp_session.session_id
            )
        except Exception:
            pass

        try:
            from cdp_use.cdp.page import CaptureScreenshotParameters
            # Direct CDP screenshot - bypasses all event system overhead
            params = CaptureScreenshotParameters(format=format, captureBeyondViewport=False, quality=90)
            result = await cdp_session.cdp_client.send.Page.captureScreenshot(
                params=params,
                session_id=cdp_session.session_id,
            )
            return result['data']

        except Exception as e:
            self.logger.error(f'Concurrent screenshot failed: {type(e).__name__}: {e}')
            raise

    async def get_html_content(self, target_id: Optional[str] = None) -> str:
        """
        Get html content of current page
        :return:
        """
        if target_id is None:
            if not self.agent_focus:
                self.logger.warning('No page focus to get html, please specify a target id.')
                return ''
            target_id = self.agent_focus.target_id
        cdp_session = await self.get_or_create_cdp_session(target_id, focus=True)
        await self._wait_for_stable_network()

        try:
            ready_state = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={'expression': 'document.readyState'}, session_id=cdp_session.session_id
            )
        except Exception:
            await self._wait_for_stable_network()

        try:
            # Get the HTML content
            body_id = await cdp_session.cdp_client.send.DOM.getDocument(session_id=cdp_session.session_id)
            page_html_result = await cdp_session.cdp_client.send.DOM.getOuterHTML(
                params={'backendNodeId': body_id['root']['backendNodeId']}, session_id=cdp_session.session_id
            )
        except Exception as e:
            raise RuntimeError(f"Couldn't extract page content: {e}")

        page_html = page_html_result['outerHTML']
        return page_html

    async def get_browser_state_summary(
            self,
            cache_clickable_elements_hashes: bool = True,
            include_screenshot: bool = True,
            cached: bool = False,
            include_recent_events: bool = False,
    ) -> BrowserStateSummary:
        if cached and self._cached_browser_state_summary is not None and self._cached_browser_state_summary.dom_state:
            # Don't use cached state if it has 0 interactive elements
            selector_map = self._cached_browser_state_summary.dom_state.selector_map

            # Don't use cached state if we need a screenshot but the cached state doesn't have one
            if include_screenshot and not self._cached_browser_state_summary.screenshot:
                self.logger.debug('âš ï¸ Cached browser state has no screenshot, fetching fresh state with screenshot')
            # Fall through to fetch fresh state with screenshot
            elif selector_map and len(selector_map) > 0:
                self.logger.debug('ğŸ”„ Using pre-cached browser state summary for open tab')
                return self._cached_browser_state_summary
            else:
                self.logger.debug('âš ï¸ Cached browser state has 0 interactive elements, fetching fresh state')
            # Fall through to fetch fresh state

        browser_state = await self._dom_watchdog.get_browser_state_no_event_bus(
            include_dom=True,
            include_screenshot=include_screenshot,
            cache_clickable_elements_hashes=cache_clickable_elements_hashes,
            include_recent_events=include_recent_events
        )
        return browser_state

    @observe_debug(ignore_input=True, ignore_output=True, name='get_tabs')
    async def get_tabs(self) -> list[TabInfo]:
        """Get information about all open tabs using CDP Target.getTargetInfo for speed."""
        tabs = []

        # Safety check - return empty list if browser not connected yet
        if not self._cdp_client_root:
            return tabs

        # Get all page targets using CDP
        pages = await self._cdp_get_all_pages()

        for i, page_target in enumerate(pages):
            target_id = page_target['targetId']
            url = page_target['url']

            # Try to get the title directly from Target.getTargetInfo - much faster!
            # The initial getTargets() doesn't include title, but getTargetInfo does
            try:
                target_info = await self.cdp_client.send.Target.getTargetInfo(params={'targetId': target_id})
                # The title is directly available in targetInfo
                title = target_info.get('targetInfo', {}).get('title', '')

                # Skip JS execution for chrome:// pages and new tab pages
                if not title:
                    # For chrome:// pages without a title, use the URL itself
                    title = url

                # Special handling for PDF pages without titles
                if (not title or title == '') and (url.endswith('.pdf') or 'pdf' in url):
                    # PDF pages might not have a title, use URL filename
                    try:
                        from urllib.parse import urlparse

                        filename = urlparse(url).path.split('/')[-1]
                        if filename:
                            title = filename
                    except Exception:
                        pass

            except Exception as e:
                # Fallback to basic title handling
                self.logger.debug(
                    f'âš ï¸ Failed to get target info for tab #{i}: {_log_pretty_url(url)} - {type(e).__name__}')
                title = ''

            tab_info = TabInfo(
                target_id=target_id,
                url=url,
                title=title,
                parent_target_id=None,
            )
            tabs.append(tab_info)

        return tabs

    async def refresh_page(self):
        cdp_session = await self.browser_session.get_or_create_cdp_session()
        try:
            # Reload the target
            await cdp_session.cdp_client.send.Page.reload(session_id=cdp_session.session_id)

            # Wait for reload
            await asyncio.sleep(1.0)

            self.logger.info('ğŸ”„ Target refreshed')
        except Exception as e:
            raise



================================================
FILE: vibe_surf/browser/browser_manager.py
================================================
from __future__ import annotations

import asyncio
import logging
import pdb
import threading
from typing import Dict, List, Optional, Set, TYPE_CHECKING

from browser_use.browser.session import CDPClient
from browser_use.browser.views import TabInfo
from cdp_use.cdp.target.types import TargetInfo
from bubus import EventBus

from vibe_surf.browser.agent_browser_session import AgentBrowserSession

if TYPE_CHECKING:
    from browser_use.browser.session import BrowserSession

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class BrowserManager:
    """Manages isolated browser sessions for multiple agents with enhanced security."""

    def __init__(self, main_browser_session: BrowserSession):
        self.main_browser_session = main_browser_session

        # Store a list of sessions for each agent
        self._agent_sessions: Dict[str, AgentBrowserSession] = {}

    @property
    def _root_cdp_client(self) -> Optional[CDPClient]:
        """Get the root CDP client from the shared browser session."""
        return getattr(self.main_browser_session, '_cdp_client_root', None)

    async def _get_root_cdp_client(self) -> CDPClient:
        """Get the shared root CDP client from browser session."""
        if self._root_cdp_client is None:
            # Ensure the browser session is connected
            if not hasattr(self.main_browser_session,
                           '_cdp_client_root') or self.main_browser_session._cdp_client_root is None:
                await self.main_browser_session.connect()
        return self._root_cdp_client

    async def register_agent(
            self, agent_id: str, target_id: Optional[str] = None
    ) -> AgentBrowserSession:
        """
        Register an agent and return its primary isolated browser session.
        An agent can only be registered once.
        """
        if agent_id in self._agent_sessions:
            logger.info(f"Agent {agent_id} is already registered.")
            agent_session = self._agent_sessions[agent_id]
        else:
            agent_session = AgentBrowserSession(
                id=agent_id,
                cdp_url=self.main_browser_session.cdp_url,
                browser_profile=self.main_browser_session.browser_profile,
                main_browser_session=self.main_browser_session,
            )
            agent_session._cdp_client_root = await self._get_root_cdp_client()
            logger.info(f"ğŸš€ Starting agent session for {agent_id} to initialize watchdogs...")
            await agent_session.start()

            self._agent_sessions[agent_id] = agent_session
        await self.assign_target_to_agent(agent_id, target_id)
        return agent_session

    async def assign_target_to_agent(
            self, agent_id: str, target_id: Optional[str] = None
    ) -> bool:
        """Assign a target to an agent, creating a new session for it with security validation."""
        # Validate agent exists
        if agent_id not in self._agent_sessions:
            logger.warning(f"Agent '{agent_id}' is not registered.")
            return False

        agent_session = self._agent_sessions[agent_id]

        # Validate target assignment
        if target_id:
            try:
                target_id = await self.main_browser_session.get_target_id_from_tab_id(target_id)
            except Exception:
                logger.warning(f"Target ID '{target_id}' not found.")
                target_id = None
            if target_id:
                target_id_owner = self.get_target_owner(target_id)
                if target_id_owner and target_id_owner != agent_id:
                    logger.warning(
                        f"Target id: {target_id} belongs to {target_id_owner}. You cannot assign it to {target_id_owner}.")
                    return False

        # Get or create available target
        if target_id is None:
            new_target = await self.main_browser_session.cdp_client.send.Target.createTarget(
                params={'url': 'about:blank'})
            target_id = new_target["targetId"]

        await agent_session.connect_agent(target_id=target_id)
        return True

    async def unassign_target(self, target_id: str) -> bool:
        """Assign a target to an agent, creating a new session for it with security validation."""
        if not target_id:
            logger.warning(f"Please provide valid target id: {target_id}")
            return False
        target_id_owner = self.get_target_owner(target_id)
        if target_id_owner is None:
            logger.warning(f"Target id: {target_id} does not belong to any agent.")
            return False
        agent_session = self._agent_sessions[target_id_owner]
        target_cdp_session = agent_session.get_cdp_session_pool().pop(target_id, None)
        if target_cdp_session is not None:
            target_cdp_session.disconnect()
        return True

    async def unregister_agent(self, agent_id: str, close_tabs: bool = False):
        """Clean up all resources for an agent with enhanced security cleanup."""
        if agent_id not in self._agent_sessions:
            logger.warning(f"Agent '{agent_id}' is not registered.")
            return

        agent_session = self._agent_sessions.pop(agent_id, None)
        root_client = self.main_browser_session.cdp_client
        if close_tabs:
            for target_id in agent_session.get_cdp_session_pool():
                try:
                    logger.info(f"Close target id: {target_id}")
                    await root_client.send.Target.closeTarget(params={'targetId': target_id})
                except Exception as e:
                    # Log error if closing tab fails, but continue cleanup
                    logger.warning(f"Error closing target {target_id}: {e}")

        # Disconnect the agent's CDP session regardless
        await agent_session.disconnect_agent()
        await agent_session.stop()

    def get_agent_sessions(self, agent_id: str) -> Optional[AgentBrowserSession]:
        """Get all sessions (pages) for an agent."""
        return self._agent_sessions.get(agent_id, None)

    def get_active_agents(self) -> List[str]:
        """List all active agent IDs."""
        return list(self._agent_sessions.keys())

    def get_agent_target_ids(self, agent_id: str) -> List[str]:
        """Get all target IDs assigned to a specific agent."""
        agent_session = self.get_agent_sessions(agent_id)
        if agent_session is None:
            return []
        else:
            return list(agent_session.get_cdp_session_pool().keys())

    def get_target_owner(self, target_id: str) -> Optional[str]:
        """Get the agent ID that owns a specific target."""
        for agent_id in self._agent_sessions:
            agent_target_ids = self.get_agent_target_ids(agent_id)
            if target_id in agent_target_ids:
                return agent_id
        return None

    async def close(self) -> None:
        """Close all agent sessions but preserve the shared browser session."""
        # Unregister all agents first
        agent_ids = list(self._agent_sessions.keys())
        for agent_id in agent_ids:
            try:
                await self.unregister_agent(agent_id, True)
                await asyncio.sleep(1)
            except Exception as e:
                logger.warning(f"Error during agent {agent_id} cleanup: {e}")

    async def __aenter__(self) -> "BrowserManager":
        """Async context manager entry."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit."""
        await self.close()

    async def check_browser_connected(self):
        import aiohttp

        if not self.main_browser_session:
            logger.info("No Main browser session available.")
            return False

        for _ in range(5):
            try:
                targets = await self.main_browser_session.cdp_client.send.Target.getTargets()
                await asyncio.sleep(1)
                return len(targets) > 0
            except Exception as e:
                logger.error(f"Connect failed: {e}")
        return False

    async def _get_active_target(self) -> str:
        """Get current focused target, or an available target, or create a new one."""
        tab_infos = await self.get_all_tabs()
        # 1. Check for a focused page among ALL pages (not just unassigned)
        for tab_info in tab_infos:
            target_id = tab_info.target_id
            try:
                simple_check = """
                            ({
                                hasFocus: document.hasFocus(),
                                isVisible: document.visibilityState === 'visible',
                                notHidden: !document.hidden
                            })
                            """
                cdb_session = await self.main_browser_session.get_or_create_cdp_session(target_id, focus=False,
                                                                                        new_socket=None)
                eval_result = await cdb_session.cdp_client.send.Runtime.evaluate(
                    params={
                        "expression": simple_check,
                        "returnByValue": True
                    },
                    session_id=cdb_session.session_id
                )
                if "result" in eval_result and "value" in eval_result["result"]:
                    data = eval_result["result"]["value"]
                    is_visible = data.get("isVisible", False)
                    not_hidden = data.get("notHidden", False)
                    if is_visible and not_hidden:
                        return target_id
            except Exception as e:
                logger.warning(f"Get active target {e}")
                continue  # Skip invalid targets

        # 2. If no pages are available, create a new one
        if tab_infos:
            target_id = tab_infos[0].target_id
        else:
            target_id = await self.main_browser_session.navigate_to_url(url="about:blank", new_tab=True)
        return target_id

    async def get_activate_tab(self) -> Optional[TabInfo]:
        """Get tab information for the currently active target."""
        try:
            # Get the active target ID
            active_target_id = await self._get_active_target()
            if active_target_id is None:
                return None
            # Get target information from CDP
            tab_infos = await self.get_all_tabs()

            # Find the active target in the targets list
            for tab_info in tab_infos:
                if tab_info.target_id == active_target_id:
                    await self.main_browser_session.get_or_create_cdp_session(active_target_id, focus=True)
                    # Create TabInfo object
                    return TabInfo(
                        url=tab_info.url,
                        title=tab_info.title,
                        target_id=active_target_id
                    )
            return None
        except Exception:
            return None

    async def get_all_tabs(self) -> list[TabInfo]:
        tabs = await self.main_browser_session.get_tabs()
        return tabs



================================================
FILE: vibe_surf/browser/utils.py
================================================
import base64
import io
import logging
from typing import Optional, Tuple

from PIL import Image, ImageDraw, ImageFont

from browser_use.dom.views import DOMSelectorMap
from browser_use.observability import observe_debug

import math
import base64
import os
import logging
import binascii  # Import specifically for the error type
import pdb

from PIL import Image, ImageDraw, ImageFont
import random
import colorsys
import numpy as np
from typing import Optional, Tuple, List, Any
import io

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


# List of common font file names (Prioritize preferred ones first)
# Consider adding fonts known for broad Unicode coverage early (like Noto)
COMMON_FONT_FILES = [
    "simsun.ttc",
    "seguisb.ttf",  # Segoe UI Semibold (Good UI Font on Windows)
    "arial.ttf",  # Arial (Very common, good compatibility)
    "verdana.ttf",  # Verdana (Good readability)
    "tahoma.ttf",  # Tahoma (Common Windows UI font)
    "calibri.ttf",  # Calibri (Modern default in Office)
    "NotoSans-Regular.ttf",  # Noto Sans Regular (Broad Unicode, often default name)
    "NotoSansCJK-Regular.otf",  # Google Noto Fonts (covers CJK) - OpenType
    "DejaVuSans.ttf",  # Common Linux font (Good coverage)
    "ubuntu-regular.ttf",  # Ubuntu font (Common on Ubuntu Linux)
    " liberation-sans.ttf",  # Liberation Sans (Common Linux alternative to Arial)
    "msyh.ttc", "msyh.ttf",  # Microsoft YaHei (Chinese - Simplified) - TTC or TTF
    "simhei.ttf",  # SimHei (Chinese - Simplified - often present)
    "wqy-zenhei.ttc",  # WenQuanYi Zen Hei (Linux Chinese) - TTC
    "wqy-microhei.ttc",  # WenQuanYi Micro Hei (Linux Chinese) - TTC
    # Add Japanese, Korean, etc. specific fonts if needed
    "msgothic.ttc",  # MS Gothic (Japanese - older Windows) - TTC
    "malgun.ttf",  # Malgun Gothic (Korean - Windows)
    "gulim.ttc",  # Gulim (Korean - older Windows) - TTC
    "AppleGothic.ttf",  # Apple Gothic (macOS Korean)
    "ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ ProN W3.otf",  # Hiragino Kaku Gothic ProN (macOS Japanese) - Use actual name if known
    "songti.ttf",  # Songti (Less common nowadays)
]

# --- Font Directory Discovery ---

FONT_DIRS = []
if os.name == 'nt':  # Windows
    system_root = os.environ.get('SYSTEMROOT', 'C:\\Windows')
    FONT_DIRS.append(os.path.join(system_root, 'Fonts'))
    # User-installed fonts
    local_app_data = os.environ.get('LOCALAPPDATA')
    if local_app_data:
        FONT_DIRS.append(os.path.join(local_app_data, 'Microsoft\\Windows\\Fonts'))
elif os.name == 'posix':
    # Common system-wide locations (Linux, macOS)
    posix_system_dirs = [
        '/usr/share/fonts',
        '/usr/local/share/fonts',
        '/Library/Fonts',  # macOS system
        '/System/Library/Fonts',  # macOS system (usually contains essential fonts)
    ]
    # User-specific locations (Linux, macOS)
    posix_user_dirs = [
        os.path.expanduser('~/.fonts'),
        os.path.expanduser('~/.local/share/fonts'),
        os.path.expanduser('~/Library/Fonts'),  # macOS user
    ]

    # Add existing system directories
    for d in posix_system_dirs:
        if os.path.isdir(d):
            FONT_DIRS.append(d)
            # Also check common subdirectories like truetype, opentype etc.
            for subdir_type in ['truetype', 'opentype', 'TTF', 'OTF', 'type1', 'bitmap']:
                potential_subdir = os.path.join(d, subdir_type)
                if os.path.isdir(potential_subdir):
                    FONT_DIRS.append(potential_subdir)

    # Add existing user directories
    for d in posix_user_dirs:
        if os.path.isdir(d):
            FONT_DIRS.append(d)

# Remove duplicates and ensure directories exist (defensive check)
_unique_dirs = []
for d in FONT_DIRS:
    if d not in _unique_dirs and os.path.isdir(d):  # Check existence again just in case
        _unique_dirs.append(d)
FONT_DIRS = _unique_dirs
# print(f"Searching for fonts in: {FONT_DIRS}") # Optional: for debugging


# --- Caching ---

# Cache found font paths (case-insensitive name -> actual path or None)
_font_path_cache = {}
# Cache loaded font objects ((actual_path, size) -> font_object)
_loaded_font_cache = {}


# --- Core Functions ---

def find_font_path(font_name):
    """
    Tries to find the full path for a given font file name, case-insensitively.
    Uses a cache to store results of previous searches.
    """
    search_name_lower = font_name.lower()

    # 1. Check cache first
    if search_name_lower in _font_path_cache:
        return _font_path_cache[search_name_lower]  # Return cached path or None

    # 2. Search in font directories
    for font_dir in FONT_DIRS:
        try:
            # Use os.scandir for potentially better performance than os.listdir+os.path.isfile
            # It yields DirEntry objects with useful attributes/methods.
            # We still need os.walk for subdirectories. Let's stick to os.walk for simplicity
            # unless performance on flat directories becomes a major issue.
            for root, _, files in os.walk(font_dir, topdown=True):  # topdown=True might find it faster if shallow
                for file in files:
                    if file.lower() == search_name_lower:
                        found_path = os.path.join(root, file)
                        # Verify it's actually a file (os.walk should only list files, but belts and suspenders)
                        if os.path.isfile(found_path):
                            # Cache the successful result (using lowercase name as key)
                            _font_path_cache[search_name_lower] = found_path
                            # print(f"DEBUG: Found '{font_name}' at '{found_path}'") # Optional debug
                            return found_path
        except OSError as e:
            # logger.debug(f"Permission error or issue accessing {font_dir}: {e}") # Optional debug
            continue  # Ignore inaccessible directories or subdirectories

    # 3. If not found after searching all directories, cache the failure (None)
    # logger.debug(f"DEBUG: Could not find font '{font_name}' in any search directory.") # Optional debug
    _font_path_cache[search_name_lower] = None
    return None


def get_font(font_size):
    """
    Loads a preferred font from the COMMON_FONT_FILES list at the specified size.
    Performs case-insensitive search and caches loaded fonts for efficiency.
    Falls back to Pillow's default font if none of the preferred fonts are found/loadable.
    """
    global _loaded_font_cache  # Allow modification of the global cache

    # 1. Iterate through the preferred font list
    for font_name in COMMON_FONT_FILES:
        font_path = find_font_path(font_name)  # Uses the case-insensitive search + path cache

        if font_path:
            # 2. Check loaded font cache ((path, size) -> font object)
            cache_key = (font_path, font_size)
            if cache_key in _loaded_font_cache:
                # print(f"DEBUG: Cache hit for {font_path} size {font_size}") # Optional debug
                return _loaded_font_cache[cache_key]

            # 3. Try to load the font if found and not in cache
            try:
                font = ImageFont.truetype(font_path, font_size)
                _loaded_font_cache[cache_key] = font  # Cache the loaded font object
                # logger.info(f"Loaded font: {font_path} at size {font_size}") # Info level might be too verbose
                # print(f"Loaded font: {font_path} at size {font_size}") # Use print for simple feedback
                return font
            except IOError as e:
                logger.warning(
                    f"Could not load font file '{font_path}' (found for '{font_name}') at size {font_size}. Reason: {e}")
            except Exception as e:
                logger.error(f"Unexpected error loading font {font_name} ({font_path}): {e}")
                # Continue to next font
                continue

    # 4. Fallback to Pillow's default font if loop completes without success
    # Use a specific key for the default font in the cache
    default_cache_key = ("_pillow_default_", font_size)  # Pillow's default doesn't really resize well
    if default_cache_key in _loaded_font_cache:
        return _loaded_font_cache[default_cache_key]

    try:
        logger.warning(
            f"No suitable font found from preferred list in system paths. Using Pillow's default font (size {font_size} requested, but default font may not scale).")
        # Note: Default font might not support all characters or sizing well.
        font = ImageFont.load_default()
        _loaded_font_cache[default_cache_key] = font  # Cache the default font object
        return font
    except IOError as e:
        logger.critical(
            f"CRITICAL ERROR: Could not load any preferred fonts AND failed to load Pillow's default font! Reason: {e}")
        return None


def check_overlap(box1: Tuple[float, float, float, float],
                  box2: Tuple[float, float, float, float]) -> bool:
    """Checks if two rectangular boxes overlap. (Logic unchanged)"""
    l1, t1, r1, b1 = box1
    l2, t2, r2, b2 = box2
    # Check for non-overlap conditions (original logic)
    if r1 <= l2 or r2 <= l1 or b1 <= t2 or b2 <= t1:
        return False
    # Otherwise, they overlap
    return True


def generate_distinct_colors(n):
    """
    Generates n visually distinct colors in RGB format using HSV color space.
    Reorders the generated list deterministically by interleaving even-indexed
    colors with reverse-ordered odd-indexed colors to improve adjacent contrast.
    Example: [0, 1, 2, 3, 4, 5] -> [0, 5, 2, 3, 4, 1]

    Args:
        n: The number of distinct colors to generate.

    Returns:
        A list of n tuples, where each tuple represents an RGB color (int 0-255).
        Returns an empty list if n <= 0.
    """
    if n <= 0:
        return []

    # --- Step 1: Generate colors based on Hue in HSV space ---
    initial_colors = []
    for i in range(n):
        hue = i / n
        # Use high saturation and value for bright colors (parameters from original code)
        saturation = 0.7
        value = 0.8
        rgb_float = colorsys.hsv_to_rgb(hue, saturation, value)
        rgb_int = tuple(int(c * 255) for c in rgb_float)
        initial_colors.append(rgb_int)

    # Handle cases with 0 or 1 color where reordering is not needed/possible
    if n <= 1:
        return initial_colors

    # --- Step 2: Separate into even and odd indexed lists ---
    # Colors originally at even indices (0, 2, 4, ...)
    even_indexed_colors = initial_colors[::2]
    # Colors originally at odd indices (1, 3, 5, ...)
    odd_indexed_colors = initial_colors[1::2]

    # --- Step 3: Reverse the odd indexed list ---
    odd_indexed_colors.reverse()  # Reverse in-place is efficient here

    # --- Step 4: Interleave the lists ---
    reordered_colors = []
    len_odds = len(odd_indexed_colors)
    len_evens = len(even_indexed_colors)

    # Iterate up to the length of the shorter list (which is odd_indexed_colors)
    for i in range(len_odds):
        reordered_colors.append(even_indexed_colors[i])
        reordered_colors.append(odd_indexed_colors[i])

    # --- Step 5: Add any remaining element from the longer list ---
    # If n is odd, the even_indexed_colors list will have one more element
    if len_evens > len_odds:
        reordered_colors.append(even_indexed_colors[-1])  # Append the last even element
    random.shuffle(reordered_colors)
    return reordered_colors

def calculate_label_placement(
        corner: str,
        outline_box: Tuple[float, float, float, float],
        text_width: float,  # Original name: width of the background box
        text_height: float,  # Original name: height of the background box
        box_width: float,  # Original name: width of the element's outline box
        box_height: float,  # Original name: height of the element's outline box
        img_width: int,
        img_height: int
) -> Tuple[Optional[Tuple[float, float, float, float]], Optional[Tuple[float, float]]]:
    """
    Calculates the potential background box and text position reference for a label.
    (Logic and parameters identical to original class method).

    Returns:
        A tuple containing:
        - The calculated background box (l, t, r, b) clamped to image bounds, or None if invalid.
        - The calculated reference position (x, y) (top-left of background), or None if invalid.
    """
    l_outline, t_outline, r_outline, b_outline = outline_box

    # Determine if text should ideally be placed outside (Original Logic)
    move_text_outside = (text_height >= (box_height * 0.5) or text_width >= (
            box_width * 0.5)) and box_height > 0 and box_width > 0

    bg_left, bg_top, bg_right, bg_bottom = 0.0, 0.0, 0.0, 0.0
    # Text offset calculation is handled by the caller based on the returned reference point
    # text_x_offset, text_y_offset = 0, 0 # Original logic didn't use these this way

    # --- Calculate base positions based on corner (Original Logic) ---
    if corner == 'top_right':
        if move_text_outside:  # Outside Top-Right
            bg_left = r_outline
            bg_top = t_outline - text_height
        else:  # Inside Top-Right
            bg_left = r_outline - text_width
            bg_top = t_outline
        bg_right = bg_left + text_width
        bg_bottom = bg_top + text_height

    elif corner == 'bottom_right':
        if move_text_outside:  # Outside Bottom-Right
            bg_left = r_outline
            bg_top = b_outline
        else:  # Inside Bottom-Right
            bg_left = r_outline - text_width
            bg_top = b_outline - text_height
        bg_right = bg_left + text_width
        bg_bottom = bg_top + text_height

    elif corner == 'bottom_left':
        if move_text_outside:  # Outside Bottom-Left
            bg_left = l_outline - text_width
            bg_top = b_outline
        else:  # Inside Bottom-Left
            bg_left = l_outline
            bg_top = b_outline - text_height
        bg_right = bg_left + text_width
        bg_bottom = bg_top + text_height

    elif corner == 'top_left':
        if move_text_outside:  # Outside Top-Left
            bg_left = l_outline - text_width
            bg_top = t_outline - text_height
        else:  # Inside Top-Left
            bg_left = l_outline
            bg_top = t_outline
        bg_right = bg_left + text_width
        bg_bottom = bg_top + text_height
    else:
        logger.error(f"Invalid corner specified: {corner}")
        return None, None

    # --- Clamp background box to IMAGE boundaries (Original Logic) ---
    final_bg_left = max(0.0, bg_left)
    final_bg_top = max(0.0, bg_top)
    final_bg_right = min(float(img_width), bg_right)
    final_bg_bottom = min(float(img_height), bg_bottom)

    # Check if clamping made the box invalid (Original Logic)
    if final_bg_right <= final_bg_left or final_bg_bottom <= final_bg_top:
        return None, None  # Indicate invalid placement

    # --- Calculate reference text position (Top-left of background box) ---
    # The actual draw position will be offset slightly by the caller using '+1, +1' per original code
    final_text_ref_x = final_bg_left
    final_text_ref_y = final_bg_top

    final_bg_box = (final_bg_left, final_bg_top, final_bg_right, final_bg_bottom)
    final_text_ref_pos = (final_text_ref_x, final_text_ref_y)

    return final_bg_box, final_text_ref_pos


def highlight_screenshot(screenshot_base64: str, elements: List[List[Any]]) -> str:
    """
    Draws highlighted bounding boxes with index numbers (avoiding label overlap)
    on a screenshot, using standalone functions. **Parameters and core logic
    are preserved exactly from the user's provided class-based version.**

    Args:
        screenshot_base64: The screenshot image encoded in base64.
        elements: A list where each item is another list:
                  [highlight_index: int, box_coords: List[float]]
                  Box coordinates are [x1, y1, x2, y2] relative to the screenshot.

    Returns:
        A base64 encoded string of the highlighted screenshot (PNG format),
        or the original base64 string if errors occur or no valid elements
        are provided.
    """
    if not elements:
        logger.warning("No elements provided to highlight.")
        return screenshot_base64

    # Filter elements based on the new list structure - basic validation
    valid_elements = []
    seen_indices = set()
    for i, element_item in enumerate(elements):
        if (isinstance(element_item, (list, tuple)) and len(element_item) >= 2 and
                isinstance(element_item[0], int) and  # Check index type
                isinstance(element_item[1], (list, tuple)) and len(element_item[1]) == 4):  # Check box structure
            try:
                # Validate box coords are numeric and index is unique
                box_coords = [float(c) for c in element_item[1]]
                highlight_index = element_item[0]
                if highlight_index in seen_indices:
                    logger.warning(
                        f"Skipping element at raw index {i} due to duplicate highlight_index: {highlight_index}")
                    continue

                # Check for non-negative index if required (original code didn't explicitly)
                if highlight_index < 0:
                    logger.warning(
                        f"Skipping element at raw index {i} due to negative highlight_index: {highlight_index}")
                    continue

                valid_elements.append([highlight_index, box_coords])  # Use validated coords
                seen_indices.add(highlight_index)
            except (ValueError, TypeError):
                logger.warning(f"Skipping element at raw index {i} due to invalid box coordinates: {element_item[1]}")
        else:
            logger.warning(
                f"Skipping element at raw index {i} due to invalid structure or types. Expected [int, [x1,y1,x2,y2]], got: {element_item}")

    if not valid_elements:
        logger.warning("No valid elements found after filtering.")
        return screenshot_base64

    # Sort elements by highlight_index (first item in inner list) - REQUIRED for consistent color
    # The conversion function already sorts, but doing it again handles direct list input.
    try:
        valid_elements.sort(key=lambda el: el[0])
    except Exception as e:
        logger.error(f"Error sorting elements: {e}. Proceeding unsorted (color assignment may be inconsistent).")

    # --- Image Loading ---
    try:
        image_data = base64.b64decode(screenshot_base64)
        image = Image.open(io.BytesIO(image_data)).convert("RGBA")
    except Exception as e:
        logger.error(f"Error decoding or opening image: {e}")
        return screenshot_base64

    img_width, img_height = image.size
    if img_width <= 0 or img_height <= 0:
        logger.error(f"Invalid image dimensions: {image.size}")
        return screenshot_base64

    # --- Setup Drawing ---
    fill_overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))
    draw_fill = ImageDraw.Draw(fill_overlay)

    num_elements = len(valid_elements)
    colors = generate_distinct_colors(num_elements)
    fill_alpha = int(0.3 * 255)  # ** PARAMETER FROM ORIGINAL CODE **

    # --- Pass 1: Draw semi-transparent fills (Logic unchanged) ---
    for i, element_item in enumerate(valid_elements):
        highlight_index = element_item[0]  # Now index 0
        box_coords = element_item[1]  # Now index 1
        rel_left, rel_top, rel_right, rel_bottom = box_coords

        rel_left = max(min(rel_left, img_width), 0)
        rel_right = max(min(rel_right, img_width), 0)
        rel_top = max(min(rel_top, img_height), 0)
        rel_bottom = max(min(rel_bottom, img_height), 0)

        # Validation and clipping (Logic unchanged)
        if rel_right <= rel_left or rel_bottom <= rel_top:
            logger.debug(
                f"Skipping fill for element index {highlight_index} due to invalid box dimensions: {box_coords}")
            continue
        # if rel_right <= 0 or rel_bottom <= 0 or rel_left >= img_width or rel_top >= img_height:
        #     logger.debug(
        #         f"Skipping fill for element index {highlight_index} as it's outside image bounds: {box_coords}")
        #     continue

        draw_box = (max(0.0, rel_left), max(0.0, rel_top),
                    min(float(img_width), rel_right), min(float(img_height), rel_bottom))

        color_rgb = colors[i % num_elements]  # Use 'i' from loop for color consistency
        fill_color = (*color_rgb, fill_alpha)

        try:
            if draw_box[2] > draw_box[0] and draw_box[3] > draw_box[1]:
                draw_fill.rectangle(draw_box, fill=fill_color)
        except Exception as draw_e:
            logger.error(f"Error drawing fill for element index {highlight_index}, Box: {draw_box}: {draw_e}")

    # --- Composite the fill overlay (Logic unchanged) ---
    try:
        image = Image.alpha_composite(image, fill_overlay)
    except ValueError as e:
        logger.error(f"Error during alpha compositing: {e}. Check image modes.")
        # Fallback: Continue drawing on the original image without the overlay
        # Note: Fills will not be semi-transparent in this fallback case.
        image = Image.open(io.BytesIO(image_data)).convert("RGBA")  # Re-load original
        logger.warning("Compositing failed. Drawing outlines/text on original image.")
        # Intentionally not re-drawing fills here to avoid opaque blocks

    draw_main = ImageDraw.Draw(image)

    # --- Pass 2: Draw outlines and text (Parameters and logic identical to original) ---
    placed_label_boxes: List[Tuple[float, float, float, float]] = []
    corners_to_try = ['top_right', 'bottom_right', 'bottom_left', 'top_left']  # ** PARAMETER FROM ORIGINAL CODE **

    for i, element_item in enumerate(valid_elements):
        highlight_index = element_item[0]
        box_coords = element_item[1]
        label = str(highlight_index)
        color_rgb = colors[i % num_elements]  # Use 'i' from loop for color
        outline_width = 2  # ** PARAMETER FROM ORIGINAL CODE **

        rel_left, rel_top, rel_right, rel_bottom = box_coords

        # Re-validate (Logic unchanged)
        if rel_right <= rel_left or rel_bottom <= rel_top: continue
        if rel_right <= 0 or rel_bottom <= 0 or rel_left >= img_width or rel_top >= img_height: continue

        draw_box_outline = (max(0.0, rel_left), max(0.0, rel_top),
                            min(float(img_width), rel_right),
                            min(float(img_height), rel_bottom))

        box_width = draw_box_outline[2] - draw_box_outline[0]
        box_height = draw_box_outline[3] - draw_box_outline[1]
        if box_width <= 0 or box_height <= 0: continue

        # --- Dynamic Font Size Calculation (Formula from original code) ---
        min_dim = min(box_width, box_height)
        font_size = max(25, min(35, int(min_dim * 0.4)))  # ** FORMULA FROM ORIGINAL CODE **
        font = get_font(font_size)

        if not font:
            logger.warning(f"Could not load font for index {highlight_index}, skipping text label.")
            try:  # Still draw outline
                if draw_box_outline[2] > draw_box_outline[0] and draw_box_outline[3] > draw_box_outline[1]:
                    draw_main.rectangle(draw_box_outline, outline=color_rgb, width=outline_width)
            except Exception as draw_e:
                logger.error(f"Error drawing outline for index {highlight_index} (no font): {draw_e}")
            continue

        # --- Estimate text bounding box size (Logic and padding from original code) ---
        try:
            text_bbox = draw_main.textbbox((0, 0), label, font=font, stroke_width=0, align="center", anchor='lt')
            text_render_width = text_bbox[2] - text_bbox[0]
            text_render_height = text_bbox[3] - text_bbox[1]
            # Padding calculation from original code
            render_width = min(text_render_width, text_render_width)  # Original code had this redundancy
            h_padding = render_width // 6  # ** FORMULA FROM ORIGINAL CODE **
            w_padding = render_width // 6  # ** FORMULA FROM ORIGINAL CODE **
            # Total background dimensions needed
            label_bg_width = text_render_width + w_padding
            label_bg_height = text_render_height + h_padding
        except AttributeError:  # Fallback logic from original code
            logger.debug("Using font.getsize fallback for text dimensions.")
            try:
                text_render_width, text_render_height = draw_main.textlength(label, font=font), font.size
            except AttributeError:
                text_render_width = len(label) * font_size * 0.6
                text_render_height = font_size
            # Padding calculation from original code (repeated)
            render_width = min(text_render_width, text_render_width)
            h_padding = render_width // 6  # ** FORMULA FROM ORIGINAL CODE **
            w_padding = render_width // 6  # ** FORMULA FROM ORIGINAL CODE **
            label_bg_width = text_render_width + w_padding
            label_bg_height = text_render_height + h_padding
        except Exception as tb_e:
            logger.error(f"Error calculating text size for index {highlight_index}: {tb_e}. Using estimate.")
            # Fallback estimate from original code
            label_bg_width = len(label) * font_size * 0.8
            label_bg_height = font_size * 1.5

        # --- Find Non-Overlapping Label Position (Logic unchanged) ---
        chosen_label_bg_box = None
        chosen_text_pos = None  # Final position for draw_main.text()
        found_non_overlapping_spot = False

        for corner_choice in corners_to_try:
            potential_bg_box, potential_text_ref_pos = calculate_label_placement(
                corner=corner_choice,
                outline_box=draw_box_outline,
                text_width=label_bg_width,  # Use calculated background width
                text_height=label_bg_height,  # Use calculated background height
                box_width=box_width,  # Use element box width
                box_height=box_height,  # Use element box height
                img_width=img_width,
                img_height=img_height
            )

            if potential_bg_box is None: continue

            if potential_bg_box[0] < 0 or potential_bg_box[1] < 0 or potential_bg_box[2] >= img_width or \
                    potential_bg_box[3] >= img_height:
                continue

            overlaps = any(check_overlap(potential_bg_box, placed_box) for placed_box in placed_label_boxes)

            if not overlaps:
                chosen_label_bg_box = potential_bg_box
                # Text position adjustment from original code
                chosen_text_pos = (
                    potential_text_ref_pos[0] + w_padding // 2,
                    potential_text_ref_pos[1] + h_padding // 2)  # ** OFFSET FROM ORIGINAL CODE **
                found_non_overlapping_spot = True
                break

        # --- Default if all corners overlap (Logic unchanged) ---
        if not found_non_overlapping_spot:
            # logger.debug(f"Could not avoid label overlap for index {highlight_index}. Defaulting to top-left.")
            chosen_label_bg_box, potential_text_ref_pos = calculate_label_placement(
                corner='top_left',  # Default corner from original code
                outline_box=draw_box_outline,
                text_width=label_bg_width,
                text_height=label_bg_height,
                box_width=box_width,
                box_height=box_height,
                img_width=img_width,
                img_height=img_height
            )
            if chosen_label_bg_box and potential_text_ref_pos:
                # Text position adjustment from original code
                chosen_text_pos = (
                    potential_text_ref_pos[0] + w_padding // 2,
                    potential_text_ref_pos[1] + h_padding // 2)  # ** OFFSET FROM ORIGINAL CODE **
            else:
                # logger.debug(f"Default top-left placement failed for index {highlight_index}. Skipping label.")
                chosen_label_bg_box = None
                chosen_text_pos = None

        # --- Draw Outline, Label Background, and Text (Logic unchanged) ---
        try:
            # 1. Draw Outline
            if draw_box_outline[2] > draw_box_outline[0] and draw_box_outline[3] > draw_box_outline[1]:
                draw_main.rectangle(draw_box_outline, outline=color_rgb, width=outline_width)

            # 2. Draw Label (if valid position found)
            if chosen_label_bg_box and chosen_text_pos:
                # Ensure background box is valid before drawing
                if chosen_label_bg_box[2] > chosen_label_bg_box[0] and chosen_label_bg_box[3] > chosen_label_bg_box[1]:
                    draw_main.rectangle(chosen_label_bg_box, fill=color_rgb)

                    # Check text position is within image bounds before drawing
                    if chosen_text_pos[0] < img_width and chosen_text_pos[1] < img_height:
                        # Text drawing call from original code
                        draw_main.text(chosen_text_pos, label, fill="white", font=font, stroke_width=0, align="center",
                                       anchor='lt')

                    # Add *after* successful drawing attempt (Logic unchanged)
                    placed_label_boxes.append(chosen_label_bg_box)
                else:
                    logger.warning(
                        f"Skipping label for index {highlight_index} due to invalid final background box: {chosen_label_bg_box}")

        except Exception as draw_e:
            logger.error(
                f"Error during final drawing for index {highlight_index}, Box: {draw_box_outline}, LabelBox: {chosen_label_bg_box}): {draw_e}")

    # --- Encode final image (Logic unchanged) ---
    try:
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        highlighted_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        return highlighted_base64
    except Exception as e:
        logger.error(f"Error encoding final image to base64: {e}")
        return screenshot_base64


@observe_debug(ignore_input=True, ignore_output=True, name='create_highlighted_screenshot')
def create_highlighted_screenshot(
        screenshot_b64: str,
        selector_map: DOMSelectorMap,
        device_pixel_ratio: float = 1.0,
        viewport_offset_x: int = 0,
        viewport_offset_y: int = 0,
) -> str:
    """Create a highlighted screenshot with bounding boxes around interactive elements.
    Args:
        screenshot_b64: Base64 encoded screenshot
        selector_map: Map of interactive elements with their positions
        device_pixel_ratio: Device pixel ratio for scaling coordinates
        viewport_offset_x: X offset for viewport positioning
        viewport_offset_y: Y offset for viewport positioning
    Returns:
        Base64 encoded highlighted screenshot
    """
    try:
        # Decode screenshot
        screenshot_data = base64.b64decode(screenshot_b64)
        image = Image.open(io.BytesIO(screenshot_data)).convert('RGBA')

        # Process each interactive element
        valid_elements = []
        for element_id, element in selector_map.items():
            try:
                # Use snapshot bounds (document coordinates) if available, otherwise absolute_position
                bounds = element.absolute_position

                # Scale coordinates from CSS pixels to device pixels for screenshot
                # The screenshot is captured at device pixel resolution, but coordinates are in CSS pixels
                x1 = int(bounds.x * device_pixel_ratio)
                y1 = int(bounds.y * device_pixel_ratio)
                x2 = int((bounds.x + bounds.width) * device_pixel_ratio)
                y2 = int((bounds.y + bounds.height) * device_pixel_ratio)

                # Ensure coordinates are within image bounds
                img_width, img_height = image.size
                x1 = max(0, min(x1, img_width))
                y1 = max(0, min(y1, img_height))
                x2 = max(x1, min(x2, img_width))
                y2 = max(y1, min(y2, img_height))

                # Skip if bounding box is too small or invalid
                if x2 - x1 < 2 or y2 - y1 < 2:
                    continue

                valid_elements.append([element_id, [x1, y1, x2, y2]])

            except Exception as e:
                logger.debug(f'Failed to draw highlight for element {element_id}: {e}')
                continue

        highlighted_b64 = highlight_screenshot(screenshot_b64, valid_elements)

        logger.debug(f'Successfully created highlighted screenshot with {len(selector_map)} elements')
        return highlighted_b64

    except Exception as e:
        logger.error(f'Failed to create highlighted screenshot: {e}')
        # Return original screenshot on error
        return screenshot_b64


async def get_viewport_info_from_cdp(cdp_session) -> Tuple[float, int, int]:
    """Get viewport information from CDP session.
    Returns:
        Tuple of (device_pixel_ratio, scroll_x, scroll_y)
    """
    try:
        # Get layout metrics which includes viewport info and device pixel ratio
        metrics = await cdp_session.cdp_client.send.Page.getLayoutMetrics(session_id=cdp_session.session_id)

        # Extract viewport information
        visual_viewport = metrics.get('visualViewport', {})
        css_visual_viewport = metrics.get('cssVisualViewport', {})
        css_layout_viewport = metrics.get('cssLayoutViewport', {})

        # Calculate device pixel ratio
        css_width = css_visual_viewport.get('clientWidth', css_layout_viewport.get('clientWidth', 1280.0))
        device_width = visual_viewport.get('clientWidth', css_width)
        device_pixel_ratio = device_width / css_width if css_width > 0 else 1.0

        # Get scroll position in CSS pixels
        scroll_x = int(css_visual_viewport.get('pageX', 0))
        scroll_y = int(css_visual_viewport.get('pageY', 0))

        return float(device_pixel_ratio), scroll_x, scroll_y

    except Exception as e:
        logger.debug(f'Failed to get viewport info from CDP: {e}')
        return 1.0, 0, 0


@observe_debug(ignore_input=True, ignore_output=True, name='create_highlighted_screenshot_async')
async def create_highlighted_screenshot_async(screenshot_b64: str, selector_map: DOMSelectorMap,
                                              cdp_session=None) -> str:
    """Async wrapper for creating highlighted screenshots.
    Args:
        screenshot_b64: Base64 encoded screenshot
        selector_map: Map of interactive elements
        cdp_session: CDP session for getting viewport info
    Returns:
        Base64 encoded highlighted screenshot
    """
    # Get viewport information if CDP session is available
    device_pixel_ratio = 1.0
    viewport_offset_x = 0
    viewport_offset_y = 0

    if cdp_session:
        try:
            device_pixel_ratio, viewport_offset_x, viewport_offset_y = await get_viewport_info_from_cdp(cdp_session)
        except Exception as e:
            logger.debug(f'Failed to get viewport info from CDP: {e}')

    # Create highlighted screenshot (run in thread pool if needed for performance)
    return create_highlighted_screenshot(screenshot_b64, selector_map, device_pixel_ratio, viewport_offset_x,
                                         viewport_offset_y)



================================================
FILE: vibe_surf/browser/watchdogs/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/browser/watchdogs/action_watchdog.py
================================================
import asyncio

from browser_use.browser.watchdogs.default_action_watchdog import DefaultActionWatchdog
from browser_use.browser.events import (
	ClickElementEvent,
	GetDropdownOptionsEvent,
	GoBackEvent,
	GoForwardEvent,
	RefreshEvent,
	ScrollEvent,
	ScrollToTextEvent,
	SelectDropdownOptionEvent,
	SendKeysEvent,
	TypeTextEvent,
	UploadFileEvent,
	WaitEvent,
)
from browser_use.browser.views import BrowserError, URLNotAllowedError
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.dom.service import EnhancedDOMTreeNode

class CustomActionWatchdog(DefaultActionWatchdog):
    async def on_ClickElementEvent(self, event: ClickElementEvent) -> None:
        """Handle click request with CDP."""
        try:
            # Check if session is alive before attempting any operations
            if not self.browser_session.agent_focus or not self.browser_session.agent_focus.target_id:
                error_msg = 'Cannot execute click: browser session is corrupted (target_id=None). Session may have crashed.'
                self.logger.error(f'âš ï¸ {error_msg}')
                raise BrowserError(error_msg)

            # Use the provided node
            element_node = event.node
            index_for_logging = element_node.element_index or 'unknown'
            starting_target_id = self.browser_session.agent_focus.target_id

            # Track initial number of tabs to detect new tab opening
            if hasattr(self.browser_session, "main_browser_session") and self.browser_session.main_browser_session:
                initial_target_ids = await self.browser_session.main_browser_session._cdp_get_all_pages()
            else:
                initial_target_ids = await self.browser_session._cdp_get_all_pages()

            # Check if element is a file input (should not be clicked)
            if self.browser_session.is_file_input(element_node):
                msg = f'Index {index_for_logging} - has an element which opens file upload dialog. To upload files please use a specific function to upload files'
                self.logger.info(msg)
                raise BrowserError(
                    message=msg,
                    long_term_memory=msg,
                )

            # Perform the actual click using internal implementation
            click_metadata = None
            click_metadata = await self._click_element_node_impl(element_node,
                                                                 while_holding_ctrl=event.while_holding_ctrl)
            download_path = None  # moved to downloads_watchdog.py

            # Build success message
            if download_path:
                msg = f'Downloaded file to {download_path}'
                self.logger.info(f'ğŸ’¾ {msg}')
            else:
                msg = f'Clicked button with index {index_for_logging}: {element_node.get_all_children_text(max_depth=2)}'
                self.logger.debug(f'ğŸ–±ï¸ {msg}')
            self.logger.debug(f'Element xpath: {element_node.xpath}')

            # Wait a bit for potential new tab to be created
            # This is necessary because tab creation is async and might not be immediate
            await asyncio.sleep(0.5)

            # Clear cached state after click action since DOM might have changed
            self.browser_session.agent_focus = await self.browser_session.get_or_create_cdp_session(
                target_id=starting_target_id, focus=True
            )

            # Check if a new tab was opened
            if hasattr(self.browser_session, "main_browser_session") and self.browser_session.main_browser_session:
                after_target_ids = await self.browser_session.main_browser_session._cdp_get_all_pages()
            else:
                after_target_ids = await self.browser_session._cdp_get_all_pages()
            new_target_ids = {t['targetId'] for t in after_target_ids} - {t['targetId'] for t in initial_target_ids}
            if new_target_ids:
                new_tab_msg = 'New tab opened - switching to it'
                msg += f' - {new_tab_msg}'
                self.logger.info(f'ğŸ”— {new_tab_msg}')
                new_target_id = new_target_ids.pop()
                if not event.while_holding_ctrl:
                    # if while_holding_ctrl=False it means agent was not expecting a new tab to be opened
                    # so we need to switch to the new tab to make the agent aware of the surprise new tab that was opened.
                    # when while_holding_ctrl=True we dont actually want to switch to it,
                    # we should match human expectations of ctrl+click which opens in the background,
                    # so in multi_act it usually already sends [click_element_by_index(123, while_holding_ctrl=True), switch_tab(tab_id=None)] anyway
                    from browser_use.browser.events import SwitchTabEvent

                    await self.browser_session.get_or_create_cdp_session(
                        target_id=new_target_id, focus=True
                    )
                else:
                    await self.browser_session.get_or_create_cdp_session(
                        target_id=new_target_id, focus=False
                    )

            return None
        except Exception as e:
            raise


================================================
FILE: vibe_surf/browser/watchdogs/dom_watchdog.py
================================================
"""DOM watchdog for browser DOM tree management using CDP."""

import asyncio
import pdb
import time
from typing import TYPE_CHECKING

from browser_use.browser.events import (
    BrowserErrorEvent,
    BrowserStateRequestEvent,
    ScreenshotEvent,
    TabCreatedEvent,
)
from browser_use.browser.watchdog_base import BaseWatchdog
from browser_use.browser.watchdogs.dom_watchdog import DOMWatchdog
from browser_use.dom.service import DomService
from browser_use.dom.views import (
    EnhancedDOMTreeNode,
    SerializedDOMState,
)

if TYPE_CHECKING:
    from browser_use.browser.views import BrowserStateSummary, PageInfo


class CustomDOMWatchdog(DOMWatchdog):

    async def get_browser_state_no_event_bus(self, include_dom: bool = True,
                                             include_screenshot: bool = True,
                                             cache_clickable_elements_hashes: bool = True,
                                             include_recent_events: bool = False) -> 'BrowserStateSummary':
        """Handle browser state request by coordinating DOM building and screenshot capture.

        This is the main entry point for getting the complete browser state.

        Args:
            event: The browser state request event with options

        Returns:
            Complete BrowserStateSummary with DOM, screenshot, and target info
        """
        from browser_use.browser.views import BrowserStateSummary, PageInfo

        self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: STARTING browser state request')
        page_url = await self.browser_session.get_current_page_url()
        self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got page URL: {page_url}')
        if self.browser_session.agent_focus:
            self.logger.debug(
                f'ğŸ“ Current page URL: {page_url}, target_id: {self.browser_session.agent_focus.target_id}, session_id: {self.browser_session.agent_focus.session_id}'
            )
        else:
            self.logger.debug(f'ğŸ“ Current page URL: {page_url}, no cdp_session attached')

        # check if we should skip DOM tree build for pointless pages
        not_a_meaningful_website = page_url.lower().split(':', 1)[0] not in ('http', 'https')

        # Wait for page stability using browser profile settings (main branch pattern)
        if not not_a_meaningful_website:
            self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: â³ Waiting for page stability...')
            try:
                await self._wait_for_stable_network()
                self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Page stability complete')
            except Exception as e:
                self.logger.warning(
                    f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Network waiting failed: {e}, continuing anyway...'
                )

        # Get tabs info once at the beginning for all paths
        self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting tabs info...')
        tabs_info = await self.browser_session.get_tabs()
        self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got {len(tabs_info)} tabs')
        self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Tabs info: {tabs_info}')

        try:
            # Execute DOM building and screenshot capture in parallel
            dom_task = None
            screenshot_task = None

            # Start DOM building task if requested
            if include_dom:
                self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸŒ³ Starting DOM tree build task...')

                previous_state = (
                    self.browser_session._cached_browser_state_summary.dom_state
                    if self.browser_session._cached_browser_state_summary
                    else None
                )

                dom_task = asyncio.create_task(self._build_dom_tree_without_highlights(previous_state))

            # Start clean screenshot task if requested (without JS highlights)
            if include_screenshot:
                self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Starting clean screenshot task...')
                screenshot_task = asyncio.create_task(self.browser_session.take_screenshot())

            # Wait for both tasks to complete
            content = None
            screenshot_b64 = None

            if dom_task:
                try:
                    content = await dom_task
                    self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… DOM tree build completed')
                except Exception as e:
                    self.logger.warning(
                        f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: DOM build failed: {e}, using minimal state')
                    content = SerializedDOMState(_root=None, selector_map={})
            else:
                content = SerializedDOMState(_root=None, selector_map={})

            if screenshot_task:
                try:
                    screenshot_b64 = await screenshot_task
                    self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Clean screenshot captured')
                except Exception as e:
                    self.logger.warning(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Clean screenshot failed: {e}')
                    screenshot_b64 = None

            # Apply Python-based highlighting if both DOM and screenshot are available
            if screenshot_b64 and content and content.selector_map and self.browser_session.browser_profile.highlight_elements:
                try:
                    self.logger.debug(
                        'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ¨ Applying Python-based highlighting...')
                    from vibe_surf.browser.utils import create_highlighted_screenshot_async

                    # Get CDP session for viewport info
                    cdp_session = await self.browser_session.get_or_create_cdp_session()

                    screenshot_b64 = await create_highlighted_screenshot_async(screenshot_b64, content.selector_map,
                                                                               cdp_session)
                    self.logger.debug(
                        f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… Applied highlights to {len(content.selector_map)} elements'
                    )
                except Exception as e:
                    self.logger.warning(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Python highlighting failed: {e}')

            # Ensure we have valid content
            if not content:
                content = SerializedDOMState(_root=None, selector_map={})

            # Tabs info already fetched at the beginning

            # Get target title safely
            try:
                self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting page title...')
                title = await asyncio.wait_for(self.browser_session.get_current_page_title(), timeout=2.0)
                self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got title: {title}')
            except Exception as e:
                self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Failed to get title: {e}')
                title = 'Page'

            # Get comprehensive page info from CDP
            try:
                self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Getting page info from CDP...')
                page_info = await self._get_page_info()
                self.logger.debug(f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Got page info from CDP: {page_info}')
            except Exception as e:
                self.logger.debug(
                    f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: Failed to get page info from CDP: {e}, using fallback'
                )
                # Fallback to default viewport dimensions
                viewport = self.browser_session.browser_profile.viewport or {'width': 1280, 'height': 720}
                page_info = PageInfo(
                    viewport_width=viewport['width'],
                    viewport_height=viewport['height'],
                    page_width=viewport['width'],
                    page_height=viewport['height'],
                    scroll_x=0,
                    scroll_y=0,
                    pixels_above=0,
                    pixels_below=0,
                    pixels_left=0,
                    pixels_right=0,
                )

            # Check for PDF viewer
            is_pdf_viewer = page_url.endswith('.pdf') or '/pdf/' in page_url

            # Build and cache the browser state summary
            if screenshot_b64:
                self.logger.debug(
                    f'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Creating BrowserStateSummary with screenshot, length: {len(screenshot_b64)}'
                )
            else:
                self.logger.debug(
                    'ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: ğŸ“¸ Creating BrowserStateSummary WITHOUT screenshot'
                )

            browser_state = BrowserStateSummary(
                dom_state=content,
                url=page_url,
                title=title,
                tabs=tabs_info,
                screenshot=screenshot_b64,
                page_info=page_info,
                pixels_above=0,
                pixels_below=0,
                browser_errors=[],
                is_pdf_viewer=is_pdf_viewer,
                recent_events=self._get_recent_events_str() if include_recent_events else None,
            )

            # Cache the state
            self.browser_session._cached_browser_state_summary = browser_state

            self.logger.debug('ğŸ” DOMWatchdog.on_BrowserStateRequestEvent: âœ… COMPLETED - Returning browser state')
            return browser_state

        except Exception as e:
            self.logger.error(f'Failed to get browser state: {e}')

            # Return minimal recovery state
            return BrowserStateSummary(
                dom_state=SerializedDOMState(_root=None, selector_map={}),
                url=page_url if 'page_url' in locals() else '',
                title='Error',
                tabs=[],
                screenshot=None,
                page_info=PageInfo(
                    viewport_width=1280,
                    viewport_height=720,
                    page_width=1280,
                    page_height=720,
                    scroll_x=0,
                    scroll_y=0,
                    pixels_above=0,
                    pixels_below=0,
                    pixels_left=0,
                    pixels_right=0,
                ),
                pixels_above=0,
                pixels_below=0,
                browser_errors=[str(e)],
                is_pdf_viewer=False,
                recent_events=None,
            )




================================================
FILE: vibe_surf/chrome_extension/background.js
================================================
// Background Script - VibeSurf Extension
// Handles extension lifecycle, side panel management, and cross-context communication

// Import configuration using importScripts for service worker
try {
  importScripts('config.js');
  console.log('[VibeSurf] Configuration loaded');
} catch (error) {
  console.error('[VibeSurf] Failed to load configuration:', error);
}

class VibeSurfBackground {
  constructor() {
    this.isInitialized = false;
    this.setupEventListeners();
    this.initDevMode();
  }

  initDevMode() {
    // Enable auto-reload in development mode
    try {
      const manifest = chrome.runtime.getManifest();
      const isDevelopment = !('update_url' in manifest);
      
      if (isDevelopment) {
        // Simple reload check every 3 seconds
        setInterval(() => {
          fetch(chrome.runtime.getURL('manifest.json'))
            .then(() => {
              // File accessible, extension is working
            })
            .catch(() => {
              // If we can't access our own files, extension might need reload
            });
        }, 3000);
      }
    } catch (error) {
      // Ignore errors in dev mode setup
    }
  }

  setupEventListeners() {
    // Extension installation and startup
    chrome.runtime.onInstalled.addListener(this.handleInstalled.bind(this));
    chrome.runtime.onStartup.addListener(this.handleStartup.bind(this));

    // Action button click (toolbar icon)
    chrome.action.onClicked.addListener(this.handleActionClick.bind(this));

    // Context menu setup (backup method)
    chrome.runtime.onInstalled.addListener(() => {
      chrome.contextMenus.create({
        id: 'open-vibesurf',
        title: 'Open VibeSurf Panel',
        contexts: ['action']
      });
    });
    
    chrome.contextMenus.onClicked.addListener((info, tab) => {
      if (info.menuItemId === 'open-vibesurf') {
        this.handleActionClick(tab);
      }
    });

    // Message handling between contexts
    chrome.runtime.onMessage.addListener(this.handleMessage.bind(this));

    // Tab updates for context awareness
    chrome.tabs.onActivated.addListener(this.handleTabActivated.bind(this));
    chrome.tabs.onUpdated.addListener(this.handleTabUpdated.bind(this));

  }

  async handleInstalled(details) {
    
    try {
      // Check Chrome version and API availability
      
      // Initialize default settings
      await this.initializeSettings();
      
      // Set default badge
      await chrome.action.setBadgeText({ text: '' });
      await chrome.action.setBadgeBackgroundColor({ color: '#007acc' });
      
      // Show welcome notification on fresh install
      if (details.reason === 'install') {
        await this.showWelcomeNotification();
      }
      
      this.isInitialized = true;
    } catch (error) {
      console.error('[VibeSurf] Initialization failed:', error);
    }
  }

  async handleStartup() {
    await this.initializeSettings();
    this.isInitialized = true;
  }

  async handleActionClick(tab) {
    
    try {
      // Check if sidePanel API is available
      if (chrome.sidePanel && chrome.sidePanel.open) {
        // Open side panel for the current tab
        await chrome.sidePanel.open({ tabId: tab.id });
        
        // Update badge to indicate active state
        await chrome.action.setBadgeText({ text: 'â—', tabId: tab.id });
        await chrome.action.setBadgeBackgroundColor({ color: '#007acc', tabId: tab.id });
        
      } else {
        
        // Use test panel first
        await chrome.tabs.create({
          url: chrome.runtime.getURL('test-panel.html'),
          index: tab.index + 1
        });
        
      }
      
      // Store current tab info for context
      await chrome.storage.local.set({
        currentTab: {
          id: tab.id,
          url: tab.url,
          title: tab.title,
          timestamp: Date.now()
        }
      });
      
    } catch (error) {
      console.error('[VibeSurf] Failed to open side panel:', error);
      
      // Show notification with helpful message
      try {
        await chrome.notifications.create({
          type: 'basic',
          iconUrl: '', // Use empty string to avoid icon issues
          title: 'VibeSurf',
          message: 'Side panel failed. Please update Chrome to the latest version or try right-clicking the extension icon.'
        });
      } catch (notifError) {
        console.warn('[VibeSurf] Notification failed:', notifError);
        // Don't throw, just log the warning
      }
      
      // Fallback: try to open in new tab
      try {
        await chrome.tabs.create({
          url: chrome.runtime.getURL('sidepanel.html'),
          index: tab.index + 1
        });
      } catch (fallbackError) {
        console.error('[VibeSurf] Fallback also failed:', fallbackError);
      }
    }
  }

  handleMessage(message, sender, sendResponse) {
    console.log('[VibeSurf] Received message:', message.type);
    
    // Handle async messages properly
    (async () => {
      try {
        let result;
        
        console.log('[VibeSurf] Processing message type:', message.type);
        switch (message.type) {
          case 'GET_CURRENT_TAB':
            result = await this.getCurrentTabInfo();
            break;
            
          case 'UPDATE_BADGE':
            result = await this.updateBadge(message.data);
            break;
            
          case 'SHOW_NOTIFICATION':
            result = await this.showNotification(message.data);
            break;
            
          case 'COPY_TO_CLIPBOARD':
            result = await this.copyToClipboard(message.text);
            break;
            
          case 'HEALTH_CHECK':
            result = { status: 'healthy', timestamp: Date.now() };
            break;
            
          case 'GET_BACKEND_STATUS':
            result = await this.checkBackendStatus(message.data?.backendUrl);
            break;
            
          case 'STORE_SESSION_DATA':
            result = await this.storeSessionData(message.data);
            break;
            
          case 'GET_SESSION_DATA':
            result = await this.getSessionData(message.data?.sessionId);
            break;
            
          case 'OPEN_FILE_URL':
            result = await this.openFileUrl(message.data?.fileUrl);
            break;
            
          case 'OPEN_FILE_SYSTEM':
            result = await this.openFileSystem(message.data?.filePath);
            break;
            
          case 'GET_ALL_TABS':
            result = await this.getAllTabs();
            break;
            
          case 'REQUEST_MICROPHONE_PERMISSION':
            result = await this.requestMicrophonePermission();
            break;
            
          case 'REQUEST_MICROPHONE_PERMISSION_WITH_UI':
            console.log('[VibeSurf] Handling REQUEST_MICROPHONE_PERMISSION_WITH_UI');
            result = await this.requestMicrophonePermissionWithUI();
            break;
            
          case 'MICROPHONE_PERMISSION_RESULT':
            console.log('[VibeSurf] Received MICROPHONE_PERMISSION_RESULT:', message);
            console.log('[VibeSurf] Permission granted:', message.granted);
            console.log('[VibeSurf] Permission error:', message.error);
            
            // Handle permission result from URL parameter approach
            if (message.granted !== undefined) {
              console.log('[VibeSurf] Processing permission result with granted:', message.granted);
              
              // Store the result for the original tab to retrieve
              chrome.storage.local.set({
                microphonePermissionResult: {
                  granted: message.granted,
                  error: message.error,
                  timestamp: Date.now()
                }
              });
              
              // Also send to any waiting listeners
              console.log('[VibeSurf] Broadcasting permission result to all tabs...');
              chrome.runtime.sendMessage({
                type: 'MICROPHONE_PERMISSION_RESULT',
                granted: message.granted,
                error: message.error
              }).then(() => {
                console.log('[VibeSurf] Permission result broadcast successful');
              }).catch((err) => {
                console.log('[VibeSurf] Permission result broadcast failed (no listeners):', err);
              });
            }
            result = { acknowledged: true };
            break;
            
          default:
            console.warn('[VibeSurf] Unknown message type:', message.type, 'Available handlers:', [
              'GET_CURRENT_TAB', 'UPDATE_BADGE', 'SHOW_NOTIFICATION', 'COPY_TO_CLIPBOARD',
              'HEALTH_CHECK', 'GET_BACKEND_STATUS', 'STORE_SESSION_DATA', 'GET_SESSION_DATA',
              'OPEN_FILE_URL', 'OPEN_FILE_SYSTEM', 'GET_ALL_TABS', 'REQUEST_MICROPHONE_PERMISSION',
              'REQUEST_MICROPHONE_PERMISSION_WITH_UI', 'MICROPHONE_PERMISSION_RESULT'
            ]);
            result = { error: 'Unknown message type', receivedType: message.type };
        }
        
        sendResponse(result);
        
      } catch (error) {
        console.error('[VibeSurf] Message handling error:', error);
        sendResponse({ error: error.message });
      }
    })();
    
    // Return true to indicate async response
    return true;
  }

  async handleTabActivated(activeInfo) {
    // Update current tab context when user switches tabs
    const tab = await chrome.tabs.get(activeInfo.tabId);
    
    await chrome.storage.local.set({
      currentTab: {
        id: tab.id,
        url: tab.url,
        title: tab.title,
        timestamp: Date.now()
      }
    });
  }

  async handleTabUpdated(tabId, changeInfo, tab) {
    // Update context when tab URL changes
    if (changeInfo.url) {
      const { currentTab } = await chrome.storage.local.get('currentTab');
      
      if (currentTab && currentTab.id === tabId) {
        await chrome.storage.local.set({
          currentTab: {
            ...currentTab,
            url: tab.url,
            title: tab.title,
            timestamp: Date.now()
          }
        });
      }
    }
  }

  async initializeSettings() {
    // Load configuration (use self instead of window in service worker)
    const config = self.VIBESURF_CONFIG || {};
    
    const defaultSettings = {
      backendUrl: config.BACKEND_URL || 'http://localhost:9335',
      defaultSessionPrefix: config.DEFAULT_SESSION_PREFIX || 'vibesurf_',
      notifications: config.NOTIFICATIONS || {
        enabled: true,
        taskComplete: true,
        taskError: true
      },
      ui: config.UI || {
        theme: 'auto',
        autoScroll: true,
        compactMode: false
      },
      debug: config.DEBUG || false
    };

    const { settings } = await chrome.storage.local.get('settings');
    
    if (!settings) {
      await chrome.storage.local.set({ settings: defaultSettings });
    } else {
      // Merge with defaults for any missing keys
      const mergedSettings = { ...defaultSettings, ...settings };
      await chrome.storage.local.set({ settings: mergedSettings });
    }
  }

  async getCurrentTabInfo() {
    try {
      const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
      return {
        id: tab.id,
        url: tab.url,
        title: tab.title,
        favIconUrl: tab.favIconUrl
      };
    } catch (error) {
      console.error('[VibeSurf] Failed to get current tab:', error);
      return null;
    }
  }

  async updateBadge(data) {
    const { text, color, tabId } = data;
    
    if (text !== undefined) {
      await chrome.action.setBadgeText({ text, tabId });
    }
    
    if (color) {
      await chrome.action.setBadgeBackgroundColor({ color, tabId });
    }
    
    return { success: true };
  }

  async showNotification(data) {
    const { title, message, type = 'info', iconUrl } = data;
    
    // Map custom types to valid Chrome notification types
    const validType = ['basic', 'image', 'list', 'progress'].includes(type) ? type : 'basic';
    
    // Simplified icon handling - try available icons without validation
    let finalIconUrl = '';
    
    // Try to use extension icons in order of preference, but don't validate with fetch
    const iconCandidates = [
      iconUrl ? chrome.runtime.getURL(iconUrl) : null,
      chrome.runtime.getURL('icons/icon48.png'),
      chrome.runtime.getURL('icons/logo.png')
    ].filter(Boolean);
    
    // Use the first candidate, or empty string as fallback
    finalIconUrl = iconCandidates[0] || '';
    
    try {
      const notificationId = await chrome.notifications.create({
        type: validType,
        iconUrl: finalIconUrl,
        title: title || 'VibeSurf',
        message
      });
      
      return { notificationId };
    } catch (error) {
      console.warn('[VibeSurf] Notification with icon failed, trying without icon:', error);
      // Try once more with empty icon URL
      try {
        const notificationId = await chrome.notifications.create({
          type: validType,
          iconUrl: '', // Empty string will use browser default
          title: title || 'VibeSurf',
          message
        });
        return { notificationId };
      } catch (fallbackError) {
        console.error('[VibeSurf] Fallback notification also failed:', fallbackError);
        throw new Error(`Failed to create notification: ${error.message}`);
      }
    }
  }

  async showWelcomeNotification() {
    try {
      await chrome.notifications.create({
        type: 'basic',
        iconUrl: '', // Use empty string to avoid icon issues
        title: 'Welcome to VibeSurf!',
        message: 'Click the VibeSurf icon in the toolbar to start automating your browsing tasks.'
      });
    } catch (error) {
      console.warn('[VibeSurf] Welcome notification failed:', error);
      // Don't throw, just log the warning
    }
  }

  async checkBackendStatus(backendUrl = null) {
    // Use configuration file value as default (use self instead of window in service worker)
    const config = self.VIBESURF_CONFIG || {};
    backendUrl = backendUrl || config.BACKEND_URL || 'http://localhost:9335';
    try {
      const response = await fetch(`${backendUrl}/health`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });
      
      if (response.ok) {
        const data = await response.json();
        return {
          status: 'connected',
          backend: data,
          timestamp: Date.now()
        };
      } else {
        return {
          status: 'error',
          error: `HTTP ${response.status}`,
          timestamp: Date.now()
        };
      }
    } catch (error) {
      console.error('[VibeSurf] Backend health check failed:', error);
      return {
        status: 'disconnected',
        error: error.message,
        timestamp: Date.now()
      };
    }
  }

  async storeSessionData(data) {
    const { sessionId, ...sessionData } = data;
    const key = `session_${sessionId}`;
    
    // Store session data
    await chrome.storage.local.set({
      [key]: {
        ...sessionData,
        lastUpdated: Date.now()
      }
    });
    
    // Update sessions list
    const { sessionsList = [] } = await chrome.storage.local.get('sessionsList');
    
    if (!sessionsList.includes(sessionId)) {
      sessionsList.unshift(sessionId); // Add to beginning
      
      // Keep only last 50 sessions
      if (sessionsList.length > 50) {
        const removedSessions = sessionsList.splice(50);
        
        // Clean up old session data
        const keysToRemove = removedSessions.map(id => `session_${id}`);
        await chrome.storage.local.remove(keysToRemove);
      }
      
      await chrome.storage.local.set({ sessionsList });
    }
    
    return { success: true };
  }

  async getSessionData(sessionId) {
    try {
      if (!sessionId) {
        // Return all sessions
        const { sessionsList = [] } = await chrome.storage.local.get('sessionsList');
        const sessionKeys = sessionsList.map(id => `session_${id}`);
        
        if (sessionKeys.length === 0) {
          console.log('[VibeSurf] No sessions found in storage');
          return { sessions: [] };
        }
        
        const sessionsData = await chrome.storage.local.get(sessionKeys);
        const sessions = sessionsList
          .map(id => {
            const data = sessionsData[`session_${id}`];
            if (data) {
              return {
                sessionId: id,
                ...data
              };
            }
            return null;
          })
          .filter(session => session !== null); // Remove null entries
        
        return { sessions };
      } else {
        // Return specific session
        const { [`session_${sessionId}`]: sessionData } = await chrome.storage.local.get(`session_${sessionId}`);
        
        if (sessionData) {
          return { sessionData };
        } else {
          return { sessionData: null, error: 'Session not found in storage' };
        }
      }
    } catch (error) {
      console.error('[VibeSurf] Error retrieving session data:', error);
      return { sessionData: null, error: error.message };
    }
  }

  async openFileUrl(fileUrl) {
    if (!fileUrl) {
      return { success: false, error: 'No file URL provided' };
    }

    // Add a unique request ID to track duplicate calls
    const requestId = Date.now() + Math.random();
    console.log(`[VibeSurf] openFileUrl called with ID: ${requestId}, URL: ${fileUrl}`);

    try {
      // Validate URL format before attempting to open
      try {
        new URL(fileUrl);
      } catch (urlError) {
        console.warn('[VibeSurf] Invalid URL format:', fileUrl, urlError);
        return { success: false, error: 'Invalid file URL format' };
      }
      
      // Check if this is an HTTP/HTTPS URL and handle it appropriately
      if (fileUrl.startsWith('http://') || fileUrl.startsWith('https://')) {
        console.log(`[VibeSurf] Detected HTTP(S) URL, creating tab for: ${fileUrl}`);
        
        // Try to create a new tab with the URL
        const tab = await chrome.tabs.create({
          url: fileUrl,
          active: true
        });
        
        if (tab && tab.id) {
          console.log(`[VibeSurf] Successfully opened HTTP(S) URL in tab: ${tab.id} (request: ${requestId})`);
          return { success: true, tabId: tab.id };
        } else {
          console.warn(`[VibeSurf] Tab creation returned but no tab ID for request: ${requestId}`);
          return { success: false, error: 'Failed to create tab - no tab ID returned' };
        }
      }
      
      // For file:// URLs, try the original approach
      console.log(`[VibeSurf] Attempting to open file URL: ${fileUrl} (request: ${requestId})`);
      
      // Try to create a new tab with the file URL
      const tab = await chrome.tabs.create({
        url: fileUrl,
        active: true
      });
      
      if (tab && tab.id) {
        console.log(`[VibeSurf] Successfully opened file in tab: ${tab.id} (request: ${requestId})`);
        return { success: true, tabId: tab.id };
      } else {
        console.warn(`[VibeSurf] Tab creation returned but no tab ID for request: ${requestId}`);
        return { success: false, error: 'Failed to create tab - no tab ID returned' };
      }
      
    } catch (error) {
      console.error(`[VibeSurf] Error opening file URL (request: ${requestId}):`, error);
      
      // Provide more specific error messages
      let errorMessage = error.message || 'Unknown error opening file';
      if (error.message && error.message.includes('file://')) {
        errorMessage = 'Browser security restricts opening local files. Try copying the file path and opening manually.';
      }
      
      return {
        success: false,
        error: errorMessage
      };
    }
  }

  async openFileSystem(filePath) {
    if (!filePath) {
      return { success: false, error: 'No file path provided' };
    }

    try {
      
      // For macOS, we can try using shell command via executeScript in content script
      // This is a workaround since Chrome extensions can't directly execute system commands
      
      // Try to create a temporary download link approach
      const fileUrl = filePath.startsWith('/') ? `file://${filePath}` : `file:///${filePath}`;
      
      // Method 1: Try to open in new tab first (might work on some systems)
      try {
        const tab = await chrome.tabs.create({
          url: fileUrl,
          active: false
        });
        
        // Check if tab was created successfully
        if (tab && tab.id) {
          
          // Close the tab after a short delay (system should have picked it up)
          setTimeout(async () => {
            try {
              await chrome.tabs.remove(tab.id);
            } catch (e) {
              // Tab might already be closed by system
            }
          }, 1000);
          
          return { success: true, method: 'system_tab', tabId: tab.id };
        }
      } catch (tabError) {
      }
      
      // Method 2: Try using the downloads API to force system open
      try {
        // Create a data URL that triggers download/open
        const response = await fetch(fileUrl);
        if (response.ok) {
          return { success: true, method: 'accessible', filePath };
        }
      } catch (fetchError) {
      }
      
      // If all methods fail
      return {
        success: false,
        error: 'Unable to open file with system default application',
        suggestion: 'Try copying the file path and opening manually'
      };
      
    } catch (error) {
      console.error('[VibeSurf] Error in openFileSystem:', error);
      return {
        success: false,
        error: error.message || 'Unknown error opening file'
      };
    }
  }

  async copyToClipboard(text) {
    console.log('[VibeSurf] Handling clipboard request, text length:', text?.length);
    
    try {
      // For Chrome extensions running in service worker context,
      // clipboard access is limited. We need to inject script into active tab.
      const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
      
      if (!tab) {
        throw new Error('No active tab found');
      }
      
      // Check if we can inject script into this tab
      if (tab.url.startsWith('chrome://') || tab.url.startsWith('chrome-extension://') ||
          tab.url.startsWith('edge://') || tab.url.startsWith('moz-extension://')) {
        throw new Error('Cannot access clipboard from this type of page');
      }
      
      // Inject script to handle clipboard operation
      const results = await chrome.scripting.executeScript({
        target: { tabId: tab.id },
        func: (textToCopy) => {
          try {
            // Method 1: Try modern clipboard API
            if (navigator.clipboard && navigator.clipboard.writeText) {
              return navigator.clipboard.writeText(textToCopy).then(() => {
                return { success: true, method: 'modern' };
              }).catch((error) => {
                console.warn('Modern clipboard API failed:', error);
                // Fall back to execCommand
                return fallbackCopy();
              });
            } else {
              // Method 2: Fall back to execCommand
              return Promise.resolve(fallbackCopy());
            }
            
            function fallbackCopy() {
              try {
                const textArea = document.createElement('textarea');
                textArea.value = textToCopy;
                textArea.style.position = 'fixed';
                textArea.style.left = '-999999px';
                textArea.style.top = '-999999px';
                textArea.style.opacity = '0';
                document.body.appendChild(textArea);
                textArea.focus();
                textArea.select();
                textArea.setSelectionRange(0, textArea.value.length);
                
                const success = document.execCommand('copy');
                document.body.removeChild(textArea);
                
                return { success: success, method: 'execCommand' };
              } catch (error) {
                return { success: false, error: error.message };
              }
            }
          } catch (error) {
            return { success: false, error: error.message };
          }
        },
        args: [text]
      });
      
      const result = await results[0].result;
      console.log('[VibeSurf] Clipboard operation result:', result);
      
      if (result.success) {
        return { success: true, method: result.method };
      } else {
        throw new Error(result.error || 'Clipboard operation failed');
      }
      
    } catch (error) {
      console.error('[VibeSurf] Clipboard operation failed:', error);
      return { success: false, error: error.message };
    }
  }

  // Request microphone permission through background script
  async requestMicrophonePermission() {
    try {
      console.log('[VibeSurf] Requesting microphone permission through background script');
      
      // Get the active tab to inject script
      const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
      
      if (!tab) {
        throw new Error('No active tab found');
      }
      
      // Check if we can inject script into this tab
      if (tab.url.startsWith('chrome://') || tab.url.startsWith('chrome-extension://') ||
          tab.url.startsWith('edge://') || tab.url.startsWith('moz-extension://')) {
        throw new Error('Cannot access microphone from this type of page');
      }
      
      // Inject script to request microphone permission
      const results = await chrome.scripting.executeScript({
        target: { tabId: tab.id },
        func: () => {
          return new Promise((resolve, reject) => {
            try {
              // Check if mediaDevices is available
              if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                reject(new Error('Media devices not supported'));
                return;
              }
              
              // Request microphone with minimal constraints
              const constraints = { audio: true, video: false };
              
              navigator.mediaDevices.getUserMedia(constraints)
                .then(stream => {
                  // Stop the stream immediately after getting permission
                  stream.getTracks().forEach(track => track.stop());
                  resolve({ success: true, hasPermission: true });
                })
                .catch(error => {
                  reject(new Error(`Microphone permission denied: ${error.name} - ${error.message}`));
                });
            } catch (error) {
              reject(new Error(`Failed to request microphone permission: ${error.message}`));
            }
          });
        }
      });
      
      const result = await results[0].result;
      console.log('[VibeSurf] Microphone permission result:', result);
      return result;
      
    } catch (error) {
      console.error('[VibeSurf] Failed to request microphone permission:', error);
      return { success: false, error: error.message };
    }
  }

  // Create a proper permission request page that opens in a new tab
  async requestMicrophonePermissionWithUI() {
    try {
      console.log('[VibeSurf] Opening permission request page in new tab');
      
      // Use the existing permission-request.html file
      const permissionPageUrl = chrome.runtime.getURL('permission-request.html');
      
      // Create a tab with the permission page
      const permissionTab = await chrome.tabs.create({
        url: permissionPageUrl,
        active: true
      });
      
      console.log('[VibeSurf] Created permission tab:', permissionTab.id);
      
      // Return a promise that resolves when we get the permission result
      return new Promise((resolve) => {
        const messageHandler = (message, sender, sendResponse) => {
          if (message.type === 'MICROPHONE_PERMISSION_RESULT') {
            console.log('[VibeSurf] Received permission result:', message);
            
            // Clean up the message listener
            chrome.runtime.onMessage.removeListener(messageHandler);
            
            // Close the permission tab
            chrome.tabs.remove(permissionTab.id).catch(() => {
              // Tab might already be closed
            });
            
            // Resolve the promise
            if (message.granted) {
              resolve({ success: true, hasPermission: true });
            } else {
              resolve({ success: false, error: message.error || 'Permission denied by user' });
            }
          }
        };
        
        // Add the message listener
        chrome.runtime.onMessage.addListener(messageHandler);
        
        // Set a timeout to clean up if the tab is closed without response
        setTimeout(() => {
          chrome.runtime.onMessage.removeListener(messageHandler);
          chrome.tabs.remove(permissionTab.id).catch(() => {});
          resolve({ success: false, error: 'Permission request timed out' });
        }, 30000); // 30 second timeout
      });
      
    } catch (error) {
      console.error('[VibeSurf] Failed to create permission UI:', error);
      return { success: false, error: error.message };
    }
  }

  // Cleanup method for extension unload
  async cleanup() {
    
    // Clear any active badges
    await chrome.action.setBadgeText({ text: '' });
    
    // Could add other cleanup tasks here
  }
}

// Initialize background service
const vibeSurfBackground = new VibeSurfBackground();

// Handle extension unload
chrome.runtime.onSuspend.addListener(() => {
  vibeSurfBackground.cleanup();
});

// Export for potential use in tests or other contexts
self.VibeSurfBackground = VibeSurfBackground;


================================================
FILE: vibe_surf/chrome_extension/config.js
================================================
// VibeSurf Extension Configuration
// Change BACKEND_URL here to update all backend connections synchronously

const VIBESURF_CONFIG = {
  // Backend server configuration
  BACKEND_URL: 'http://127.0.0.1:9335',
  
  // API related configuration
  API_PREFIX: '/api',
  DEFAULT_TIMEOUT: 30000,
  RETRY_ATTEMPTS: 3,
  RETRY_DELAY: 1000,
  
  // Session configuration
  DEFAULT_SESSION_PREFIX: '',
  
  // Notification configuration
  NOTIFICATIONS: {
    enabled: true,
    taskComplete: true,
    taskError: true
  },
  
  // UI configuration
  UI: {
    theme: 'auto',
    autoScroll: true,
    compactMode: false
  },

  // Social media links
  SOCIAL_LINKS: {
    github: "https://github.com/vibesurf-ai/VibeSurf",
    discord: "https://discord.gg/EZ2YnUXP",
    x: "https://x.com/warmshao",
    website: "https://vibe-surf.com/"
  },
  
  // Debug mode
  DEBUG: false
};

// Export configuration for use in other files
if (typeof window !== 'undefined') {
  window.VIBESURF_CONFIG = VIBESURF_CONFIG;
}

// Service worker environment (background script)
if (typeof self !== 'undefined' && typeof window === 'undefined') {
  self.VIBESURF_CONFIG = VIBESURF_CONFIG;
}

// Node.js environment compatibility (if needed)
if (typeof module !== 'undefined' && module.exports) {
  module.exports = VIBESURF_CONFIG;
}


================================================
FILE: vibe_surf/chrome_extension/content.js
================================================
// Content Script - VibeSurf Extension
// Runs in the context of web pages and can interact with page content

(function() {
  'use strict';
  
  // Avoid running multiple times
  if (window.vibeSurfContentLoaded) {
    return;
  }
  window.vibeSurfContentLoaded = true;
  
  console.log('[VibeSurf Content] Content script loaded on:', window.location.href);
  
  class VibeSurfContent {
    constructor() {
      this.initialized = false;
      this.pageContext = null;
      this.setupMessageListener();
      this.collectPageContext();
    }
    
    setupMessageListener() {
      // Listen for messages from background script or side panel
      chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
        console.log('[VibeSurf Content] Received message:', message.type);
        
        switch (message.type) {
          case 'GET_PAGE_CONTEXT':
            sendResponse(this.getPageContext());
            break;
            
          case 'SCROLL_TO_ELEMENT':
            this.scrollToElement(message.data?.selector);
            sendResponse({ success: true });
            break;
            
          case 'HIGHLIGHT_ELEMENT':
            this.highlightElement(message.data?.selector);
            sendResponse({ success: true });
            break;
            
          case 'GET_PAGE_TEXT':
            sendResponse({ text: this.getPageText() });
            break;
            
          case 'GET_PAGE_LINKS':
            sendResponse({ links: this.getPageLinks() });
            break;
            
          case 'CLICK_ELEMENT':
            const clickResult = this.clickElement(message.data?.selector);
            sendResponse(clickResult);
            break;
            
          case 'INJECT_MICROPHONE_PERMISSION_IFRAME':
            this.injectMicrophonePermissionIframe()
              .then(result => sendResponse(result))
              .catch(error => sendResponse({ success: false, error: error.message }));
            return true; // Will respond asynchronously
            
          case 'REMOVE_MICROPHONE_PERMISSION_IFRAME':
            this.removeMicrophonePermissionIframe();
            sendResponse({ success: true });
            break;
            
          default:
            console.warn('[VibeSurf Content] Unknown message type:', message.type);
        }
      });
      
      // Listen for postMessage from iframe
      window.addEventListener('message', (event) => {
        if (event.data && event.data.type === 'MICROPHONE_PERMISSION_RESULT') {
          console.log('[VibeSurf Content] Received permission result from iframe:', event.data);
          
          // Forward to extension
          chrome.runtime.sendMessage({
            type: 'MICROPHONE_PERMISSION_RESULT',
            ...event.data
          }).catch(() => {
            // Ignore if no listeners
          });
        }
      });
    }
    
    collectPageContext() {
      this.pageContext = {
        url: window.location.href,
        title: document.title,
        domain: window.location.hostname,
        timestamp: Date.now(),
        meta: this.getPageMeta(),
        hasForm: document.querySelector('form') !== null,
        hasTable: document.querySelector('table') !== null,
        linkCount: document.querySelectorAll('a[href]').length,
        imageCount: document.querySelectorAll('img').length,
        inputCount: document.querySelectorAll('input, textarea, select').length
      };
    }
    
    getPageContext() {
      // Refresh context data
      this.collectPageContext();
      return this.pageContext;
    }
    
    getPageMeta() {
      const meta = {};
      
      // Get meta tags
      const metaTags = document.querySelectorAll('meta');
      metaTags.forEach(tag => {
        const name = tag.getAttribute('name') || tag.getAttribute('property');
        const content = tag.getAttribute('content');
        if (name && content) {
          meta[name] = content;
        }
      });
      
      return meta;
    }
    
    getPageText() {
      // Get main content text, excluding scripts and styles
      const walker = document.createTreeWalker(
        document.body,
        NodeFilter.SHOW_TEXT,
        {
          acceptNode: function(node) {
            const parent = node.parentElement;
            if (parent && (
              parent.tagName === 'SCRIPT' ||
              parent.tagName === 'STYLE' ||
              parent.tagName === 'NOSCRIPT'
            )) {
              return NodeFilter.FILTER_REJECT;
            }
            return NodeFilter.FILTER_ACCEPT;
          }
        }
      );
      
      let text = '';
      let node;
      while (node = walker.nextNode()) {
        const textContent = node.textContent.trim();
        if (textContent) {
          text += textContent + ' ';
        }
      }
      
      return text.trim();
    }
    
    getPageLinks() {
      const links = [];
      const linkElements = document.querySelectorAll('a[href]');
      
      linkElements.forEach((link, index) => {
        const href = link.getAttribute('href');
        const text = link.textContent.trim();
        
        if (href && text) {
          links.push({
            index,
            href: this.resolveURL(href),
            text,
            title: link.getAttribute('title') || '',
            target: link.getAttribute('target') || '_self'
          });
        }
      });
      
      return links;
    }
    
    resolveURL(url) {
      try {
        return new URL(url, window.location.href).href;
      } catch (error) {
        return url;
      }
    }
    
    scrollToElement(selector) {
      try {
        const element = document.querySelector(selector);
        if (element) {
          element.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center' 
          });
          return true;
        }
      } catch (error) {
        console.error('[VibeSurf Content] Scroll error:', error);
      }
      return false;
    }
    
    highlightElement(selector) {
      try {
        // Remove previous highlights
        this.removeHighlights();
        
        const element = document.querySelector(selector);
        if (element) {
          // Add highlight styling
          element.style.outline = '3px solid #007acc';
          element.style.outlineOffset = '2px';
          element.setAttribute('data-vibesurf-highlight', 'true');
          
          // Auto-remove highlight after 5 seconds
          setTimeout(() => {
            this.removeHighlights();
          }, 5000);
          
          return true;
        }
      } catch (error) {
        console.error('[VibeSurf Content] Highlight error:', error);
      }
      return false;
    }
    
    removeHighlights() {
      const highlighted = document.querySelectorAll('[data-vibesurf-highlight]');
      highlighted.forEach(element => {
        element.style.outline = '';
        element.style.outlineOffset = '';
        element.removeAttribute('data-vibesurf-highlight');
      });
    }
    
    clickElement(selector) {
      try {
        const element = document.querySelector(selector);
        if (element) {
          // Scroll to element first
          element.scrollIntoView({ 
            behavior: 'smooth', 
            block: 'center' 
          });
          
          // Wait a bit for scroll, then click
          setTimeout(() => {
            element.click();
          }, 500);
          
          return { success: true, message: 'Element clicked' };
        } else {
          return { success: false, message: 'Element not found' };
        }
      } catch (error) {
        console.error('[VibeSurf Content] Click error:', error);
        return { success: false, message: error.message };
      }
    }
    
    // Inject hidden iframe for microphone permission request
    async injectMicrophonePermissionIframe() {
      try {
        console.log('[VibeSurf Content] Injecting microphone permission iframe...');
        
        // Check if iframe already exists
        const existingIframe = document.getElementById('vibesurf-permission-iframe');
        if (existingIframe) {
          console.log('[VibeSurf Content] Permission iframe already exists');
          return { success: true, alreadyExists: true };
        }
        
        // Create the iframe element
        const iframe = document.createElement('iframe');
        iframe.setAttribute('id', 'vibesurf-permission-iframe');
        iframe.setAttribute('allow', 'microphone');
        iframe.setAttribute('hidden', 'hidden');
        iframe.style.display = 'none';
        iframe.style.width = '0px';
        iframe.style.height = '0px';
        iframe.style.border = 'none';
        iframe.style.position = 'fixed';
        iframe.style.top = '-9999px';
        iframe.style.left = '-9999px';
        iframe.style.zIndex = '-1';
        
        // Set the source to our permission iframe page
        const iframeUrl = chrome.runtime.getURL('permission-iframe.html');
        iframe.src = iframeUrl;
        
        console.log('[VibeSurf Content] Creating iframe with URL:', iframeUrl);
        
        // Return a promise that resolves when permission is granted/denied
        return new Promise((resolve, reject) => {
          const timeout = setTimeout(() => {
            console.log('[VibeSurf Content] Permission iframe timeout');
            this.removeMicrophonePermissionIframe();
            reject(new Error('Permission request timeout'));
          }, 30000); // 30 second timeout
          
          // Listen for permission result
          const messageHandler = (event) => {
            if (event.data && event.data.type === 'MICROPHONE_PERMISSION_RESULT') {
              console.log('[VibeSurf Content] Received permission result:', event.data);
              
              clearTimeout(timeout);
              window.removeEventListener('message', messageHandler);
              
              if (event.data.success) {
                resolve({
                  success: true,
                  granted: event.data.granted,
                  source: 'iframe'
                });
              } else {
                resolve({
                  success: false,
                  granted: false,
                  error: event.data.error || 'Permission denied',
                  userMessage: event.data.userMessage
                });
              }
              
              // Clean up iframe after a short delay
              setTimeout(() => {
                this.removeMicrophonePermissionIframe();
              }, 1000);
            }
          };
          
          window.addEventListener('message', messageHandler);
          
          // Handle iframe load
          iframe.onload = () => {
            console.log('[VibeSurf Content] Permission iframe loaded successfully');
            
            // Send message to iframe to start permission request
            setTimeout(() => {
              if (iframe.contentWindow) {
                iframe.contentWindow.postMessage({
                  type: 'REQUEST_MICROPHONE_PERMISSION'
                }, '*');
              }
            }, 100);
          };
          
          iframe.onerror = (error) => {
            console.error('[VibeSurf Content] Permission iframe load error:', error);
            clearTimeout(timeout);
            window.removeEventListener('message', messageHandler);
            this.removeMicrophonePermissionIframe();
            reject(new Error('Failed to load permission iframe'));
          };
          
          // Append to document body
          document.body.appendChild(iframe);
          console.log('[VibeSurf Content] Permission iframe injected into page');
        });
        
      } catch (error) {
        console.error('[VibeSurf Content] Failed to inject permission iframe:', error);
        throw error;
      }
    }
    
    // Remove microphone permission iframe
    removeMicrophonePermissionIframe() {
      try {
        const iframe = document.getElementById('vibesurf-permission-iframe');
        if (iframe) {
          console.log('[VibeSurf Content] Removing permission iframe');
          iframe.remove();
          return true;
        }
        return false;
      } catch (error) {
        console.error('[VibeSurf Content] Error removing permission iframe:', error);
        return false;
      }
    }
    
    // Utility method to send context updates to background
    sendContextUpdate() {
      try {
        chrome.runtime.sendMessage({
          type: 'PAGE_CONTEXT_UPDATE',
          data: this.getPageContext()
        });
      } catch (error) {
        // Silently handle errors (extension might be reloading)
      }
    }
  }
  
  // Initialize content script
  const vibeSurfContent = new VibeSurfContent();
  
  // Send initial context when page loads
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', () => {
      vibeSurfContent.sendContextUpdate();
    });
  } else {
    vibeSurfContent.sendContextUpdate();
  }
  
  // Send context updates on navigation
  let lastUrl = window.location.href;
  const observer = new MutationObserver(() => {
    if (window.location.href !== lastUrl) {
      lastUrl = window.location.href;
      vibeSurfContent.collectPageContext();
      vibeSurfContent.sendContextUpdate();
    }
  });
  
  observer.observe(document.body, {
    childList: true,
    subtree: true
  });
  
  // Cleanup on page unload
  window.addEventListener('beforeunload', () => {
    vibeSurfContent.removeHighlights();
    observer.disconnect();
  });
  
  console.log('[VibeSurf Content] Content script initialized');
  
})();


================================================
FILE: vibe_surf/chrome_extension/dev-reload.js
================================================
// Development Auto-Reload Script for VibeSurf Extension
// This script enables automatic reloading when files change

(function() {
  'use strict';
  
  // Only run in development mode
  if (typeof chrome !== 'undefined' && chrome.runtime && chrome.runtime.getManifest) {
    const manifest = chrome.runtime.getManifest();
    
    // Check if this is a development extension (unpacked)
    const isDevelopment = !('update_url' in manifest);
    
    if (isDevelopment) {
      console.log('[VibeSurf Dev] Auto-reload enabled');
      
      // Check for file changes every 2 seconds
      setInterval(() => {
        fetch(chrome.runtime.getURL('manifest.json'))
          .then(response => response.text())
          .then(content => {
            const currentTime = new Date().getTime();
            const storageKey = 'vibesurf_last_reload';
            
            chrome.storage.local.get([storageKey], (result) => {
              const lastReload = result[storageKey] || 0;
              
              // Check if manifest was modified (simple content check)
              const contentHash = content.length + content.charCodeAt(0);
              const lastHash = localStorage.getItem('vibesurf_content_hash');
              
              if (lastHash && lastHash !== contentHash.toString()) {
                console.log('[VibeSurf Dev] Files changed, reloading extension...');
                chrome.runtime.reload();
              }
              
              localStorage.setItem('vibesurf_content_hash', contentHash.toString());
              chrome.storage.local.set({ [storageKey]: currentTime });
            });
          })
          .catch(error => {
            // Silently ignore errors (extension might be reloading)
          });
      }, 2000);
    }
  }
})();


================================================
FILE: vibe_surf/chrome_extension/manifest.json
================================================
{
  "manifest_version": 3,
  "name": "VibeSurf: Your browser assistant for vibe surfing",
  "version": "1.0.0",
  "description": "Let's vibe surfing the world",
  "icons": {
    "16": "icons/logo.png",
    "48": "icons/logo.png",
    "128": "icons/logo.png"
  },
  "permissions": [
    "activeTab",
    "storage",
    "notifications",
    "sidePanel",
    "contextMenus",
    "tabs",
    "clipboardWrite",
    "scripting"
  ],
  "host_permissions": [
    "http://localhost:*/*",
    "http://127.0.0.1:*/*",
    "file:///*"
  ],
  "background": {
    "service_worker": "background.js",
    "type": "module"
  },
  "content_scripts": [
    {
      "matches": ["<all_urls>"],
      "js": ["content.js"],
      "run_at": "document_end"
    }
  ],
  "action": {
    "default_title": "VibeSurf",
    "default_icon": {
      "16": "icons/logo.png",
      "24": "icons/logo.png",
      "32": "icons/logo.png"
    }
  },
  "side_panel": {
    "default_path": "sidepanel.html"
  },

  "web_accessible_resources": [
    {
      "resources": ["sidepanel.html", "styles/*", "scripts/*", "config.js", "icons/*"],
      "matches": ["<all_urls>"]
    }
  ]
}


================================================
FILE: vibe_surf/chrome_extension/permission-iframe.html
================================================
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Microphone Permission Request</title>
    <style>
        body {
            margin: 0;
            padding: 10px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            font-size: 12px;
            background: #f5f5f5;
            color: #333;
        }
        .status {
            padding: 8px;
            border-radius: 4px;
            text-align: center;
        }
        .loading {
            background: #bee3f8;
            color: #2a4365;
        }
        .success {
            background: #c6f6d5;
            color: #22543d;
        }
        .error {
            background: #fed7d7;
            color: #742a2a;
        }
    </style>
</head>
<body>
    <div id="status" class="status loading">Requesting microphone access...</div>
    <script src="scripts/permission-iframe-request.js"></script>
</body>
</html>


================================================
FILE: vibe_surf/chrome_extension/permission-request.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VibeSurf - Microphone Permission</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 12px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            text-align: center;
            max-width: 400px;
        }
        .logo {
            width: 60px;
            height: 60px;
            margin-bottom: 20px;
        }
        h1 {
            color: #2d3748;
            margin-bottom: 10px;
            font-size: 24px;
        }
        p {
            color: #718096;
            margin-bottom: 30px;
            line-height: 1.6;
        }
        .button-group {
            display: flex;
            gap: 12px;
            justify-content: center;
        }
        button {
            padding: 12px 24px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
        }
        .allow-btn {
            background: #4299e1;
            color: white;
        }
        .allow-btn:hover {
            background: #3182ce;
            transform: translateY(-1px);
        }
        .deny-btn {
            background: #e2e8f0;
            color: #4a5568;
        }
        .deny-btn:hover {
            background: #cbd5e0;
        }
        #status {
            margin-top: 20px;
            padding: 12px;
            border-radius: 6px;
            font-weight: 500;
        }
        .success {
            background: #c6f6d5;
            color: #22543d;
        }
        .error {
            background: #fed7d7;
            color: #742a2a;
        }
        .loading {
            background: #bee3f8;
            color: #2a4365;
        }
    </style>
</head>
<body>
    <div class="container">
        <img src="icons/logo.png" alt="VibeSurf" class="logo">
        <h1>Microphone Permission Required</h1>
        <p>VibeSurf needs access to your microphone to enable voice input. This permission is used only when you click the microphone button.</p>
        <div class="button-group">
            <button id="allowBtn" class="allow-btn">Allow Microphone</button>
            <button id="denyBtn" class="deny-btn">Deny</button>
        </div>
        <div id="status"></div>
    </div>

    <script src="scripts/permission-request.js"></script>
</body>
</html>


================================================
FILE: vibe_surf/chrome_extension/popup.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VibeSurf</title>
    <style>
        body {
            width: 300px;
            height: 200px;
            margin: 0;
            padding: 20px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            text-align: center;
        }
        
        .logo {
            font-size: 32px;
            margin-bottom: 10px;
        }
        
        h1 {
            font-size: 24px;
            margin: 0 0 10px 0;
            font-weight: 600;
        }
        
        p {
            font-size: 14px;
            margin: 0 0 20px 0;
            opacity: 0.9;
            line-height: 1.4;
        }
        
        .open-panel-btn {
            background: rgba(255, 255, 255, 0.2);
            border: 1px solid rgba(255, 255, 255, 0.3);
            color: white;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 500;
            transition: all 0.2s ease;
        }
        
        .open-panel-btn:hover {
            background: rgba(255, 255, 255, 0.3);
            border-color: rgba(255, 255, 255, 0.5);
        }
        
        .status {
            position: absolute;
            bottom: 10px;
            right: 10px;
            font-size: 12px;
            opacity: 0.7;
        }
        
        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #28a745;
            display: inline-block;
            margin-right: 5px;
        }
        
        .status-dot.disconnected {
            background: #dc3545;
        }
    </style>
</head>
<body>
    <div class="logo">ğŸŒŠ</div>
    <h1>VibeSurf</h1>
    <p>AI-powered browsing automation at your fingertips</p>
    <button class="open-panel-btn" id="openPanelBtn">Open Side Panel</button>
    
    <div class="status">
        <span class="status-dot" id="statusDot"></span>
        <span id="statusText">Ready</span>
    </div>
    
    <script>
        document.addEventListener('DOMContentLoaded', async () => {
            const openPanelBtn = document.getElementById('openPanelBtn');
            const statusDot = document.getElementById('statusDot');
            const statusText = document.getElementById('statusText');
            
            // Handle open panel button click
            openPanelBtn.addEventListener('click', async () => {
                try {
                    // Get current tab
                    const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
                    
                    // Open side panel
                    await chrome.sidePanel.open({ tabId: tab.id });
                    
                    // Close popup
                    window.close();
                } catch (error) {
                    console.error('Failed to open side panel:', error);
                    statusText.textContent = 'Error';
                    statusDot.classList.add('disconnected');
                }
            });
            
            // Check backend status
            try {
                const response = await chrome.runtime.sendMessage({ type: 'GET_BACKEND_STATUS' });
                
                if (response.status === 'connected') {
                    statusText.textContent = 'Connected';
                    statusDot.classList.remove('disconnected');
                } else {
                    statusText.textContent = 'Disconnected';
                    statusDot.classList.add('disconnected');
                }
            } catch (error) {
                console.error('Failed to check backend status:', error);
                statusText.textContent = 'Error';
                statusDot.classList.add('disconnected');
            }
        });
    </script>
</body>
</html>


================================================
FILE: vibe_surf/chrome_extension/sidepanel.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VibeSurf Extension</title>
    <!-- Core Styles -->
    <link rel="stylesheet" href="styles/variables.css">
    <link rel="stylesheet" href="styles/base.css">
    <link rel="stylesheet" href="styles/layout.css">
    
    <!-- Feature Styles -->
    <link rel="stylesheet" href="styles/activity.css">
    <link rel="stylesheet" href="styles/input.css">
    <link rel="stylesheet" href="styles/history-modal.css">
    <link rel="stylesheet" href="styles/components.css">
    <link rel="stylesheet" href="styles/animations.css">
    <!-- Settings Styles (Modular) -->
    <link rel="stylesheet" href="styles/settings-modal.css">
    <link rel="stylesheet" href="styles/settings-profiles.css">
    <link rel="stylesheet" href="styles/settings-forms.css">
    <link rel="stylesheet" href="styles/settings-environment.css">
    <link rel="stylesheet" href="styles/settings-utilities.css">
    <link rel="stylesheet" href="styles/settings-responsive.css">
    
    <!-- Responsive Styles -->
    <link rel="stylesheet" href="styles/responsive.css">
</head>
<body>
    <div id="app" class="vibesurf-container">
        <!-- Header -->
        <header class="header">
            <div class="header-left">
                <div class="logo">
                    <div class="logo-content">
                        <div class="logo-brand">
                            <img src="icons/logo.png" alt="VibeSurf" class="logo-image">
                            <span class="logo-text">VibeSurf</span>
                            <div class="social-links" id="social-links-container">
                                <!-- Social links will be populated dynamically from config -->
                            </div>
                        </div>
                        <div class="session-info">
                            <span class="session-label">Session:</span>
                            <span id="session-id">-</span>
                            <button id="copy-session-btn" class="copy-btn" title="Copy Session ID">
                                <svg width="12" height="12" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M16 1H4C2.9 1 2 1.9 2 3V17H4V3H16V1ZM19 5H8C6.9 5 6 5.9 6 7V21C6 22.1 6.9 23 8 23H19C20.1 23 21 22.1 21 21V7C21 5.9 20.1 5 19 5ZM19 21H8V7H19V21Z" fill="currentColor"/>
                                </svg>
                            </button>
                        </div>
                    </div>
                </div>
            </div>
            <div class="header-right">
                <button id="new-session-btn" class="icon-btn" title="New Session">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                    </svg>
                </button>
                <button id="history-btn" class="icon-btn" title="Chat History">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M3 3V11A4 4 0 0 0 7 15H17L21 19V3H3Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                </button>
                <button id="settings-btn" class="icon-btn" title="Settings">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M12 15C13.6569 15 15 13.6569 15 12C15 10.3431 13.6569 9 12 9C10.3431 9 9 10.3431 9 12C9 13.6569 10.3431 15 12 15Z" stroke="currentColor" stroke-width="2"/>
                        <path d="M19.4 15C19.2669 15.3016 19.2272 15.6362 19.286 15.9606C19.3448 16.285 19.4995 16.5843 19.73 16.82L19.79 16.88C19.976 17.0657 20.1235 17.2863 20.2241 17.5291C20.3248 17.7719 20.3766 18.0322 20.3766 18.295C20.3766 18.5578 20.3248 18.8181 20.2241 19.0609C20.1235 19.3037 19.976 19.5243 19.79 19.71C19.6043 19.896 19.3837 20.0435 19.1409 20.1441C18.8981 20.2448 18.6378 20.2966 18.375 20.2966C18.1122 20.2966 17.8519 20.2448 17.6091 20.1441C17.3663 20.0435 17.1457 19.896 16.96 19.71L16.9 19.65C16.6643 19.4195 16.365 19.2648 16.0406 19.206C15.7162 19.1472 15.3816 19.1869 15.08 19.32C14.7842 19.4468 14.532 19.6572 14.3543 19.9255C14.1766 20.1938 14.0813 20.5082 14.08 20.83V21C14.08 21.5304 13.8693 22.0391 13.4942 22.4142C13.1191 22.7893 12.6104 23 12.08 23C11.5496 23 11.0409 22.7893 10.6658 22.4142C10.2907 22.0391 10.08 21.5304 10.08 21V20.91C10.0723 20.579 9.96512 20.2579 9.77251 19.9887C9.5799 19.7194 9.31074 19.5143 9 19.4C8.69838 19.2669 8.36381 19.2272 8.03941 19.286C7.71502 19.3448 7.41568 19.4995 7.18 19.73L7.12 19.79C6.93425 19.976 6.71368 20.1235 6.47088 20.2241C6.22808 20.3248 5.96783 20.3766 5.705 20.3766C5.44217 20.3766 5.18192 20.3248 4.93912 20.2241C4.69632 20.1235 4.47575 19.976 4.29 19.79C4.10405 19.6043 3.95653 19.3837 3.85588 19.1409C3.75523 18.8981 3.70343 18.6378 3.70343 18.375C3.70343 18.1122 3.75523 17.8519 3.85588 17.6091C3.95653 17.3663 4.10405 17.1457 4.29 16.96L4.35 16.9C4.58054 16.6643 4.73519 16.365 4.794 16.0406C4.85282 15.7162 4.81312 15.3816 4.68 15.08C4.55324 14.7842 4.34276 14.532 4.07447 14.3543C3.80618 14.1766 3.49179 14.0813 3.17 14.08H3C2.46957 14.08 1.96086 13.8693 1.58579 13.4942C1.21071 13.1191 1 12.6104 1 12.08C1 11.5496 1.21071 11.0409 1.58579 10.6658C1.96086 10.2907 2.46957 10.08 3 10.08H3.09C3.42099 10.0723 3.742 9.96512 4.01127 9.77251C4.28053 9.5799 4.48572 9.31074 4.6 9C4.73312 8.69838 4.77282 8.36381 4.714 8.03941C4.65519 7.71502 4.50054 7.41568 4.27 7.18L4.21 7.12C4.02405 6.93425 3.87653 6.71368 3.77588 6.47088C3.67523 6.22808 3.62343 5.96783 3.62343 5.705C3.62343 5.44217 3.67523 5.18192 3.77588 4.93912C3.87653 4.69632 4.02405 4.47575 4.21 4.29C4.39575 4.10405 4.61632 3.95653 4.85912 3.85588C5.10192 3.75523 5.36217 3.70343 5.625 3.70343C5.88783 3.70343 6.14808 3.75523 6.39088 3.85588C6.63368 3.95653 6.85425 4.10405 7.04 4.29L7.1 4.35C7.33568 4.58054 7.63502 4.73519 7.95941 4.794C8.28381 4.85282 8.61838 4.81312 8.92 4.68H9C9.29577 4.55324 9.54802 4.34276 9.72569 4.07447C9.90337 3.80618 9.99872 3.49179 10 3.17V3C10 2.46957 10.2107 1.96086 10.5858 1.58579C10.9609 1.21071 11.4696 1 12 1C12.5304 1 13.0391 1.21071 13.4142 1.58579C13.7893 1.96086 14 2.46957 14 3V3.09C14.0013 3.41179 14.0966 3.72618 14.2743 3.99447C14.452 4.26276 14.7042 4.47324 15 4.6C15.3016 4.73312 15.6362 4.77282 15.9606 4.714C16.285 4.65519 16.5843 4.50054 16.82 4.27L16.88 4.21C17.0657 4.02405 17.2863 3.87653 17.5291 3.77588C17.7719 3.67523 18.0322 3.62343 18.295 3.62343C18.5578 3.62343 18.8181 3.67523 19.0609 3.77588C19.3037 3.87653 19.5243 4.02405 19.71 4.21C19.896 4.39575 20.0435 4.61632 20.1441 4.85912C20.2448 5.10192 20.2966 5.36217 20.2966 5.625C20.2966 5.88783 20.2448 6.14808 20.1441 6.39088C20.0435 6.63368 19.896 6.85425 19.71 7.04L19.65 7.1C19.4195 7.33568 19.2648 7.63502 19.206 7.95941C19.1472 8.28381 19.1869 8.61838 19.32 8.92V9C19.4468 9.29577 19.6572 9.54802 19.9255 9.72569C20.1938 9.90337 20.5082 9.99872 20.83 10H21C21.5304 10 22.0391 10.2107 22.4142 10.5858C22.7893 10.9609 23 11.4696 23 12C23 12.5304 22.7893 13.0391 22.4142 13.4142C22.0391 13.7893 21.5304 14 21 14H20.91C20.5882 14.0013 20.2738 14.0966 20.0055 14.2743C19.7372 14.452 19.5268 14.7042 19.4 15Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                </button>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Activity Log Area -->
            <div class="activity-section">
                <div id="activity-log" class="activity-log">
                    <div class="welcome-message">
                        <div class="welcome-text">
                            <h4>Welcome to VibeSurf</h4>
                            <p>Let's vibe surfing the internet with AI automation</p>
                        </div>
                        <div class="quick-tasks">
                            <div class="task-suggestion" data-task="research">
                                <div class="task-icon">ğŸ”</div>
                                <div class="task-content">
                                    <div class="task-title">Research Founders</div>
                                    <div class="task-description">Search information about browser-use and browser-use-webui, write a brief report</div>
                                </div>
                            </div>
                            <div class="task-suggestion" data-task="news">
                                <div class="task-icon">ğŸ“°</div>
                                <div class="task-content">
                                    <div class="task-title">HackerNews Summary</div>
                                    <div class="task-description">Get top 10 news from HackerNews and provide a summary</div>
                                </div>
                            </div>
                            <div class="task-suggestion" data-task="analysis">
                                <div class="task-icon">ğŸ“ˆ</div>
                                <div class="task-content">
                                    <div class="task-title">Stock Market Analysis</div>
                                    <div class="task-description">Analyze recent week stock market trends for major tech companies</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Control Panel -->
            <div id="control-panel" class="control-panel hidden">
                <button id="cancel-btn" class="control-btn cancel-btn">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M6 4H10V20H6V4ZM14 4H18V20H14V4Z" fill="currentColor"/>
                    </svg>
                    Pause
                </button>
                <button id="resume-btn" class="control-btn resume-btn hidden">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M8 5V19L19 12L8 5Z" fill="currentColor"/>
                    </svg>
                    Resume
                </button>
                <button id="terminate-btn" class="control-btn terminate-btn hidden">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M5 5H19V19H5V5Z" fill="currentColor"/>
                    </svg>
                    Terminate
                </button>
            </div>
        </main>

        <!-- Input Section -->
        <footer class="input-section">
            <div class="input-container">
                <div class="input-main">
                    <div class="textarea-container">
                        <textarea
                            id="task-input"
                            class="task-input"
                            placeholder="Ask anything (/ for skills, @ to specify tab)"
                            rows="3"></textarea>
                        <!-- Tab Selection Dropdown -->
                        <div id="tab-selector-dropdown" class="tab-selector-dropdown hidden">
                            <div class="tab-selector-header">
                                <span class="tab-selector-title">Select Tabs</span>
                            </div>
                            <div class="tab-selector-content">
                                <div class="tab-selector-controls">
                                    <label class="tab-option select-all-option">
                                        <input type="radio" id="select-all-tabs" name="tab-selection" class="tab-radio">
                                        <span class="tab-name">Select All</span>
                                    </label>
                                </div>
                                <div id="tab-options-list" class="tab-options-list">
                                    <!-- Tab options will be populated here -->
                                </div>
                            </div>
                        </div>
                        
                        <!-- Skill Selection Dropdown -->
                        <div id="skill-selector-dropdown" class="skill-selector-dropdown hidden">
                            <div class="skill-selector-header">
                                <span class="skill-selector-title">Select Skills</span>
                            </div>
                            <div class="skill-selector-content">
                                <div id="skill-options-list" class="skill-options-list">
                                    <!-- Skill options will be populated here -->
                                </div>
                            </div>
                        </div>
                        <div class="input-actions">
                            <button id="voice-record-btn" class="action-btn voice-record-btn" title="Voice Input">
                                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M12 1C10.34 1 9 2.34 9 4V10C9 11.66 10.34 13 12 13C13.66 13 15 11.66 15 10V4C15 2.34 13.66 1 12 1Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                    <path d="M19 10V11C19 14.87 15.87 18 12 18C8.13 18 5 14.87 5 11V10" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                    <path d="M12 18V23" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                    <path d="M8 23H16" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                </svg>
                            </button>
                            <button id="attach-file-btn" class="action-btn attach-btn" title="Attach Files">
                                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M21.44 11.05L12.25 20.24C11.1242 21.3658 9.59722 21.9983 8.005 21.9983C6.41278 21.9983 4.88583 21.3658 3.76 20.24C2.63417 19.1142 2.00166 17.5872 2.00166 15.995C2.00166 14.4028 2.63417 12.8758 3.76 11.75L12.33 3.18C13.0806 2.42944 14.0986 2.00696 15.16 2.00696C16.2214 2.00696 17.2394 2.42944 17.99 3.18C18.7406 3.93056 19.163 4.94859 19.163 6.01C19.163 7.07141 18.7406 8.08944 17.99 8.84L10.07 16.76C9.69469 17.1353 9.1897 17.3442 8.665 17.3442C8.1403 17.3442 7.63531 17.1353 7.26 16.76C6.88469 16.3847 6.67581 15.8797 6.67581 15.355C6.67581 14.8303 6.88469 14.3253 7.26 13.95L15.19 6.02" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                </svg>
                            </button>
                            <button id="send-btn" class="action-btn send-btn" title="Send Task">
                                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M22 2L11 13M22 2L15 22L11 13M22 2L2 9L11 13" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                </svg>
                            </button>
                        </div>
                    </div>
                </div>
                <div class="input-footer">
                    <select id="llm-profile-select" class="llm-select compact">
                    </select>
                    <select id="agent-mode-select" class="agent-mode-select compact">
                        <option value="thinking" selected>Thinking</option>
                        <option value="no-thinking">No-thinking</option>
                        <option value="flash">Flash</option>
                    </select>
                </div>
            </div>
            <input type="file" id="file-input" class="hidden" multiple accept="*/*">
        </footer>
    </div>

    <!-- Modals -->
    <!-- History Modal -->
    <div id="history-modal" class="modal hidden">
        <div class="modal-overlay"></div>
        <div class="modal-content history-modal-content">
            <div class="modal-header">
                <h3>Chat History</h3>
                <button class="modal-close">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M6 6L18 18M6 18L18 6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                    </svg>
                </button>
            </div>
            <div class="history-content">
                <!-- Recent Tasks Section -->
                <div class="history-section recent-tasks-section">
                    <div class="section-header">
                        <h4>Recent Tasks</h4>
                        <span class="section-subtitle">Last 3 sessions</span>
                    </div>
                    <div id="recent-tasks-list" class="recent-tasks-list">
                        <!-- Recent tasks will be populated here -->
                    </div>
                    <button id="view-more-tasks-btn" class="view-more-btn">
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        View More Tasks
                    </button>
                </div>

                <!-- All Sessions Section (initially hidden) -->
                <div id="all-sessions-section" class="history-section all-sessions-section hidden">
                    <div class="section-header">
                        <h4>All Sessions</h4>
                        <button id="back-to-recent-btn" class="back-btn">
                            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                <path d="M19 12H5M12 19L5 12L12 5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                            </svg>
                            Back to Recent
                        </button>
                    </div>
                    <div class="search-filter-bar">
                        <input type="text" id="session-search" placeholder="Search sessions..." class="search-input">
                        <select id="session-filter" class="filter-select">
                            <option value="all">All Sessions</option>
                            <option value="active">Active</option>
                            <option value="completed">Completed</option>
                            <option value="error">Error</option>
                        </select>
                    </div>
                    <div id="all-sessions-list" class="sessions-list">
                        <!-- All sessions will be populated here -->
                    </div>
                    <div class="pagination-controls">
                        <button id="prev-page-btn" class="page-btn" disabled>Previous</button>
                        <span id="page-info" class="page-info">Page 1 of 1</span>
                        <button id="next-page-btn" class="page-btn" disabled>Next</button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Settings Modal -->
    <div id="settings-modal" class="modal hidden">
        <div class="modal-overlay"></div>
        <div class="modal-content settings-modal-content">
            <div class="settings-header">
                <div class="settings-title">
                    <img src="icons/logo.png" alt="VibeSurf" class="settings-logo">
                    <h3>VibeSurf Settings</h3>
                </div>
                <button class="modal-close">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M6 6L18 18M6 18L18 6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                    </svg>
                </button>
            </div>
            
            <!-- Tab Navigation -->
            <div class="settings-tabs">
                <button class="settings-tab active" data-tab="general">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin-right: 8px;">
                        <path d="M12 15C13.6569 15 15 13.6569 15 12C15 10.3431 13.6569 9 12 9C10.3431 9 9 10.3431 9 12C9 13.6569 10.3431 15 12 15Z" stroke="currentColor" stroke-width="2"/>
                        <path d="M19.4 15C19.2669 15.3016 19.2272 15.6362 19.286 15.9606C19.3448 16.285 19.4995 16.5843 19.73 16.82L19.79 16.88C19.976 17.0657 20.1235 17.2863 20.2241 17.5291C20.3248 17.7719 20.3766 18.0322 20.3766 18.295C20.3766 18.5578 20.3248 18.8181 20.2241 19.0609C20.1235 19.3037 19.976 19.5243 19.79 19.71C19.6043 19.896 19.3837 20.0435 19.1409 20.1441C18.8981 20.2448 18.6378 20.2966 18.375 20.2966C18.1122 20.2966 17.8519 20.2448 17.6091 20.1441C17.3663 20.0435 17.1457 19.896 16.96 19.71L16.9 19.65C16.6643 19.4195 16.365 19.2648 16.0406 19.206C15.7162 19.1472 15.3816 19.1869 15.08 19.32C14.7842 19.4468 14.532 19.6572 14.3543 19.9255C14.1766 20.1938 14.0813 20.5082 14.08 20.83V21C14.08 21.5304 13.8693 22.0391 13.4942 22.4142C13.1191 22.7893 12.6104 23 12.08 23C11.5496 23 11.0409 22.7893 10.6658 22.4142C10.2907 22.0391 10.08 21.5304 10.08 21V20.91C10.0723 20.579 9.96512 20.2579 9.77251 19.9887C9.5799 19.7194 9.31074 19.5143 9 19.4C8.69838 19.2669 8.36381 19.2272 8.03941 19.286C7.71502 19.3448 7.41568 19.4995 7.18 19.73L7.12 19.79C6.93425 19.976 6.71368 20.1235 6.47088 20.2241C6.22808 20.3248 5.96783 20.3766 5.705 20.3766C5.44217 20.3766 5.18192 20.3248 4.93912 20.2241C4.69632 20.1235 4.47575 19.976 4.29 19.79C4.10405 19.6043 3.95653 19.3837 3.85588 19.1409C3.75523 18.8981 3.70343 18.6378 3.70343 18.375C3.70343 18.1122 3.75523 17.8519 3.85588 17.6091C3.95653 17.3663 4.10405 17.1457 4.29 16.96L4.35 16.9C4.58054 16.6643 4.73519 16.365 4.794 16.0406C4.85282 15.7162 4.81312 15.3816 4.68 15.08C4.55324 14.7842 4.34276 14.532 4.07447 14.3543C3.80618 14.1766 3.49179 14.0813 3.17 14.08H3C2.46957 14.08 1.96086 13.8693 1.58579 13.4942C1.21071 13.1191 1 12.6104 1 12.08C1 11.5496 1.21071 11.0409 1.58579 10.6658C1.96086 10.2907 2.46957 10.08 3 10.08H3.09C3.42099 10.0723 3.742 9.96512 4.01127 9.77251C4.28053 9.5799 4.48572 9.31074 4.6 9C4.73312 8.69838 4.77282 8.36381 4.714 8.03941C4.65519 7.71502 4.50054 7.41568 4.27 7.18L4.21 7.12C4.02405 6.93425 3.87653 6.71368 3.77588 6.47088C3.67523 6.22808 3.62343 5.96783 3.62343 5.705C3.62343 5.44217 3.67523 5.18192 3.77588 4.93912C3.87653 4.69632 4.02405 4.47575 4.21 4.29C4.39575 4.10405 4.61632 3.95653 4.85912 3.85588C5.10192 3.75523 5.36217 3.70343 5.625 3.70343C5.88783 3.70343 6.14808 3.75523 6.39088 3.85588C6.63368 3.95653 6.85425 4.10405 7.04 4.29L7.1 4.35C7.33568 4.58054 7.63502 4.73519 7.95941 4.794C8.28381 4.85282 8.61838 4.81312 8.92 4.68H9C9.29577 4.55324 9.54802 4.34276 9.72569 4.07447C9.90337 3.80618 9.99872 3.49179 10 3.17V3C10 2.46957 10.2107 1.96086 10.5858 1.58579C10.9609 1.21071 11.4696 1 12 1C12.5304 1 13.0391 1.21071 13.4142 1.58579C13.7893 1.96086 14 2.46957 14 3V3.09C14.0013 3.41179 14.0966 3.72618 14.2743 3.99447C14.452 4.26276 14.7042 4.47324 15 4.6C15.3016 4.73312 15.6362 4.77282 15.9606 4.714C16.285 4.65519 16.5843 4.50054 16.82 4.27L16.88 4.21C17.0657 4.02405 17.2863 3.87653 17.5291 3.77588C17.7719 3.67523 18.0322 3.62343 18.295 3.62343C18.5578 3.62343 18.8181 3.67523 19.0609 3.77588C19.3037 3.87653 19.5243 4.02405 19.71 4.21C19.896 4.39575 20.0435 4.61632 20.1441 4.85912C20.2448 5.10192 20.2966 5.36217 20.2966 5.625C20.2966 5.88783 20.2448 6.14808 20.1441 6.39088C20.0435 6.63368 19.896 6.85425 19.71 7.04L19.65 7.1C19.4195 7.33568 19.2648 7.63502 19.206 7.95941C19.1472 8.28381 19.1869 8.61838 19.32 8.92V9C19.4468 9.29577 19.6572 9.54802 19.9255 9.72569C20.1938 9.90337 20.5082 9.99872 20.83 10H21C21.5304 10 22.0391 10.2107 22.4142 10.5858C22.7893 10.9609 23 11.4696 23 12C23 12.5304 22.7893 13.0391 22.4142 13.4142C22.0391 13.7893 21.5304 14 21 14H20.91C20.5882 14.0013 20.2738 14.0966 20.0055 14.2743C19.7372 14.452 19.5268 14.7042 19.4 15Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                    General
                </button>
                <button class="settings-tab" data-tab="llm-profiles">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin-right: 8px;">
                        <path d="M12 2L2 7L12 12L22 7L12 2Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        <path d="M2 17L12 22L22 17" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        <path d="M2 12L12 17L22 12" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                    LLM
                </button>
                <button class="settings-tab" data-tab="mcp-profiles">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin-right: 8px;">
                        <path d="M13 2L3 14H12L11 22L21 10H12L13 2Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                    MCP
                </button>
                <button class="settings-tab" data-tab="voice-profiles">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin-right: 8px;">
                        <path d="M19 14C19 18.5 15.5 22 11 22C6.5 22 3 18.5 3 14V12C3 7.5 6.5 4 11 4S19 7.5 19 12V14ZM11 8C8.8 8 7 9.8 7 12V14C7 16.2 8.8 18 11 18S15 16.2 15 14V12C15 9.8 13.2 8 11 8Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                        <circle cx="11" cy="11" r="2" stroke="currentColor" stroke-width="2"/>
                    </svg>
                    Voice
                </button>
            </div>
            
            <!-- Tab Content -->
            <div class="settings-content">
                <!-- General Tab -->
                <div id="general-tab" class="settings-tab-content active">
                    <div class="general-sections">
                        <!-- General Settings Section -->
                        <div class="general-section">
                            <div class="section-header">
                                <h3>General Settings</h3>
                                <p class="section-description">Configure application-wide settings</p>
                            </div>
                            <div class="general-settings-content">
                                <div class="form-group">
                                    <label class="form-label">Application Theme</label>
                                    <select class="form-select" id="theme-select">
                                        <option value="auto">Auto (System)</option>
                                        <option value="light">Light</option>
                                        <option value="dark">Dark</option>
                                    </select>
                                    <div class="form-help">Choose your preferred theme</div>
                                </div>
                                <div class="form-group">
                                    <label class="form-label">Default ASR Profile</label>
                                    <select class="form-select" id="default-asr-select">
                                        <option value="">No ASR profile selected</option>
                                    </select>
                                    <div class="form-help">Default voice recognition profile for speech-to-text</div>
                                </div>
                                <div class="form-group">
                                    <label class="form-label">Default TTS Profile</label>
                                    <select class="form-select" id="default-tts-select">
                                        <option value="">No TTS profile selected</option>
                                    </select>
                                    <div class="form-help">Default text-to-speech profile for voice output</div>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Environment Variables Section -->
                        <div class="general-section">
                            <div class="section-header">
                                <h3>Environment Variables</h3>
                                <p class="section-description">Manage environment variables for the application</p>
                            </div>
                            <div class="env-variables-container">
                                <div class="form-group">
                                    <label class="form-label">Environment Variables</label>
                                    <div id="env-variables-list">
                                        <!-- Environment variables will be populated here -->
                                    </div>
                                </div>
                                <div class="env-var-actions-center">
                                    <button id="save-env-vars-btn" class="save-env-vars-btn-compact">
                                        <div class="btn-icon">
                                            <svg width="14" height="14" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                                                <path d="M19 21H5C4.46957 21 3.96086 20.7893 3.58579 20.4142C3.21071 20.0391 3 19.5304 3 19V5C3 4.46957 3.21071 3.96086 3.58579 3.58579C3.96086 3.21071 4.46957 3 5 3H16L21 8V19C21 19.5304 20.7893 20.0391 20.4142 20.4142C20.0391 20.7893 19.5304 21 19 21Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                                <path d="M17 21V13H7V21M7 3V8H15" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                                            </svg>
                                        </div>
                                        <span class="btn-text">Update Variables</span>
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- LLM Profiles Tab -->
                <div id="llm-profiles-tab" class="settings-tab-content">
                    <div id="llm-profiles-container" class="profiles-container">
                        <div id="llm-profiles-list" class="profiles-list">
                            <!-- LLM profiles will be populated here -->
                        </div>
                    </div>
                    <button id="add-llm-profile-btn" class="add-profile-btn">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        Add LLM
                    </button>
                </div>
                
                <!-- MCP Profiles Tab -->
                <div id="mcp-profiles-tab" class="settings-tab-content">
                    <div id="mcp-profiles-container" class="profiles-container">
                        <div id="mcp-profiles-list" class="profiles-list">
                            <!-- MCP profiles will be populated here -->
                        </div>
                    </div>
                    <button id="add-mcp-profile-btn" class="add-profile-btn">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        Add MCP
                    </button>
                </div>
                
                <!-- Voice Profiles Tab -->
                <div id="voice-profiles-tab" class="settings-tab-content">
                    <div id="voice-profiles-container" class="profiles-container">
                        <div id="voice-profiles-list" class="profiles-list">
                            <!-- Voice profiles will be populated here -->
                        </div>
                    </div>
                    <button id="add-voice-profile-btn" class="add-profile-btn">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        Add Voice
                    </button>
                </div>
            </div>
        </div>
    </div>

    <!-- Profile Form Modal -->
    <div id="profile-form-modal" class="profile-form-modal hidden">
        <div class="profile-form-container">
            <div class="profile-form-header">
                <h3 id="profile-form-title">Add LLM</h3>
                <button class="profile-form-close">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M6 6L18 18M6 18L18 6" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                    </svg>
                </button>
            </div>
            <div class="profile-form-content">
                <form id="profile-form">
                    <!-- Form content will be dynamically generated -->
                </form>
            </div>
            <div class="profile-form-actions">
                <button type="button" class="form-btn secondary" id="profile-form-cancel">Cancel</button>
                <button type="submit" class="form-btn primary" id="profile-form-submit">Save Profile</button>
            </div>
        </div>
    </div>

    <!-- Loading Overlay -->
    <div id="loading-overlay" class="loading-overlay hidden">
        <div class="loading-spinner"></div>
        <div class="loading-text">Connecting to VibeSurf...</div>
    </div>

    <!-- Scripts -->
    <script src="config.js"></script>
    <script src="scripts/markdown-it.min.js"></script>
    <script src="scripts/api-client.js"></script>
    <script src="scripts/session-manager.js"></script>
    <!-- User Settings Storage (must load before managers) -->
    <script src="scripts/user-settings-storage.js"></script>
    <!-- Voice Recording -->
    <script src="scripts/voice-recorder.js"></script>
    <!-- Specialized UI Managers -->
    <script src="scripts/modal-manager.js"></script>
    <script src="scripts/settings-manager.js"></script>
    <script src="scripts/history-manager.js"></script>
    <script src="scripts/file-manager.js"></script>
    <!-- Core UI Manager -->
    <script src="scripts/ui-manager.js"></script>
    <script src="scripts/main.js"></script>
</body>
</html>


================================================
FILE: vibe_surf/chrome_extension/scripts/api-client.js
================================================
// API Client - VibeSurf Backend Communication
// Handles all HTTP requests to the VibeSurf backend API

class VibeSurfAPIClient {
  constructor(baseURL = null) {
    // Use configuration file values as defaults
    const config = window.VIBESURF_CONFIG || {};
    this.baseURL = (baseURL || config.BACKEND_URL || 'http://localhost:9335').replace(/\/$/, ''); // Remove trailing slash
    this.apiPrefix = config.API_PREFIX || '/api';
    this.timeout = config.DEFAULT_TIMEOUT || 30000;
    this.retryAttempts = config.RETRY_ATTEMPTS || 3;
    this.retryDelay = config.RETRY_DELAY || 1000;
  }

  // Utility method to build full URL
  buildURL(endpoint) {
    const cleanEndpoint = endpoint.startsWith('/') ? endpoint : `/${endpoint}`;
    return `${this.baseURL}${this.apiPrefix}${cleanEndpoint}`;
  }

  // Generic HTTP request method with error handling and retries
  async request(method, endpoint, options = {}) {
    const {
      data,
      params,
      headers = {},
      timeout = this.timeout,
      retries = this.retryAttempts,
      ...fetchOptions
    } = options;

    const url = new URL(this.buildURL(endpoint));
    
    // Add query parameters
    if (params) {
      Object.keys(params).forEach(key => {
        if (params[key] !== undefined && params[key] !== null) {
          url.searchParams.append(key, params[key]);
        }
      });
    }

    const config = {
      method,
      headers: {
        'Content-Type': 'application/json',
        ...headers
      },
      signal: AbortSignal.timeout(timeout),
      ...fetchOptions
    };

    // Add body for POST/PUT requests
    if (data && method !== 'GET') {
      if (data instanceof FormData) {
        // Remove Content-Type for FormData (browser will set it with boundary)
        delete config.headers['Content-Type'];
        config.body = data;
      } else {
        config.body = JSON.stringify(data);
      }
    }

    let lastError;
    
    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        console.log(`[API] ${method} ${url} (attempt ${attempt + 1}/${retries + 1})`);
        
        const response = await fetch(url, config);
        
        // Handle different response types
        const contentType = response.headers.get('content-type');
        let responseData;
        
        if (contentType && contentType.includes('application/json')) {
          responseData = await response.json();
        } else {
          responseData = await response.text();
        }

        if (!response.ok) {
          throw new APIError(
            responseData.detail || responseData.message || `HTTP ${response.status}`,
            response.status,
            responseData
          );
        }

        console.log(`[API] ${method} ${url} - Success`);
        return responseData;

      } catch (error) {
        lastError = error;
        console.error(`[API] ${method} ${url} - Error (attempt ${attempt + 1}):`, error);

        // Don't retry on certain errors
        if (error instanceof APIError) {
          if (error.status >= 400 && error.status < 500) {
            throw error; // Client errors shouldn't be retried
          }
          
          // Don't retry on LLM connection failures
          if (error.data && error.data.error === 'llm_connection_failed') {
            console.log('[API] LLM connection failed - skipping retry');
            throw error;
          }
        }

        // Don't retry on timeout for the last attempt
        if (attempt === retries) {
          break;
        }

        // Wait before retry
        await this.delay(this.retryDelay * (attempt + 1));
      }
    }

    throw lastError;
  }

  // HTTP method helpers
  async get(endpoint, options = {}) {
    return this.request('GET', endpoint, options);
  }

  async post(endpoint, data, options = {}) {
    return this.request('POST', endpoint, { data, ...options });
  }

  async put(endpoint, data, options = {}) {
    return this.request('PUT', endpoint, { data, ...options });
  }

  async delete(endpoint, options = {}) {
    return this.request('DELETE', endpoint, options);
  }

  // Health check - special method that bypasses API prefix
  async healthCheck() {
    try {
      // Build URL without API prefix for health endpoint
      const url = `${this.baseURL}/health`;
      const response = await fetch(url, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        },
        signal: AbortSignal.timeout(5000)
      });
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`);
      }
      
      const data = await response.json();
      console.log('[API] Health check - Success');
      return { status: 'healthy', data };
    } catch (error) {
      console.error('[API] Health check - Error:', error);
      return { status: 'unhealthy', error: error.message };
    }
  }

  // System status
  async getSystemStatus() {
    return this.get('/status');
  }

  // Task Management APIs
  async submitTask(taskData) {
    const {
      session_id,
      task_description,
      llm_profile_name,
      upload_files_path,
      mcp_server_config,
      agent_mode
    } = taskData;

    return this.post('/tasks/submit', {
      session_id,
      task_description,
      llm_profile_name,
      upload_files_path,
      mcp_server_config,
      agent_mode
    });
  }

  async getTaskStatus() {
    return this.get('/tasks/status');
  }

  async checkTaskRunning() {
    try {
      const status = await this.getTaskStatus();
      
      console.log('[API] Task status check result:', {
        has_active_task: status.has_active_task,
        active_task: status.active_task
      });
      
      // Check if there's an active task and its status
      const hasActiveTask = status.has_active_task;
      const activeTask = status.active_task;
      
      if (!hasActiveTask || !activeTask) {
        return { isRunning: false, taskInfo: null };
      }
      
      // Check if the active task is in a "running" state
      const runningStates = ['running', 'submitted', 'paused'];
      const taskStatus = activeTask.status || '';
      const isRunning = runningStates.includes(taskStatus.toLowerCase());
      
      console.log('[API] Task running check:', {
        taskStatus,
        isRunning,
        runningStates
      });
      
      return {
        isRunning,
        taskInfo: hasActiveTask ? activeTask : null
      };
    } catch (error) {
      console.error('[API] Failed to check task status:', error);
      return { isRunning: false, taskInfo: null };
    }
  }

  async pauseTask(reason = 'User requested pause') {
    return this.post('/tasks/pause', { reason });
  }

  async resumeTask(reason = 'User requested resume') {
    return this.post('/tasks/resume', { reason });
  }

  async stopTask(reason = 'User requested stop') {
    return this.post('/tasks/stop', { reason });
  }

  async addNewTask(newTask) {
    return this.post('/tasks/add-new-task', { reason: newTask });
  }

  // Activity APIs
  async getTaskInfo(taskId) {
    return this.get(`/activity/${taskId}`);
  }

  async getSessionTasks(sessionId) {
    return this.get(`/activity/sessions/${sessionId}/tasks`);
  }

  async getSessionActivity(sessionId, params = {}) {
    return this.get(`/activity/sessions/${sessionId}/activity`, { params });
  }

  async getLatestActivity(sessionId) {
    return this.get(`/activity/sessions/${sessionId}/latest_activity`);
  }

  async getRecentTasks(limit = -1) {
    return this.get('/activity/tasks', { params: { limit } });
  }

  async getAllSessions(limit = -1, offset = 0) {
    return this.get('/activity/sessions', { params: { limit, offset } });
  }

  // Real-time activity polling
  async pollSessionActivity(sessionId, messageIndex = null, interval = 1000) {
    const params = messageIndex !== null ? { message_index: messageIndex } : {};
    
    console.log(`[API] Polling session activity:`, {
      sessionId,
      messageIndex,
      params,
      url: `/activity/sessions/${sessionId}/activity`
    });
    
    try {
      const response = await this.getSessionActivity(sessionId, params);
      console.log(`[API] Poll response:`, {
        hasActivityLog: !!response.activity_log,
        messageIndex: response.message_index,
        agentName: response.activity_log?.agent_name,
        agentStatus: response.activity_log?.agent_status
      });
      return response;
    } catch (error) {
      console.error('[API] Activity polling error:', error);
      throw error;
    }
  }

  // File Management APIs
  async uploadFiles(files, sessionId = null) {
    const formData = new FormData();
    
    // Add files
    for (const file of files) {
      formData.append('files', file);
    }
    
    // Add session ID if provided
    if (sessionId) {
      formData.append('session_id', sessionId);
    }

    return this.post('/files/upload', formData);
  }

  async listFiles(sessionId = null, limit = 50, offset = 0) {
    const params = { limit, offset };
    if (sessionId) {
      params.session_id = sessionId;
    }
    
    return this.get('/files', { params });
  }

  async downloadFile(fileId) {
    const url = this.buildURL(`/files/${fileId}`);
    
    try {
      const response = await fetch(url);
      if (!response.ok) {
        throw new Error(`Failed to download file: ${response.status}`);
      }
      
      return response.blob();
    } catch (error) {
      console.error('[API] File download error:', error);
      throw error;
    }
  }

  async deleteFile(fileId) {
    return this.delete(`/files/${fileId}`);
  }

  // Configuration APIs
  async getConfigStatus() {
    return this.get('/config/status');
  }

  // LLM Profile Management
  async getLLMProfiles(activeOnly = true, limit = 50, offset = 0) {
    return this.get('/config/llm-profiles', {
      params: { active_only: activeOnly, limit, offset }
    });
  }

  async getLLMProfile(profileName) {
    return this.get(`/config/llm-profiles/${encodeURIComponent(profileName)}`);
  }

  async createLLMProfile(profileData) {
    return this.post('/config/llm-profiles', profileData);
  }

  async updateLLMProfile(profileName, updateData) {
    return this.put(`/config/llm-profiles/${encodeURIComponent(profileName)}`, updateData);
  }

  async deleteLLMProfile(profileName) {
    return this.delete(`/config/llm-profiles/${encodeURIComponent(profileName)}`);
  }

  async setDefaultLLMProfile(profileName) {
    return this.post(`/config/llm-profiles/${encodeURIComponent(profileName)}/set-default`);
  }

  // MCP Profile Management
  async getMCPProfiles(activeOnly = true, limit = 50, offset = 0) {
    return this.get('/config/mcp-profiles', {
      params: { active_only: activeOnly, limit, offset }
    });
  }

  async getMCPProfile(profileName) {
    return this.get(`/config/mcp-profiles/${encodeURIComponent(profileName)}`);
  }

  async createMCPProfile(profileData) {
    return this.post('/config/mcp-profiles', profileData);
  }

  async updateMCPProfile(profileName, updateData) {
    console.log('[API Client] updateMCPProfile called with profile:', profileName);
    const result = await this.put(`/config/mcp-profiles/${encodeURIComponent(profileName)}`, updateData);
    return result;
  }

  async deleteMCPProfile(profileName) {
    return this.delete(`/config/mcp-profiles/${encodeURIComponent(profileName)}`);
  }

  // LLM Providers and Models
  async getLLMProviders() {
    return this.get('/config/llm/providers');
  }

  async getLLMProviderModels(providerName) {
    return this.get(`/config/llm/providers/${encodeURIComponent(providerName)}/models`);
  }

  // Voice Profile Management
  async getVoiceProfiles(activeOnly = true, voiceModelType = null, limit = 50, offset = 0) {
    const params = { active_only: activeOnly, limit, offset };
    if (voiceModelType) {
      params.voice_model_type = voiceModelType;
    }
    return this.get('/voices/voice-profiles', { params });
  }

  async getVoiceProfile(profileName) {
    return this.get(`/voices/${encodeURIComponent(profileName)}`);
  }

  async createVoiceProfile(profileData) {
    return this.post('/voices/voice-profiles', profileData);
  }

  async updateVoiceProfile(profileName, updateData) {
    return this.put(`/voices/voice-profiles/${encodeURIComponent(profileName)}`, updateData);
  }

  async deleteVoiceProfile(profileName) {
    return this.delete(`/voices/voice-profiles/${encodeURIComponent(profileName)}`);
  }

  // Voice Models - matches the backend route @router.get("/models")
  async getVoiceModels(modelType = null) {
    let url = '/voices/models';
    if (modelType) {
      url += `?model_type=${encodeURIComponent(modelType)}`;
    }
    return this.get(url);
  }

  // Voice Recording API
  async transcribeAudio(audioBlob, voiceProfileName = null) {
    const formData = new FormData();
    formData.append('audio_file', audioBlob, 'recording.webm');
    
    // Add voice profile name if provided
    const params = {};
    if (voiceProfileName) {
      params.voice_profile_name = voiceProfileName;
    }
    
    return this.post('/voices/asr', formData, {
      params,
      headers: {} // Let browser set Content-Type with boundary for FormData
    });
  }

  // Get available ASR profiles
  async getASRProfiles(activeOnly = true) {
    return this.get('/voices/voice-profiles', {
      params: {
        voice_model_type: 'asr',
        active_only: activeOnly
      }
    });
  }

  // Environment Variables
  async getEnvironmentVariables() {
    return this.get('/config/environments');
  }

  async updateEnvironmentVariables(variables) {
    return this.put('/config/environments', { environments: variables });
  }

  // Controller Configuration
  async getControllerConfig() {
    return this.get('/config/controller');
  }

  async updateControllerConfig(configData) {
    return this.put('/config/controller', configData);
  }

  // Browser APIs
  async getActiveBrowserTab() {
    return this.get('/browser/active-tab');
  }

  async getAllBrowserTabs() {
    return this.get('/browser/all-tabs');
  }

  // Agent APIs - Get available skills
  async getAllSkills() {
    return this.get('/agent/get_all_skills');
  }

  // Utility methods
  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  // Update base URL
  setBaseURL(newBaseURL) {
    this.baseURL = newBaseURL.replace(/\/$/, '');
    console.log('[API] Base URL updated to:', this.baseURL);
  }

  // Create a session ID
  // Session ID generation using backend endpoint with fallback
  async generateSessionId(prefix = 'vibesurf_') {
    try {
      // Use backend endpoint for session ID generation
      const response = await fetch(`${this.baseURL}/generate-session-id?prefix=${encodeURIComponent(prefix)}`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        },
        signal: AbortSignal.timeout(5000)
      });
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`);
      }
      
      const data = await response.json();
      return data.session_id;
    } catch (error) {
      console.warn('[API] Failed to generate session ID from backend, using fallback:', error);
      // Fallback to simple local generation
      const timestamp = Date.now();
      const random = Math.random().toString(36).substr(2, 9);
      return `${prefix}${timestamp}_${random}`;
    }
  }
}

// Custom error class for API errors
class APIError extends Error {
  constructor(message, status, data) {
    super(message);
    this.name = 'APIError';
    this.status = status;
    this.data = data;
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfAPIClient = VibeSurfAPIClient;
  window.APIError = APIError;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/file-manager.js
================================================
// File Manager - Handles file uploads, file links, and file operations
// Manages file selection, upload, display, and file:// protocol handling

class VibeSurfFileManager {
  constructor(sessionManager) {
    this.sessionManager = sessionManager;
    this.state = {
      uploadedFiles: [],
      isHandlingFileLink: false
    };
    this.elements = {};
    this.eventListeners = new Map();
    
    this.bindElements();
    this.bindEvents();
  }

  bindElements() {
    this.elements = {
      // File input and buttons
      attachFileBtn: document.getElementById('attach-file-btn'),
      fileInput: document.getElementById('file-input'),
      
      // File list container (created dynamically)
      uploadedFilesList: null
    };
  }

  bindEvents() {
    // File attachment
    this.elements.attachFileBtn?.addEventListener('click', this.handleAttachFiles.bind(this));
    this.elements.fileInput?.addEventListener('change', this.handleFileSelection.bind(this));
    
    // File link handling with delegation
    document.addEventListener('click', this.handleFileLinkClick.bind(this));
    
    // Listen for files uploaded event from session manager
    if (this.sessionManager) {
      this.sessionManager.on('filesUploaded', this.handleFilesUploaded.bind(this));
    }
  }

  // Event system for communicating with main UI manager
  on(event, callback) {
    if (!this.eventListeners.has(event)) {
      this.eventListeners.set(event, []);
    }
    this.eventListeners.get(event).push(callback);
  }

  emit(event, data) {
    if (this.eventListeners.has(event)) {
      this.eventListeners.get(event).forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error(`[FileManager] Event callback error for ${event}:`, error);
        }
      });
    }
  }

  // File Upload Handling
  handleAttachFiles() {
    this.elements.fileInput?.click();
  }

  async handleFileSelection(event) {
    const files = Array.from(event.target.files);
    
    if (files.length === 0) return;
    
    try {
      this.emit('loading', { message: `Uploading ${files.length} file(s)...` });
      
      const response = await this.sessionManager.uploadFiles(files);
      
      console.log('[FileManager] File upload response:', response);
      
      // SessionManager will emit 'filesUploaded' event, no need to handle manually
      // Remove duplicate handling that was causing files to appear twice
      
      this.emit('loading', { hide: true });
      this.emit('notification', {
        message: `${files.length} file(s) uploaded successfully`,
        type: 'success'
      });
      
      // Clear file input
      event.target.value = '';
    } catch (error) {
      this.emit('loading', { hide: true });
      this.emit('notification', {
        message: `File upload failed: ${error.message}`,
        type: 'error'
      });
    }
  }

  handleFilesUploaded(data) {
    console.log('[FileManager] Files uploaded event received:', data);
    
    // Ensure data.files is always an array - handle both single file and array cases
    let filesArray = [];
    if (data.files) {
      if (Array.isArray(data.files)) {
        filesArray = data.files;
      } else {
        // If single file object, wrap in array
        filesArray = [data.files];
        console.log('[FileManager] Single file detected, wrapping in array');
      }
    }
    
    console.log('[FileManager] Processing files array:', filesArray);
    
    if (filesArray.length > 0) {
      // Append new files to existing uploaded files (for multiple uploads)
      const newFiles = filesArray.map(file => ({
        id: file.file_id,
        name: file.original_filename,
        path: file.file_path,  // Updated to use file_path field
        size: file.file_size,
        type: file.mime_type,
        stored_filename: file.stored_filename,
        file_path: file.file_path  // Add file_path for backward compatibility
      }));
      
      console.log('[FileManager] Mapped new files:', newFiles);
      
      // Add to existing files instead of replacing
      this.state.uploadedFiles = [...this.state.uploadedFiles, ...newFiles];
      
      console.log('[FileManager] Updated uploaded files state:', this.state.uploadedFiles);
      
      // Update the visual file list
      this.updateFilesList();
    } else {
      console.warn('[FileManager] No files to process in uploaded data');
    }
  }

  // File List Management
  updateFilesList() {
    const container = this.getOrCreateFilesListContainer();
    
    // Debug logging to identify the issue
    console.log('[FileManager] updateFilesList called');
    console.log('[FileManager] uploadedFiles type:', typeof this.state.uploadedFiles);
    console.log('[FileManager] uploadedFiles isArray:', Array.isArray(this.state.uploadedFiles));
    console.log('[FileManager] uploadedFiles value:', this.state.uploadedFiles);
    
    // Ensure uploadedFiles is always an array
    if (!Array.isArray(this.state.uploadedFiles)) {
      console.error('[FileManager] uploadedFiles is not an array, resetting to empty array');
      this.state.uploadedFiles = [];
    }
    
    if (this.state.uploadedFiles.length === 0) {
      container.style.display = 'none';
      return;
    }
    
    container.style.display = 'block';
    
    // Build HTML safely with proper validation
    let filesHTML = '';
    try {
      filesHTML = this.state.uploadedFiles.map((file, index) => {
        console.log(`[FileManager] Processing file ${index}:`, file);
        
        // Validate file object structure
        if (!file || typeof file !== 'object') {
          console.error(`[FileManager] Invalid file object at index ${index}:`, file);
          return '';
        }
        
        // Extract properties safely with fallbacks
        const fileId = file.id || file.file_id || `file_${index}`;
        const fileName = file.name || file.original_filename || 'Unknown file';
        const filePath = file.path || file.file_path || file.stored_filename || 'Unknown path';
        
        console.log(`[FileManager] File display data: id=${fileId}, name=${fileName}, path=${filePath}`);
        
        return `
          <div class="file-item" data-file-id="${fileId}">
            <span class="file-name" title="${filePath}">${fileName}</span>
            <button class="file-remove-btn" title="Remove file" data-file-id="${fileId}">Ã—</button>
          </div>
        `;
      }).join('');
    } catch (error) {
      console.error('[FileManager] Error generating files HTML:', error);
      filesHTML = '<div class="error-message">Error displaying files</div>';
    }
    
    container.innerHTML = `
      <div class="files-items">
        ${filesHTML}
      </div>
    `;
    
    // Add event listeners for remove buttons
    container.querySelectorAll('.file-remove-btn').forEach(btn => {
      btn.addEventListener('click', (e) => {
        e.preventDefault();
        const fileId = btn.dataset.fileId;
        this.removeUploadedFile(fileId);
      });
    });
  }

  getOrCreateFilesListContainer() {
    let container = document.getElementById('uploaded-files-list');
    
    if (!container) {
      container = document.createElement('div');
      container.id = 'uploaded-files-list';
      container.className = 'uploaded-files-container';
      
      // Insert after the textarea-container to avoid affecting button layout
      const taskInput = document.getElementById('task-input');
      if (taskInput) {
        const textareaContainer = taskInput.closest('.textarea-container');
        if (textareaContainer && textareaContainer.parentElement) {
          // Insert after the textarea-container but before the input-footer
          const inputFooter = textareaContainer.parentElement.querySelector('.input-footer');
          if (inputFooter) {
            textareaContainer.parentElement.insertBefore(container, inputFooter);
          } else {
            textareaContainer.parentElement.insertBefore(container, textareaContainer.nextSibling);
          }
        }
      }
    }
    
    return container;
  }

  removeUploadedFile(fileId) {
    console.log('[FileManager] Removing uploaded file:', fileId);
    
    // Remove from state
    this.state.uploadedFiles = this.state.uploadedFiles.filter(file => file.id !== fileId);
    
    // Update visual list
    this.updateFilesList();
    
    this.emit('notification', {
      message: 'File removed from upload list',
      type: 'info'
    });
  }

  clearUploadedFiles() {
    this.state.uploadedFiles = [];
    this.updateFilesList();
  }

  // File Link Handling
  handleFileLinkClick(event) {
    const target = event.target;
    
    // Check if clicked element is a file link
    if (target.matches('a.file-link') || target.closest('a.file-link')) {
      event.preventDefault();
      
      const fileLink = target.matches('a.file-link') ? target : target.closest('a.file-link');
      const filePath = fileLink.getAttribute('data-file-path');
      
      this.handleFileLink(filePath);
    }
  }

  async handleFileLink(filePath) {
    // Prevent multiple simultaneous calls
    if (this.state.isHandlingFileLink) {
      console.log('[FileManager] File link handling already in progress, skipping...');
      return;
    }
    
    this.state.isHandlingFileLink = true;
    
    try {
      console.log('[FileManager] Handling file link:', filePath);
      
      // Validate input
      if (!filePath || typeof filePath !== 'string') {
        throw new Error('Invalid file path provided');
      }
      
      // First decode the URL-encoded path safely
      let decodedPath;
      try {
        decodedPath = decodeURIComponent(filePath);
      } catch (decodeError) {
        console.warn('[FileManager] Failed to decode URL, using original path:', decodeError);
        decodedPath = filePath;
      }
      
      // Remove file:// protocol prefix and normalize
      let cleanPath = decodedPath.replace(/^file:\/\/\//, '').replace(/^file:\/\//, '');
      
      // Ensure path starts with / for Unix paths if not Windows drive
      if (!cleanPath.startsWith('/') && !cleanPath.match(/^[A-Za-z]:/)) {
        cleanPath = '/' + cleanPath;
      }
      
      // Convert all backslashes to forward slashes
      cleanPath = cleanPath.replace(/\\/g, '/');
      
      // Create proper file URL - always use triple slash for proper format
      const fileUrl = cleanPath.match(/^[A-Za-z]:/) ?
        `file:///${cleanPath}` :
        `file:///${cleanPath.replace(/^\//, '')}`;  // Remove leading slash and add triple slash
      
      console.log('[FileManager] Processed file URL:', fileUrl);
      
      // Show user notification about the action
      this.emit('notification', {
        message: `Opening file: ${cleanPath}`,
        type: 'info'
      });
      
      // Use setTimeout to prevent UI blocking and ensure proper cleanup
      setTimeout(async () => {
        try {
          // For user-clicked file links, use OPEN_FILE_URL to keep tab open
          // This prevents the auto-close behavior in OPEN_FILE_SYSTEM
          const fileOpenResponse = await Promise.race([
            chrome.runtime.sendMessage({
              type: 'OPEN_FILE_URL',
              data: { fileUrl: fileUrl }
            }),
            new Promise((_, reject) =>
              setTimeout(() => reject(new Error('File open timeout')), 5000)
            )
          ]);
          
          if (fileOpenResponse && fileOpenResponse.success) {
            this.emit('notification', {
              message: 'File opened in browser tab',
              type: 'success'
            });
            return;
          } else if (fileOpenResponse && fileOpenResponse.error) {
            console.warn('[FileManager] Background script file open failed:', fileOpenResponse.error);
          }
          
          // If OPEN_FILE_URL fails, try direct browser open with additional safety
          try {
            const opened = window.open(fileUrl, '_blank', 'noopener,noreferrer');
            if (opened && !opened.closed) {
              this.emit('notification', {
                message: 'File opened in browser',
                type: 'success'
              });
              return;
            }
          } catch (browserError) {
            console.error('[FileManager] Browser open failed:', browserError);
          }
          
          // Last resort: Copy path to clipboard
          await this.copyToClipboardFallback(fileUrl);
          
        } catch (error) {
          console.error('[FileManager] Error in async file handling:', error);
          
          // Provide more helpful error messages
          let userMessage = 'Unable to open file';
          if (error.message.includes('timeout')) {
            userMessage = 'File open operation timed out. Try copying the path manually.';
          } else if (error.message.includes('protocol')) {
            userMessage = 'Browser security restricts opening local files. File path copied to clipboard.';
          } else {
            userMessage = `Unable to open file: ${error.message}`;
          }
          
          this.emit('notification', {
            message: userMessage,
            type: 'error'
          });
          
          // Fallback to clipboard
          try {
            await this.copyToClipboardFallback(fileUrl);
          } catch (clipboardError) {
            console.error('[FileManager] Clipboard fallback also failed:', clipboardError);
          }
        } finally {
          this.state.isHandlingFileLink = false;
        }
      }, 50); // Small delay to prevent UI blocking
      
    } catch (error) {
      console.error('[FileManager] Error handling file link:', error);
      this.emit('notification', {
        message: `File link processing failed: ${error.message}`,
        type: 'error'
      });
      this.state.isHandlingFileLink = false;
    }
  }
  

  async copyToClipboardFallback(fileUrl) {
    try {
      await navigator.clipboard.writeText(fileUrl);
      this.emit('notification', {
        message: 'File URL copied to clipboard - paste in browser address bar',
        type: 'info'
      });
    } catch (clipboardError) {
      console.error('[FileManager] Clipboard failed:', clipboardError);
      this.emit('notification', {
        message: 'Unable to open file. URL: ' + fileUrl,
        type: 'warning'
      });
    }
  }

  // Utility Methods
  formatFileSize(bytes) {
    if (!bytes) return '0 B';
    
    const k = 1024;
    const sizes = ['B', 'KB', 'MB', 'GB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    
    return parseFloat((bytes / Math.pow(k, i)).toFixed(1)) + ' ' + sizes[i];
  }

  escapeHtml(text) {
    if (typeof text !== 'string') return '';
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  }

  // Public interface
  getUploadedFiles() {
    return [...this.state.uploadedFiles]; // Return copy
  }

  getUploadedFilesForTask() {
    if (this.state.uploadedFiles.length === 0) {
      return null;
    }

    console.log('[FileManager] Raw uploaded files state:', this.state.uploadedFiles);
    
    // Extract the first file path (backend expects single string)
    const firstFile = this.state.uploadedFiles[0];
    let filePath = null;
    
    if (typeof firstFile === 'string') {
      filePath = firstFile;
    } else if (firstFile && typeof firstFile === 'object') {
      // Extract path and normalize
      filePath = firstFile.file_path || firstFile.path || firstFile.stored_filename || firstFile.file_path;
      if (filePath) {
        filePath = filePath.replace(/\\/g, '/');
        console.log('[FileManager] Normalized file path:', filePath);
      }
    }
    
    if (filePath) {
      // Show info if multiple files uploaded but only first will be processed
      if (this.state.uploadedFiles.length > 1) {
        console.warn('[FileManager] Multiple files uploaded, but backend only supports single file. Using first file:', filePath);
        this.emit('notification', {
          message: `Multiple files uploaded. Only the first file "${firstFile.name || filePath}" will be processed.`,
          type: 'warning'
        });
      }
      
      return filePath;
    } else {
      console.error('[FileManager] Could not extract file path from uploaded file:', firstFile);
      return null;
    }
  }

  hasUploadedFiles() {
    return this.state.uploadedFiles.length > 0;
  }

  // Enable/disable file attachment based on task running state
  setEnabled(enabled) {
    if (this.elements.attachFileBtn) {
      this.elements.attachFileBtn.disabled = !enabled;
      
      if (enabled) {
        this.elements.attachFileBtn.classList.remove('task-running-disabled');
        this.elements.attachFileBtn.removeAttribute('title');
      } else {
        this.elements.attachFileBtn.classList.add('task-running-disabled');
        this.elements.attachFileBtn.setAttribute('title', 'Disabled while task is running');
      }
    }
  }

  // Get current state
  getState() {
    return { ...this.state };
  }

  // Update session manager reference
  setSessionManager(sessionManager) {
    this.sessionManager = sessionManager;
    
    // Re-bind files uploaded event
    if (this.sessionManager) {
      this.sessionManager.on('filesUploaded', this.handleFilesUploaded.bind(this));
    }
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfFileManager = VibeSurfFileManager;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/history-manager.js
================================================
// History Manager - Handles history modal, recent tasks, and session history
// Manages history display, pagination, search, and filtering

class VibeSurfHistoryManager {
  constructor(apiClient) {
    this.apiClient = apiClient;
    this.state = {
      historyMode: 'recent', // 'recent' or 'all'
      currentPage: 1,
      totalPages: 1,
      pageSize: 10,
      searchQuery: '',
      statusFilter: 'all',
      recentTasks: [],
      allSessions: []
    };
    this.elements = {};
    this.eventListeners = new Map();
    
    this.bindElements();
    this.bindEvents();
  }

  bindElements() {
    this.elements = {
      // History Modal
      historyModal: document.getElementById('history-modal'),
      
      // Recent Tasks Section
      recentTasksList: document.getElementById('recent-tasks-list'),
      viewMoreTasksBtn: document.getElementById('view-more-tasks-btn'),
      
      // All Sessions Section
      allSessionsSection: document.getElementById('all-sessions-section'),
      backToRecentBtn: document.getElementById('back-to-recent-btn'),
      sessionSearch: document.getElementById('session-search'),
      sessionFilter: document.getElementById('session-filter'),
      allSessionsList: document.getElementById('all-sessions-list'),
      
      // Pagination
      prevPageBtn: document.getElementById('prev-page-btn'),
      nextPageBtn: document.getElementById('next-page-btn'),
      pageInfo: document.getElementById('page-info')
    };
  }

  bindEvents() {
    // History modal close button
    const historyModalClose = this.elements.historyModal?.querySelector('.modal-close');
    if (historyModalClose) {
      historyModalClose.addEventListener('click', this.hideModal.bind(this));
    }
    
    // History modal overlay click to close
    const historyModalOverlay = this.elements.historyModal?.querySelector('.modal-overlay');
    if (historyModalOverlay) {
      historyModalOverlay.addEventListener('click', (event) => {
        if (event.target === historyModalOverlay) {
          this.hideModal();
        }
      });
    }
    
    // View More Tasks button
    this.elements.viewMoreTasksBtn?.addEventListener('click', this.handleViewMoreTasks.bind(this));
    
    // Back to Recent button
    this.elements.backToRecentBtn?.addEventListener('click', this.handleBackToRecent.bind(this));
    
    // Search and filter
    this.elements.sessionSearch?.addEventListener('input', this.handleSessionSearch.bind(this));
    this.elements.sessionFilter?.addEventListener('change', this.handleSessionFilter.bind(this));
    
    // Pagination
    this.elements.prevPageBtn?.addEventListener('click', this.handlePrevPage.bind(this));
    this.elements.nextPageBtn?.addEventListener('click', this.handleNextPage.bind(this));
    
    // Global keyboard shortcuts for this modal
    document.addEventListener('keydown', this.handleKeydown.bind(this));
  }

  handleKeydown(event) {
    // Close history modal on Escape key
    if (event.key === 'Escape') {
      if (this.elements.historyModal && !this.elements.historyModal.classList.contains('hidden')) {
        this.hideModal();
      }
    }
  }

  // Event system for communicating with main UI manager
  on(event, callback) {
    if (!this.eventListeners.has(event)) {
      this.eventListeners.set(event, []);
    }
    this.eventListeners.get(event).push(callback);
  }

  emit(event, data) {
    if (this.eventListeners.has(event)) {
      this.eventListeners.get(event).forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error(`[HistoryManager] Event callback error for ${event}:`, error);
        }
      });
    }
  }

  // Public interface for showing history
  async showHistory() {
    try {
      this.emit('loading', { message: 'Loading recent tasks...' });
      
      // Reset to recent tasks view
      this.state.historyMode = 'recent';
      await this.loadRecentTasks();
      this.displayHistoryModal();
      
      this.emit('loading', { hide: true });
    } catch (error) {
      this.emit('loading', { hide: true });
      this.emit('error', { message: `Failed to load history: ${error.message}` });
    }
  }

  // Event Handlers
  async handleViewMoreTasks() {
    try {
      console.log('[HistoryManager] View More Tasks clicked');
      this.emit('loading', { message: 'Loading all sessions...' });
      
      // Switch to all sessions view
      this.state.historyMode = 'all';
      console.log('[HistoryManager] Set history mode to "all"');
      
      await this.loadAllSessions();
      console.log('[HistoryManager] All sessions loaded, switching view');
      
      this.displayAllSessionsView();
      console.log('[HistoryManager] All sessions view displayed');
      
      this.emit('loading', { hide: true });
    } catch (error) {
      this.emit('loading', { hide: true });
      console.error('[HistoryManager] Error in handleViewMoreTasks:', error);
      this.emit('error', { message: `Failed to load sessions: ${error.message}` });
    }
  }

  handleBackToRecent() {
    this.state.historyMode = 'recent';
    this.displayRecentTasksView();
  }

  handleSessionSearch(event) {
    this.state.searchQuery = event.target.value.trim().toLowerCase();
    this.filterAndDisplaySessions();
  }

  handleSessionFilter(event) {
    this.state.statusFilter = event.target.value;
    this.filterAndDisplaySessions();
  }

  handlePrevPage() {
    if (this.state.currentPage > 1) {
      this.state.currentPage--;
      this.filterAndDisplaySessions();
    }
  }

  handleNextPage() {
    if (this.state.currentPage < this.state.totalPages) {
      this.state.currentPage++;
      this.filterAndDisplaySessions();
    }
  }

  // Data Loading Methods
  async loadRecentTasks() {
    try {
      console.log('[HistoryManager] Loading recent tasks...');
      const response = await this.apiClient.getRecentTasks();
      
      // Handle API response structure: { tasks: [...], total_count: ..., limit: ... }
      let tasks = [];
      if (response && response.tasks && Array.isArray(response.tasks)) {
        tasks = response.tasks;
      } else if (response && Array.isArray(response)) {
        tasks = response;
      } else if (response && response.data && Array.isArray(response.data)) {
        tasks = response.data;
      }
      
      // Take only the first 3 most recent tasks
      this.state.recentTasks = tasks.slice(0, 3);
      console.log('[HistoryManager] Recent tasks loaded:', this.state.recentTasks.length);
      
      return this.state.recentTasks;
    } catch (error) {
      console.error('[HistoryManager] Failed to load recent tasks:', error);
      this.state.recentTasks = [];
      throw error;
    }
  }

  async loadAllSessions() {
    try {
      console.log('[HistoryManager] Loading all sessions...');
      const response = await this.apiClient.getAllSessions();
      
      // Handle API response structure: { sessions: [...], total_count: ..., limit: ..., offset: ... }
      let sessions = [];
      if (response && response.sessions && Array.isArray(response.sessions)) {
        sessions = response.sessions;
      } else if (response && Array.isArray(response)) {
        sessions = response;
      } else if (response && response.data && Array.isArray(response.data)) {
        sessions = response.data;
      }
      
      this.state.allSessions = sessions;
      console.log('[HistoryManager] All sessions loaded:', this.state.allSessions.length);
      
      return this.state.allSessions;
    } catch (error) {
      console.error('[HistoryManager] Failed to load all sessions:', error);
      this.state.allSessions = [];
      throw error;
    }
  }

  // Display Methods
  displayHistoryModal() {
    if (this.state.historyMode === 'recent') {
      this.displayRecentTasksView();
    } else {
      this.displayAllSessionsView();
    }
    this.showModal();
  }

  displayRecentTasksView() {
    console.log('[HistoryManager] Switching to recent tasks view');
    
    // Show recent tasks section and hide all sessions section
    if (this.elements.recentTasksList && this.elements.allSessionsSection) {
      const recentParent = this.elements.recentTasksList.parentElement;
      if (recentParent) {
        recentParent.classList.remove('hidden');
        recentParent.style.display = 'block';
        console.log('[HistoryManager] Showed recent tasks section');
      }
      this.elements.allSessionsSection.classList.add('hidden');
      this.elements.allSessionsSection.style.display = 'none';
      console.log('[HistoryManager] Hidden all sessions section');
    }
    
    this.renderRecentTasks();
  }

  displayAllSessionsView() {
    console.log('[HistoryManager] Switching to all sessions view');
    console.log('[HistoryManager] Elements check:', {
      recentTasksList: !!this.elements.recentTasksList,
      allSessionsSection: !!this.elements.allSessionsSection,
      recentTasksParent: !!this.elements.recentTasksList?.parentElement
    });
    
    // Hide recent tasks section and show all sessions section
    if (this.elements.recentTasksList && this.elements.allSessionsSection) {
      const recentParent = this.elements.recentTasksList.parentElement;
      if (recentParent) {
        recentParent.style.display = 'none';
        recentParent.classList.add('hidden');
        console.log('[HistoryManager] Hidden recent tasks section');
      }
      
      // Remove hidden class and set display block
      this.elements.allSessionsSection.classList.remove('hidden');
      this.elements.allSessionsSection.style.display = 'block';
      console.log('[HistoryManager] Showed all sessions section - removed hidden class and set display block');
      
      // Debug: Check computed styles
      const computedStyle = window.getComputedStyle(this.elements.allSessionsSection);
      console.log('[HistoryManager] All sessions section computed display:', computedStyle.display);
      console.log('[HistoryManager] All sessions section classList:', this.elements.allSessionsSection.classList.toString());
      
    } else {
      console.error('[HistoryManager] Missing elements for view switching:', {
        recentTasksList: !!this.elements.recentTasksList,
        allSessionsSection: !!this.elements.allSessionsSection
      });
    }
    
    // Reset search and filter
    this.state.currentPage = 1;
    this.state.searchQuery = '';
    this.state.statusFilter = 'all';
    
    if (this.elements.sessionSearch) {
      this.elements.sessionSearch.value = '';
    }
    if (this.elements.sessionFilter) {
      this.elements.sessionFilter.value = 'all';
    }
    
    console.log('[HistoryManager] About to filter and display sessions');
    this.filterAndDisplaySessions();
  }

  renderRecentTasks() {
    if (!this.elements.recentTasksList) return;
    
    this.elements.recentTasksList.innerHTML = '';
    
    if (this.state.recentTasks.length === 0) {
      this.elements.recentTasksList.innerHTML = `
        <div class="empty-state">
          <div class="empty-state-icon">ğŸ“</div>
          <div class="empty-state-title">No Recent Tasks</div>
          <div class="empty-state-description">Start a new task to see it here.</div>
        </div>
      `;
      return;
    }
    
    this.state.recentTasks.forEach(task => {
      const taskItem = this.createTaskItem(task);
      this.elements.recentTasksList.appendChild(taskItem);
    });
  }

  filterAndDisplaySessions() {
    if (!this.elements.allSessionsList) {
      console.error('[HistoryManager] allSessionsList element not found');
      return;
    }
    
    console.log('[HistoryManager] Filtering sessions. Total sessions:', this.state.allSessions.length);
    console.log('[HistoryManager] Search query:', this.state.searchQuery);
    console.log('[HistoryManager] Status filter:', this.state.statusFilter);
    
    let filteredSessions = [...this.state.allSessions]; // Create copy
    
    // Apply search filter
    if (this.state.searchQuery) {
      filteredSessions = filteredSessions.filter(session =>
        session.session_id.toLowerCase().includes(this.state.searchQuery) ||
        (session.description && session.description.toLowerCase().includes(this.state.searchQuery))
      );
    }
    
    // Apply status filter
    if (this.state.statusFilter !== 'all') {
      filteredSessions = filteredSessions.filter(session =>
        (session.status || 'active').toLowerCase() === this.state.statusFilter.toLowerCase()
      );
    }
    
    console.log('[HistoryManager] Filtered sessions count:', filteredSessions.length);
    
    // Calculate pagination
    const totalSessions = filteredSessions.length;
    this.state.totalPages = Math.ceil(totalSessions / this.state.pageSize);
    
    // Ensure current page is valid
    if (this.state.currentPage > this.state.totalPages) {
      this.state.currentPage = Math.max(1, this.state.totalPages);
    }
    
    // Get sessions for current page
    const startIndex = (this.state.currentPage - 1) * this.state.pageSize;
    const endIndex = startIndex + this.state.pageSize;
    const paginatedSessions = filteredSessions.slice(startIndex, endIndex);
    
    console.log('[HistoryManager] Paginated sessions for display:', paginatedSessions.length);
    
    // Render sessions
    this.renderSessionsList(paginatedSessions);
    
    // Update pagination controls
    this.updatePaginationControls();
  }

  renderSessionsList(sessions) {
    if (!this.elements.allSessionsList) {
      console.error('[HistoryManager] allSessionsList element not found for rendering');
      return;
    }
    
    console.log('[HistoryManager] Rendering sessions list with', sessions.length, 'sessions');
    
    this.elements.allSessionsList.innerHTML = '';
    
    if (sessions.length === 0) {
      console.log('[HistoryManager] No sessions to display, showing empty state');
      this.elements.allSessionsList.innerHTML = `
        <div class="empty-state">
          <div class="empty-state-icon">ğŸ”</div>
          <div class="empty-state-title">No Sessions Found</div>
          <div class="empty-state-description">Try adjusting your search or filter criteria.</div>
        </div>
      `;
      return;
    }
    
    sessions.forEach((session, index) => {
      console.log(`[HistoryManager] Creating session item ${index + 1}:`, session.session_id);
      const sessionItem = this.createSessionItem(session);
      this.elements.allSessionsList.appendChild(sessionItem);
    });
    
    console.log('[HistoryManager] Sessions list rendered successfully');
  }

  updatePaginationControls() {
    // Update pagination buttons
    if (this.elements.prevPageBtn) {
      this.elements.prevPageBtn.disabled = this.state.currentPage <= 1;
    }
    
    if (this.elements.nextPageBtn) {
      this.elements.nextPageBtn.disabled = this.state.currentPage >= this.state.totalPages;
    }
    
    // Update page info
    if (this.elements.pageInfo) {
      if (this.state.totalPages === 0) {
        this.elements.pageInfo.textContent = 'No results';
      } else {
        this.elements.pageInfo.textContent = `Page ${this.state.currentPage} of ${this.state.totalPages}`;
      }
    }
  }

  // Item Creation Methods
  createTaskItem(task) {
    const item = document.createElement('div');
    item.className = 'recent-task-item';
    
    const sessionId = task.session_id || 'Unknown';
    const taskDesc = task.description || task.task_description || 'No description';
    const timestamp = new Date(task.created_at || task.timestamp || Date.now()).toLocaleString();
    const status = task.status || 'completed';
    
    item.innerHTML = `
      <div class="task-item-header">
        <div class="task-session-id">${sessionId}</div>
        <div class="task-timestamp">${timestamp}</div>
      </div>
      <div class="task-description">${this.truncateText(taskDesc, 100)}</div>
      <div class="task-status">
        <span class="status-dot ${status}"></span>
        <span class="status-text">${status}</span>
      </div>
    `;
    
    item.addEventListener('click', () => {
      this.handleTaskItemClick(task);
    });
    
    return item;
  }

  createSessionItem(session) {
    const item = document.createElement('div');
    item.className = 'session-item';
    
    const sessionId = session.session_id || 'Unknown';
    const createdAt = new Date(session.created_at || session.timestamp || Date.now()).toLocaleString();
    const lastActivity = session.last_activity ? new Date(session.last_activity).toLocaleString() : 'No activity';
    const taskCount = session.task_count || 0;
    const status = session.status || 'active';
    
    item.innerHTML = `
      <div class="session-item-header">
        <div class="session-id">${sessionId}</div>
        <div class="session-timestamp">${createdAt}</div>
      </div>
      <div class="session-details">
        <div class="session-info">
          <span class="session-task-count">${taskCount} task(s)</span>
          <span class="session-last-activity">Last: ${lastActivity}</span>
        </div>
        <div class="session-status">
          <span class="status-dot ${status}"></span>
          <span class="status-text">${status}</span>
        </div>
      </div>
    `;
    
    // Add enhanced click handler with debugging
    item.addEventListener('click', (event) => {
      event.preventDefault();
      event.stopPropagation();
      this.handleSessionItemClick(session);
    });
    
    // Add visual feedback for clickability
    item.style.cursor = 'pointer';
    item.setAttribute('title', `Click to load session: ${sessionId}`);
    
    return item;
  }

  // Click Handlers
  async handleTaskItemClick(task) {
    try {
      console.log('[HistoryManager] Task item clicked:', task.session_id);
      const sessionId = task.session_id;
      if (!sessionId) {
        console.error('[HistoryManager] No session ID found in task data:', task);
        this.emit('error', { message: 'Invalid task - no session ID found' });
        return;
      }
      
      // Close the modal first
      this.hideModal();
      
      // Emit event to load session
      this.emit('loadSession', { sessionId });
      
    } catch (error) {
      console.error('[HistoryManager] Error in handleTaskItemClick:', error);
      this.emit('error', { message: `Failed to load task session: ${error.message}` });
    }
  }

  async handleSessionItemClick(session) {
    try {
      console.log('[HistoryManager] Session item clicked:', session.session_id);
      const sessionId = session.session_id;
      if (!sessionId) {
        console.error('[HistoryManager] No session ID found in session data:', session);
        this.emit('error', { message: 'Invalid session - no session ID found' });
        return;
      }
      
      // Close the modal first
      this.hideModal();
      
      // Emit event to load session
      this.emit('loadSession', { sessionId });
      
    } catch (error) {
      console.error('[HistoryManager] Error in handleSessionItemClick:', error);
      this.emit('error', { message: `Failed to load session: ${error.message}` });
    }
  }

  // Utility Methods
  truncateText(text, maxLength) {
    if (!text) return '';
    if (text.length <= maxLength) return text;
    return text.substring(0, maxLength) + '...';
  }

  escapeHtml(text) {
    if (typeof text !== 'string') return '';
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  }

  // Public interface
  getState() {
    return { ...this.state };
  }

  showModal() {
    if (this.elements.historyModal) {
      this.elements.historyModal.classList.remove('hidden');
      this.elements.historyModal.classList.add('scale-in');
    }
  }

  hideModal() {
    if (this.elements.historyModal) {
      this.elements.historyModal.classList.add('hidden');
      this.elements.historyModal.classList.remove('scale-in');
    }
  }

  // Reset state when modal is closed
  reset() {
    this.state.historyMode = 'recent';
    this.state.currentPage = 1;
    this.state.searchQuery = '';
    this.state.statusFilter = 'all';
    
    if (this.elements.sessionSearch) {
      this.elements.sessionSearch.value = '';
    }
    if (this.elements.sessionFilter) {
      this.elements.sessionFilter.value = 'all';
    }
  }

  // Create a specific history item (can be overridden for custom history items)
  createHistoryItem(session) {
    const item = document.createElement('div');
    item.className = 'history-item';
    
    const createdAt = new Date(session.createdAt || session.lastUpdated).toLocaleString();
    const taskCount = session.taskHistory?.length || 0;
    
    item.innerHTML = `
      <div class="history-item-header">
        <span class="history-session-id">${session.sessionId}</span>
        <span class="history-timestamp">${createdAt}</span>
      </div>
      <div class="history-task">${taskCount} task(s)</div>
      <div class="history-status">
        <span class="status-dot ${session.status || 'active'}"></span>
        ${session.status || 'active'}
      </div>
    `;
    
    item.addEventListener('click', () => {
      this.emit('loadSession', { sessionId: session.sessionId });
      this.hideModal();
    });
    
    return item;
  }

  // Display generic history list (backwards compatibility)
  displayHistoryList(sessions) {
    if (!this.elements.recentTasksList) return;
    
    this.elements.recentTasksList.innerHTML = '';
    
    if (sessions.length === 0) {
      this.elements.recentTasksList.innerHTML = `
        <div class="empty-state">
          <div class="empty-state-icon">ğŸ“</div>
          <div class="empty-state-title">No Sessions Found</div>
          <div class="empty-state-description">Create a new session to get started.</div>
        </div>
      `;
    } else {
      sessions.forEach(session => {
        const item = this.createHistoryItem(session);
        this.elements.recentTasksList.appendChild(item);
      });
    }
    
    this.showModal();
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfHistoryManager = VibeSurfHistoryManager;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/main.js
================================================
// Main Entry Point - VibeSurf Chrome Extension
// Initializes and coordinates all components of the extension

console.log('[VibeSurf] Main script starting...');

class VibeSurfApp {
  constructor() {
    this.apiClient = null;
    this.sessionManager = null;
    this.uiManager = null;
    this.settings = {};
    this.isInitialized = false;
    
    console.log('[VibeSurf] Application starting...');
  }

  async initialize() {
    try {
      console.log('[VibeSurf] Initializing application...');
      
      // Load settings from storage
      await this.loadSettings();
      
      // Initialize API client
      this.initializeAPIClient();
      
      // Check backend connectivity
      await this.checkBackendConnection();
      
      // Initialize session manager
      this.initializeSessionManager();
      
      // Initialize UI manager
      await this.initializeUIManager();
      
      // Setup global error handling
      this.setupErrorHandling();
      
      // Setup periodic health checks
      this.setupHealthChecks();
      
      this.isInitialized = true;
      
      console.log('[VibeSurf] Application initialized successfully');
      
      // Show success notification
      chrome.runtime.sendMessage({
        type: 'SHOW_NOTIFICATION',
        data: {
          title: 'VibeSurf Ready',
          message: 'Extension is ready to automate your browsing tasks!'
        }
      });
      
    } catch (error) {
      console.error('[VibeSurf] Initialization failed:', error);
      this.handleInitializationError(error);
    }
  }

  async loadSettings() {
    try {
      const result = await chrome.storage.local.get('settings');
      this.settings = result.settings || {};
      
      // Apply default settings if not present
      const defaultSettings = {
        backendUrl: 'http://localhost:9335',
        defaultSessionPrefix: 'vibesurf_',
        pollingFrequency: 1000,
        notifications: {
          enabled: true,
          taskComplete: true,
          taskError: true
        },
        ui: {
          theme: 'auto',
          autoScroll: true,
          compactMode: false
        },
        debug: false
      };
      
      this.settings = { ...defaultSettings, ...this.settings };
      
      // Save merged settings back
      await chrome.storage.local.set({ settings: this.settings });
      
      console.log('[VibeSurf] Settings loaded:', this.settings);
    } catch (error) {
      console.error('[VibeSurf] Failed to load settings:', error);
      this.settings = {};
    }
  }

  initializeAPIClient() {
    const backendUrl = this.settings.backendUrl || 'http://localhost:8000';
    this.apiClient = new VibeSurfAPIClient(backendUrl);
    
    console.log('[VibeSurf] API client initialized with URL:', backendUrl);
  }

  async checkBackendConnection() {
    try {
      console.log('[VibeSurf] Checking backend connection...');
      
      const healthCheck = await this.apiClient.healthCheck();
      
      if (healthCheck.status === 'healthy') {
        console.log('[VibeSurf] Backend connection successful');
        
        // Update badge to show connected status
        chrome.runtime.sendMessage({
          type: 'UPDATE_BADGE',
          data: { text: 'â—', color: '#28a745' }
        });
        
      } else {
        throw new Error('Backend health check failed');
      }
      
    } catch (error) {
      console.error('[VibeSurf] Backend connection failed:', error);
      
      // Update badge to show disconnected status
      chrome.runtime.sendMessage({
        type: 'UPDATE_BADGE',
        data: { text: 'â—', color: '#dc3545' }
      });
      
      // Show warning notification
      chrome.runtime.sendMessage({
        type: 'SHOW_NOTIFICATION',
        data: {
          title: 'VibeSurf Backend Disconnected',
          message: 'Cannot connect to VibeSurf backend. Please check if the server is running.'
        }
      });
      
      throw error;
    }
  }

  initializeSessionManager() {
    this.sessionManager = new VibeSurfSessionManager(this.apiClient);
    
    // Configure polling frequency from settings
    if (this.settings.pollingFrequency) {
      this.sessionManager.pollingFrequency = this.settings.pollingFrequency;
    }
    
    console.log('[VibeSurf] Session manager initialized');
  }

  async initializeUIManager() {
    this.uiManager = new VibeSurfUIManager(this.sessionManager, this.apiClient);

    // Initialize UI with loaded data
    await this.uiManager.initialize();
    
    console.log('[VibeSurf] UI manager initialized successfully');
  }

  setupErrorHandling() {
    // Global error handler for unhandled promise rejections
    window.addEventListener('unhandledrejection', (event) => {
      console.error('[VibeSurf] Unhandled promise rejection:', event.reason);
      
      if (this.settings.notifications?.enabled) {
        chrome.runtime.sendMessage({
          type: 'SHOW_NOTIFICATION',
          data: {
            title: 'VibeSurf Error',
            message: 'An unexpected error occurred. Check the console for details.'
          }
        });
      }
    });

    // Global error handler for script errors
    window.addEventListener('error', (event) => {
      console.error('[VibeSurf] Script error:', event.error);
    });

    console.log('[VibeSurf] Error handling setup complete');
  }

  setupHealthChecks() {
    // Periodic backend health check
    setInterval(async () => {
      try {
        // Check if apiClient exists and is initialized
        if (!this.apiClient || typeof this.apiClient.healthCheck !== 'function') {
          console.warn('[VibeSurf] Health check skipped - API client not available');
          return;
        }
        
        const healthCheck = await this.apiClient.healthCheck();
        
        if (healthCheck.status === 'healthy') {
          // Update badge to green if we're connected
          chrome.runtime.sendMessage({
            type: 'UPDATE_BADGE',
            data: { text: 'â—', color: '#28a745' }
          });
        } else {
          // Update badge to red if health check fails
          chrome.runtime.sendMessage({
            type: 'UPDATE_BADGE',
            data: { text: 'â—', color: '#dc3545' }
          });
        }
        
      } catch (error) {
        // Silently handle health check failures
        console.warn('[VibeSurf] Health check failed:', error.message);
        
        chrome.runtime.sendMessage({
          type: 'UPDATE_BADGE',
          data: { text: 'â—', color: '#dc3545' }
        });
      }
    }, 30000); // Check every 30 seconds

    console.log('[VibeSurf] Health checks setup complete');
  }

  handleInitializationError(error) {
    console.error('[VibeSurf] Initialization error:', error);
    
    // Show error in UI
    const errorElement = document.createElement('div');
    errorElement.className = 'initialization-error';
    errorElement.innerHTML = `
      <div style="padding: 20px; text-align: center; color: #dc3545;">
        <h3>VibeSurf Initialization Failed</h3>
        <p>${error.message}</p>
        <button id="retry-initialization-btn" style="
          padding: 8px 16px;
          border: 1px solid #dc3545;
          background: #dc3545;
          color: white;
          border-radius: 4px;
          cursor: pointer;
          margin-top: 10px;
        ">Retry</button>
      </div>
    `;
    
    document.body.innerHTML = '';
    document.body.appendChild(errorElement);
    
    // Add proper retry event listener
    const retryBtn = document.getElementById('retry-initialization-btn');
    retryBtn.addEventListener('click', () => {
      console.log('[VibeSurf] Retrying initialization...');
      this.retryInitialization();
    });
    
    // Update badge to show error
    chrome.runtime.sendMessage({
      type: 'UPDATE_BADGE',
      data: { text: '!', color: '#dc3545' }
    });
    
    // Show error notification
    chrome.runtime.sendMessage({
      type: 'SHOW_NOTIFICATION',
      data: {
        title: 'VibeSurf Error',
        message: `Initialization failed: ${error.message}`
      }
    });
  }

  async retryInitialization() {
    console.log('[VibeSurf] Retrying initialization...');
    
    try {
      // Clear any existing error UI
      const errorElement = document.querySelector('.initialization-error');
      if (errorElement) {
        errorElement.remove();
      }
      
      // Restore the original HTML structure
      document.body.innerHTML = `
        <div id="app" class="vibesurf-container">
          <!-- Header -->
          <header class="header">
            <div class="header-left">
              <div class="logo">
                <div class="logo-content">
                  <span class="logo-text">VibeSurf Activity Logs</span>
                  <div class="session-info">
                    <span class="session-label">Session:</span>
                    <span id="session-id">-</span>
                    <button id="copy-session-btn" class="copy-btn" title="Copy Session ID">
                      <svg width="12" height="12" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M16 1H4C2.9 1 2 1.9 2 3V17H4V3H16V1ZM19 5H8C6.9 5 6 5.9 6 7V21C6 22.1 6.9 23 8 23H19C20.1 23 21 22.1 21 21V7C21 5.9 20.1 5 19 5ZM19 21H8V7H19V21Z" fill="currentColor"/>
                      </svg>
                    </button>
                  </div>
                </div>
              </div>
            </div>
            <div class="header-right">
              <button id="new-session-btn" class="icon-btn" title="New Session">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M12 4V20M4 12H20" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                </svg>
              </button>
              <button id="history-btn" class="icon-btn" title="Chat History">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M3 3V11A4 4 0 0 0 7 15H17L21 19V3H3Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
              </button>
              <button id="settings-btn" class="icon-btn" title="Settings">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M12 15C13.6569 15 15 13.6569 15 12C15 10.3431 13.6569 9 12 9C10.3431 9 9 10.3431 9 12C9 13.6569 10.3431 15 12 15Z" stroke="currentColor" stroke-width="2"/>
                </svg>
              </button>
            </div>
          </header>
          
          <!-- Main Content -->
          <main class="main-content">
            <div class="activity-section">
              <div id="activity-log" class="activity-log">
                <div class="loading-message" style="text-align: center; padding: 20px; color: #666;">
                  <div>Connecting to VibeSurf...</div>
                </div>
              </div>
            </div>
            <div id="control-panel" class="control-panel hidden"></div>
          </main>
          
          <!-- Input Section -->
          <footer class="input-section">
            <div class="input-container">
              <div class="input-main">
                <div class="textarea-container">
                  <textarea id="task-input" class="task-input" placeholder="Ask anything (/ for skills, @ to specify tab)" rows="3"></textarea>
                  <div class="input-actions">
                    <button id="attach-file-btn" class="action-btn attach-btn" title="Attach Files">
                      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M21.44 11.05L12.25 20.24C11.1242 21.3658 9.59722 21.9983 8.005 21.9983C6.41278 21.9983 4.88583 21.3658 3.76 20.24C2.63417 19.1142 2.00166 17.5872 2.00166 15.995C2.00166 14.4028 2.63417 12.8758 3.76 11.75L12.33 3.18C13.0806 2.42944 14.0986 2.00696 15.16 2.00696C16.2214 2.00696 17.2394 2.42944 17.99 3.18C18.7406 3.93056 19.163 4.94859 19.163 6.01C19.163 7.07141 18.7406 8.08944 17.99 8.84L10.07 16.76C9.69469 17.1353 9.1897 17.3442 8.665 17.3442C8.1403 17.3442 7.63531 17.1353 7.26 16.76C6.88469 16.3847 6.67581 15.8797 6.67581 15.355C6.67581 14.8303 6.88469 14.3253 7.26 13.95L15.19 6.02" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                      </svg>
                    </button>
                    <button id="send-btn" class="action-btn send-btn" title="Send Task">
                      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M22 2L11 13M22 2L15 22L11 13M22 2L2 9L11 13" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                      </svg>
                    </button>
                  </div>
                </div>
              </div>
              <div class="input-footer">
                <select id="llm-profile-select" class="llm-select compact"></select>
              </div>
            </div>
            <input type="file" id="file-input" class="hidden" multiple accept="*/*">
          </footer>
        </div>
      `;
      
      // Reset state
      this.isInitialized = false;
      this.apiClient = null;
      this.sessionManager = null;
      this.uiManager = null;
      
      // Retry initialization
      await this.initialize();
      
    } catch (error) {
      console.error('[VibeSurf] Retry initialization failed:', error);
      this.handleInitializationError(error);
    }
  }

  // Settings management
  async updateSettings(newSettings) {
    this.settings = { ...this.settings, ...newSettings };
    
    try {
      await chrome.storage.local.set({ settings: this.settings });
      
      // Apply settings to components
      if (newSettings.backendUrl && this.apiClient) {
        this.apiClient.setBaseURL(newSettings.backendUrl);
        
        // Re-check backend connection
        await this.checkBackendConnection();
      }
      
      if (newSettings.pollingFrequency && this.sessionManager) {
        this.sessionManager.pollingFrequency = newSettings.pollingFrequency;
      }
      
      console.log('[VibeSurf] Settings updated:', newSettings);
      
    } catch (error) {
      console.error('[VibeSurf] Failed to update settings:', error);
      throw error;
    }
  }

  // Current tab context
  async getCurrentTabContext() {
    try {
      const tabInfo = await chrome.runtime.sendMessage({ type: 'GET_CURRENT_TAB' });
      return tabInfo;
    } catch (error) {
      console.error('[VibeSurf] Failed to get current tab context:', error);
      return null;
    }
  }

  // Cleanup method
  destroy() {
    // Prevent multiple cleanup calls
    if (this.isDestroying || !this.isInitialized) {
      console.log('[VibeSurf] Cleanup already in progress or app not initialized, skipping...');
      return;
    }
    
    this.isDestroying = true;
    console.log('[VibeSurf] Cleaning up application...');
    
    try {
      if (this.uiManager) {
        this.uiManager.destroy();
        this.uiManager = null;
      }
      
      if (this.sessionManager) {
        this.sessionManager.destroy();
        this.sessionManager = null;
      }
      
      if (this.apiClient) {
        this.apiClient = null;
      }
      
      this.isInitialized = false;
      console.log('[VibeSurf] Application cleanup complete');
    } catch (error) {
      console.error('[VibeSurf] Error during cleanup:', error);
    } finally {
      this.isDestroying = false;
    }
  }

  // Get application status
  getStatus() {
    return {
      initialized: this.isInitialized,
      hasActiveSession: this.sessionManager?.isSessionActive() || false,
      hasActiveTask: this.sessionManager?.hasActiveTask() || false,
      currentSessionId: this.sessionManager?.getCurrentSessionId() || null,
      backendUrl: this.settings.backendUrl || 'Not configured',
      taskStatus: this.sessionManager?.getTaskStatus() || null
    };
  }
}

// Initialize the application when DOM is loaded
document.addEventListener('DOMContentLoaded', async () => {
  console.log('[VibeSurf] DOM loaded, initializing application...');
  
  // Create global app instance
  window.vibeSurfApp = new VibeSurfApp();
  
  try {
    await window.vibeSurfApp.initialize();
  } catch (error) {
    console.error('[VibeSurf] Failed to initialize app:', error);
  }
});

// Handle page unload
window.addEventListener('beforeunload', () => {
  if (window.vibeSurfApp && window.vibeSurfApp.isInitialized && !window.vibeSurfApp.isDestroying) {
    console.log('[VibeSurf] Page unloading, cleaning up...');
    window.vibeSurfApp.destroy();
  }
});

// Handle visibility change to prevent unnecessary cleanup
document.addEventListener('visibilitychange', () => {
  if (document.visibilityState === 'hidden') {
    console.log('[VibeSurf] Page hidden, but not cleaning up (might be tab switch)');
  } else if (document.visibilityState === 'visible') {
    console.log('[VibeSurf] Page visible again');
  }
});

// Make app accessible for debugging
if (typeof window !== 'undefined') {
  window.VibeSurfApp = VibeSurfApp;
}

// Handle messages from background script
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  console.log('[VibeSurf] Received message from background:', message.type);
  
  if (window.vibeSurfApp) {
    switch (message.type) {
      case 'GET_APP_STATUS':
        sendResponse(window.vibeSurfApp.getStatus());
        break;
        
      case 'UPDATE_SETTINGS':
        window.vibeSurfApp.updateSettings(message.data)
          .then(() => sendResponse({ success: true }))
          .catch(error => sendResponse({ success: false, error: error.message }));
        return true; // Keep message channel open for async response
        
      case 'MICROPHONE_PERMISSION_RESULT':
        console.log('[VibeSurf] Received microphone permission result:', message);
        // This message is typically handled by voice recorder, just acknowledge
        sendResponse({ acknowledged: true });
        break;
        
      default:
        console.warn('[VibeSurf] Unknown message type:', message.type);
    }
  }
});

console.log('[VibeSurf] Main script loaded');


================================================
FILE: vibe_surf/chrome_extension/scripts/modal-manager.js
================================================
// Modal Manager - Handles modal dialogs and user confirmations
// Manages warning modals, confirmation dialogs, and generic modal utilities

class VibeSurfModalManager {
  constructor() {
    this.state = {
      activeModals: new Set(),
      modalCounter: 0
    };
    this.eventListeners = new Map();
    
    this.bindGlobalEvents();
  }

  bindGlobalEvents() {
    // Handle escape key to close modals
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        this.closeTopModal();
      }
    });
    
    // Handle background clicks to close modals
    document.addEventListener('click', (e) => {
      if (e.target.classList.contains('modal-overlay')) {
        this.closeModal(e.target.querySelector('.modal'));
      }
    });
  }

  // Event system for communicating with main UI manager
  on(event, callback) {
    if (!this.eventListeners.has(event)) {
      this.eventListeners.set(event, []);
    }
    this.eventListeners.get(event).push(callback);
  }

  emit(event, data) {
    if (this.eventListeners.has(event)) {
      this.eventListeners.get(event).forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error(`[ModalManager] Event callback error for ${event}:`, error);
        }
      });
    }
  }

  // Warning Modal
  showWarningModal(title, message, options = {}) {
    const {
      confirmText = 'OK',
      showCancel = false,
      cancelText = 'Cancel',
      onConfirm = null,
      onCancel = null,
      className = ''
    } = options;

    const modalId = this.generateModalId();
    
    // Add warning icon to title and always show cancel for warning modals
    const warningTitle = `âš ï¸ ${title}`;
    const shouldShowCancel = showCancel || true; // Always show cancel for warning modals
    
    const modalHTML = `
      <div class="modal-overlay dynamic-modal warning-modal" id="${modalId}-overlay">
        <div class="modal warning-modal ${className}" id="${modalId}">
          <div class="modal-content">
            <div class="modal-header">
              <h3>${this.escapeHtml(warningTitle)}</h3>
            </div>
            <div class="modal-body">
              <p>${this.escapeHtml(message)}</p>
            </div>
            <div class="modal-footer">
              ${shouldShowCancel ? `<button class="btn-secondary modal-cancel-btn" data-modal-id="${modalId}">${this.escapeHtml(cancelText)}</button>` : ''}
              <button class="btn-primary modal-confirm-btn" data-modal-id="${modalId}">${this.escapeHtml(confirmText)}</button>
            </div>
          </div>
        </div>
      </div>
    `;

    // Add to DOM
    document.body.insertAdjacentHTML('beforeend', modalHTML);
    
    const modal = document.getElementById(modalId);
    const overlay = document.getElementById(`${modalId}-overlay`);
    
    // Track active modal
    this.state.activeModals.add(modalId);
    
    // Add event listeners
    this.bindModalEvents(modalId, { onConfirm, onCancel });
    
    // Show modal
    requestAnimationFrame(() => {
      overlay.classList.add('show');
    });
    
    return modalId;
  }

  // Confirmation Modal
  showConfirmModal(title, message, options = {}) {
    const {
      confirmText = 'Confirm',
      cancelText = 'Cancel',
      onConfirm = null,
      onCancel = null,
      className = '',
      type = 'question' // question, danger, info
    } = options;

    const modalId = this.generateModalId();
    
    const iconMap = {
      question: 'â“',
      danger: 'ğŸš¨',
      info: 'â„¹ï¸'
    };
    
    const icon = iconMap[type] || iconMap.question;
    
    // Add icon to title and remove close button
    const iconTitle = `${icon} ${title}`;
    
    const modalHTML = `
      <div class="modal-overlay dynamic-modal" id="${modalId}-overlay">
        <div class="modal confirm-modal ${className}" id="${modalId}">
          <div class="modal-content">
            <div class="modal-header">
              <h3>${this.escapeHtml(iconTitle)}</h3>
            </div>
            <div class="modal-body">
              <p>${this.escapeHtml(message)}</p>
            </div>
            <div class="modal-footer">
              <button class="btn-secondary modal-cancel-btn" data-modal-id="${modalId}">${this.escapeHtml(cancelText)}</button>
              <button class="btn-primary modal-confirm-btn" data-modal-id="${modalId}" ${type === 'danger' ? 'data-danger="true"' : ''}>${this.escapeHtml(confirmText)}</button>
            </div>
          </div>
        </div>
      </div>
    `;

    // Add to DOM
    document.body.insertAdjacentHTML('beforeend', modalHTML);
    
    const modal = document.getElementById(modalId);
    const overlay = document.getElementById(`${modalId}-overlay`);
    
    // Track active modal
    this.state.activeModals.add(modalId);
    
    // Add event listeners
    this.bindModalEvents(modalId, { onConfirm, onCancel });
    
    // Show modal
    requestAnimationFrame(() => {
      overlay.classList.add('show');
    });
    
    return modalId;
  }

  // Generic Modal Creator
  createModal(content, options = {}) {
    const {
      title = '',
      className = '',
      showCloseButton = true,
      backdrop = true,
      onShow = null,
      onHide = null
    } = options;

    const modalId = this.generateModalId();
    
    const modalHTML = `
      <div class="modal-overlay dynamic-modal ${backdrop ? 'backdrop' : ''}" id="${modalId}-overlay">
        <div class="modal ${className}" id="${modalId}">
          <div class="modal-content">
            ${title || showCloseButton ? `
              <div class="modal-header">
                ${title ? `<h3>${this.escapeHtml(title)}</h3>` : ''}
                ${showCloseButton ? `<button class="modal-close-btn" data-modal-id="${modalId}">Ã—</button>` : ''}
              </div>
            ` : ''}
            <div class="modal-body">
              ${content}
            </div>
          </div>
        </div>
      </div>
    `;

    // Add to DOM
    document.body.insertAdjacentHTML('beforeend', modalHTML);
    
    const modal = document.getElementById(modalId);
    const overlay = document.getElementById(`${modalId}-overlay`);
    
    // Track active modal
    this.state.activeModals.add(modalId);
    
    // Add basic event listeners
    this.bindModalEvents(modalId, { onShow, onHide });
    
    // Show modal
    requestAnimationFrame(() => {
      overlay.classList.add('show');
      if (onShow) onShow(modal);
    });
    
    return {
      modalId,
      modal,
      overlay,
      close: () => this.closeModal(modal)
    };
  }

  // Modal Event Binding
  bindModalEvents(modalId, callbacks = {}) {
    const { onConfirm, onCancel, onShow, onHide } = callbacks;
    
    // Close button
    const closeBtn = document.querySelector(`[data-modal-id="${modalId}"].modal-close-btn`);
    if (closeBtn) {
      closeBtn.addEventListener('click', () => {
        this.closeModal(document.getElementById(modalId));
        if (onCancel) onCancel();
      });
    }
    
    // Confirm button
    const confirmBtn = document.querySelector(`[data-modal-id="${modalId}"].modal-confirm-btn`);
    if (confirmBtn) {
      confirmBtn.addEventListener('click', () => {
        if (onConfirm) {
          const result = onConfirm();
          // Only close if callback doesn't return false
          if (result !== false) {
            this.closeModal(document.getElementById(modalId));
          }
        } else {
          this.closeModal(document.getElementById(modalId));
        }
      });
    }
    
    // Cancel button
    const cancelBtn = document.querySelector(`[data-modal-id="${modalId}"].modal-cancel-btn`);
    if (cancelBtn) {
      cancelBtn.addEventListener('click', () => {
        this.closeModal(document.getElementById(modalId));
        if (onCancel) onCancel();
      });
    }
  }

  // Close Modal
  closeModal(modal) {
    if (!modal) return;
    
    const modalId = modal.id;
    const overlay = document.getElementById(`${modalId}-overlay`);
    
    if (overlay) {
      overlay.classList.remove('show');
      
      // Remove from DOM after animation
      setTimeout(() => {
        if (overlay.parentNode) {
          overlay.parentNode.removeChild(overlay);
        }
        this.state.activeModals.delete(modalId);
      }, 300); // Match CSS transition duration
    }
  }

  // Close the topmost modal
  closeTopModal() {
    const modalIds = Array.from(this.state.activeModals);
    if (modalIds.length > 0) {
      const topModalId = modalIds[modalIds.length - 1];
      const modal = document.getElementById(topModalId);
      if (modal) {
        this.closeModal(modal);
      }
    }
  }

  // Close all modals
  closeAllModals() {
    const modalIds = Array.from(this.state.activeModals);
    modalIds.forEach(modalId => {
      const modal = document.getElementById(modalId);
      if (modal) {
        this.closeModal(modal);
      }
    });
  }

  // Utility Methods
  generateModalId() {
    return `modal-${++this.state.modalCounter}-${Date.now()}`;
  }

  escapeHtml(text) {
    if (typeof text !== 'string') return '';
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  }

  // Promise-based modal methods
  showWarningModalAsync(title, message, options = {}) {
    return new Promise((resolve) => {
      this.showWarningModal(title, message, {
        ...options,
        onConfirm: () => {
          if (options.onConfirm) options.onConfirm();
          resolve(true);
        },
        onCancel: () => {
          if (options.onCancel) options.onCancel();
          resolve(false);
        }
      });
    });
  }

  showConfirmModalAsync(title, message, options = {}) {
    return new Promise((resolve) => {
      this.showConfirmModal(title, message, {
        ...options,
        onConfirm: async () => {
          try {
            let result = true;
            if (options.onConfirm) {
              result = await options.onConfirm();
              // If onConfirm returns false, don't resolve with true
              if (result === false) {
                resolve(false);
                return;
              }
            }
            resolve(true);
          } catch (error) {
            console.error('[ModalManager] onConfirm error:', error);
            resolve(false);
          }
        },
        onCancel: async () => {
          try {
            if (options.onCancel) {
              await options.onCancel();
            }
            resolve(false);
          } catch (error) {
            console.error('[ModalManager] onCancel error:', error);
            resolve(false);
          }
        }
      });
    });
  }

  // Modal state queries
  hasActiveModals() {
    return this.state.activeModals.size > 0;
  }

  getActiveModalCount() {
    return this.state.activeModals.size;
  }

  isModalActive(modalId) {
    return this.state.activeModals.has(modalId);
  }

  // Quick notification modal (auto-close)
  showNotificationModal(message, type = 'info', duration = 3000) {
    const typeIcons = {
      success: 'âœ…',
      error: 'âŒ',
      warning: 'âš ï¸',
      info: 'â„¹ï¸'
    };
    
    const icon = typeIcons[type] || typeIcons.info;
    const typeClass = `notification-${type}`;
    
    const modalData = this.createModal(`
      <div class="notification-content">
        <div class="notification-icon">${icon}</div>
        <div class="notification-message">${this.escapeHtml(message)}</div>
      </div>
    `, {
      className: `notification-modal ${typeClass}`,
      showCloseButton: false,
      backdrop: false
    });
    
    // Auto-close after duration
    if (duration > 0) {
      setTimeout(() => {
        modalData.close();
      }, duration);
    }
    
    return modalData.modalId;
  }

  // Quick input modal
  showInputModal(title, placeholder = '', defaultValue = '', options = {}) {
    const {
      inputType = 'text',
      confirmText = 'OK',
      cancelText = 'Cancel',
      onConfirm = null,
      onCancel = null,
      validator = null
    } = options;

    return new Promise((resolve) => {
      const inputId = `input-${Date.now()}`;
      
      const modalData = this.createModal(`
        <div class="input-modal-content">
          <label for="${inputId}" class="input-label">${title}</label>
          <input type="${inputType}" id="${inputId}" class="modal-input" 
                 placeholder="${this.escapeHtml(placeholder)}" 
                 value="${this.escapeHtml(defaultValue)}">
          <div class="modal-footer">
            <button class="btn-secondary input-cancel-btn">${this.escapeHtml(cancelText)}</button>
            <button class="btn-primary input-confirm-btn">${this.escapeHtml(confirmText)}</button>
          </div>
        </div>
      `, {
        className: 'input-modal',
        showCloseButton: true
      });
      
      const input = document.getElementById(inputId);
      const confirmBtn = modalData.modal.querySelector('.input-confirm-btn');
      const cancelBtn = modalData.modal.querySelector('.input-cancel-btn');
      
      // Focus input
      setTimeout(() => input.focus(), 100);
      
      const handleConfirm = () => {
        const value = input.value.trim();
        
        if (validator && !validator(value)) {
          return; // Don't close modal if validation fails
        }
        
        modalData.close();
        if (onConfirm) onConfirm(value);
        resolve(value);
      };
      
      const handleCancel = () => {
        modalData.close();
        if (onCancel) onCancel();
        resolve(null);
      };
      
      confirmBtn.addEventListener('click', handleConfirm);
      cancelBtn.addEventListener('click', handleCancel);
      
      // Enter key to confirm
      input.addEventListener('keydown', (e) => {
        if (e.key === 'Enter') {
          handleConfirm();
        }
      });
    });
  }

  // Get current state
  getState() {
    return { ...this.state };
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfModalManager = VibeSurfModalManager;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/permission-iframe-request.js
================================================
// Permission iframe request script
// This script runs inside the iframe to request microphone permissions

(function() {
    'use strict';
    
    console.log('[PermissionIframe] Permission iframe script loaded');
    console.log('[PermissionIframe] Current URL:', window.location.href);
    console.log('[PermissionIframe] Parent origin:', window.parent.location.origin);
    console.log('[PermissionIframe] Is secure context:', window.isSecureContext);
    
    const statusEl = document.getElementById('status');
    
    // Function to request microphone permission
    async function requestMicrophonePermission() {
        try {
            console.log('[PermissionIframe] Starting microphone permission request...');
            console.log('[PermissionIframe] Window location:', window.location.href);
            console.log('[PermissionIframe] Is top window:', window === window.top);
            console.log('[PermissionIframe] Document domain:', document.domain);
            
            // Check if media devices are available
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error('Media devices not supported in this context');
            }
            
            console.log('[PermissionIframe] Media devices available, requesting getUserMedia...');
            statusEl.textContent = 'Requesting microphone access...';
            statusEl.className = 'status loading';
            
            // Request microphone access with minimal constraints
            console.log('[PermissionIframe] About to call getUserMedia...');
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: true,
                video: false
            });
            
            console.log('[PermissionIframe] Permission granted! Stream received');
            console.log('[PermissionIframe] Stream tracks:', stream.getTracks().length);
            
            // Stop all tracks immediately after getting permission
            stream.getTracks().forEach(track => {
                console.log('[PermissionIframe] Stopping track:', track.kind);
                track.stop();
            });
            
            // Update status
            statusEl.textContent = 'Microphone access granted!';
            statusEl.className = 'status success';
            
            // Send success message to parent window
            console.log('[PermissionIframe] Sending success message to parent...');
            console.log('[PermissionIframe] Parent window:', window.parent);
            
            // First try to send to parent window
            try {
                window.parent.postMessage({
                    type: 'MICROPHONE_PERMISSION_RESULT',
                    success: true,
                    granted: true,
                    source: 'iframe'
                }, '*');
                console.log('[PermissionIframe] PostMessage to parent sent successfully');
            } catch (postMessageError) {
                console.error('[PermissionIframe] Failed to send postMessage to parent:', postMessageError);
            }
            
            // Also try to send to extension if available
            if (typeof chrome !== 'undefined' && chrome.runtime) {
                try {
                    chrome.runtime.sendMessage({
                        type: 'MICROPHONE_PERMISSION_RESULT',
                        success: true,
                        granted: true,
                        source: 'iframe'
                    }, (response) => {
                        if (chrome.runtime.lastError) {
                            console.log('[PermissionIframe] Chrome runtime error:', chrome.runtime.lastError);
                        } else {
                            console.log('[PermissionIframe] Extension message sent successfully:', response);
                        }
                    });
                } catch (e) {
                    console.error('[PermissionIframe] Could not send to extension:', e);
                }
            }
            
            return true;
            
        } catch (error) {
            console.error('[PermissionIframe] Permission request failed:', error);
            console.error('[PermissionIframe] Error details:', {
                name: error.name,
                message: error.message,
                stack: error.stack
            });
            
            // Update status with error
            let errorMessage = '';
            let userMessage = '';
            
            if (error.name === 'NotAllowedError') {
                errorMessage = 'Microphone access denied';
                userMessage = 'Please allow microphone access when prompted by your browser';
            } else if (error.name === 'NotFoundError') {
                errorMessage = 'No microphone found';
                userMessage = 'Please ensure a microphone is connected';
            } else if (error.name === 'NotReadableError') {
                errorMessage = 'Microphone in use';
                userMessage = 'Please close other apps using the microphone';
            } else if (error.name === 'SecurityError') {
                errorMessage = 'Security restriction';
                userMessage = 'Cannot access microphone due to security settings';
            } else {
                errorMessage = 'Permission failed';
                userMessage = error.message;
            }
            
            statusEl.textContent = `${errorMessage}: ${userMessage}`;
            statusEl.className = 'status error';
            
            // Send error message to parent window
            console.log('[PermissionIframe] Sending error message to parent...');
            console.log('[PermissionIframe] Error details:', { name: error.name, message: error.message });
            
            // First try to send to parent window
            try {
                window.parent.postMessage({
                    type: 'MICROPHONE_PERMISSION_RESULT',
                    success: false,
                    granted: false,
                    error: error.message,
                    errorName: error.name,
                    userMessage: userMessage,
                    source: 'iframe'
                }, '*');
                console.log('[PermissionIframe] Error postMessage to parent sent successfully');
            } catch (postMessageError) {
                console.error('[PermissionIframe] Failed to send error postMessage to parent:', postMessageError);
            }
            
            // Also try to send to extension if available
            if (typeof chrome !== 'undefined' && chrome.runtime) {
                try {
                    chrome.runtime.sendMessage({
                        type: 'MICROPHONE_PERMISSION_RESULT',
                        success: false,
                        granted: false,
                        error: error.message,
                        errorName: error.name,
                        userMessage: userMessage,
                        source: 'iframe'
                    }, (response) => {
                        if (chrome.runtime.lastError) {
                            console.log('[PermissionIframe] Chrome runtime error for error message:', chrome.runtime.lastError);
                        } else {
                            console.log('[PermissionIframe] Error extension message sent successfully:', response);
                        }
                    });
                } catch (e) {
                    console.error('[PermissionIframe] Could not send error to extension:', e);
                }
            }
            
            return false;
        }
    }
    
    // Start permission request when iframe loads
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', requestMicrophonePermission);
    } else {
        requestMicrophonePermission();
    }
    
    // Also listen for messages from parent requesting permission
    window.addEventListener('message', (event) => {
        console.log('[PermissionIframe] Received message from parent:', event.data);
        
        if (event.data && event.data.type === 'REQUEST_MICROPHONE_PERMISSION') {
            console.log('[PermissionIframe] Parent requested permission, starting request...');
            requestMicrophonePermission();
        }
    });
    
    console.log('[PermissionIframe] Permission iframe script initialized');
    
})();


================================================
FILE: vibe_surf/chrome_extension/scripts/permission-request.js
================================================
// Permission Request Page Script
// Handles microphone permission request in new tab

const statusEl = document.getElementById('status');

// Add debug logging
console.log('[PermissionPage] Permission page loaded');
console.log('[PermissionPage] Location:', window.location.href);
console.log('[PermissionPage] Media devices available:', !!navigator.mediaDevices);
console.log('[PermissionPage] getUserMedia available:', !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia));

document.getElementById('allowBtn').onclick = async function() {
    console.log('[PermissionPage] Allow button clicked');
    console.log('[PermissionPage] Current URL:', window.location.href);
    console.log('[PermissionPage] Media devices available:', !!navigator.mediaDevices);
    console.log('[PermissionPage] getUserMedia available:', !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia));
    console.log('[PermissionPage] Is secure context:', window.isSecureContext);
    console.log('[PermissionPage] Chrome runtime available:', !!(typeof chrome !== 'undefined' && chrome.runtime));
    
    statusEl.className = 'loading';
    statusEl.textContent = 'Requesting microphone access...';
    
    try {
        // Check if media devices are available
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            throw new Error('Media devices not supported in this context');
        }
        
        console.log('[PermissionPage] Requesting getUserMedia...');
        console.log('[PermissionPage] About to call getUserMedia with constraints: {audio: true, video: false}');
        
        // This will trigger Chrome's standard permission popup in the address bar
        const stream = await navigator.mediaDevices.getUserMedia({audio: true, video: false});
        
        console.log('[PermissionPage] Permission granted, stream received');
        console.log('[PermissionPage] Stream tracks:', stream.getTracks().length);
        
        // Stop the stream immediately after getting permission
        stream.getTracks().forEach(track => track.stop());
        
        statusEl.className = 'success';
        statusEl.textContent = 'Permission granted! You can close this tab.';
        
        // Send success message to voice recorder
        console.log('[PermissionPage] Sending success message');
        chrome.runtime.sendMessage({
            type: "MICROPHONE_PERMISSION_RESULT", 
            granted: true
        });
        
        // Close tab after a short delay
        setTimeout(() => window.close(), 2000);
        
    } catch (error) {
        console.error('[PermissionPage] Permission error:', error);
        console.error('[PermissionPage] Error details:', {
            name: error.name,
            message: error.message,
            stack: error.stack
        });
        
        statusEl.className = 'error';
        
        // Provide more user-friendly error messages
        let errorMessage = '';
        let debugInfo = '';
        
        if (error.name === 'NotAllowedError') {
            errorMessage = 'Microphone access was denied. Please check your browser permissions.';
            debugInfo = 'Try clicking the microphone icon in your browser\'s address bar to allow access.';
        } else if (error.name === 'NotFoundError') {
            errorMessage = 'No microphone found on this device.';
            debugInfo = 'Please ensure a microphone is connected and try again.';
        } else if (error.name === 'NotReadableError') {
            errorMessage = 'Microphone is already in use by another application.';
            debugInfo = 'Please close other applications that might be using the microphone.';
        } else if (error.name === 'SecurityError') {
            errorMessage = 'Security restrictions prevent microphone access.';
            debugInfo = 'This might be due to browser security settings or the page context.';
        } else {
            errorMessage = `Permission denied: ${error.message}`;
            debugInfo = `Error type: ${error.name}`;
        }
        
        statusEl.textContent = errorMessage;
        
        // Add debug info to the page
        const debugDiv = document.createElement('div');
        debugDiv.style.marginTop = '10px';
        debugDiv.style.fontSize = '12px';
        debugDiv.style.color = '#666';
        debugDiv.textContent = debugInfo;
        statusEl.appendChild(debugDiv);
        
        // Send error message to voice recorder
        chrome.runtime.sendMessage({
            type: "MICROPHONE_PERMISSION_RESULT",
            granted: false,
            error: error.message,
            errorName: error.name,
            userMessage: errorMessage
        });
    }
};

document.getElementById('denyBtn').onclick = function() {
    console.log('[PermissionPage] Deny button clicked');
    statusEl.className = 'error';
    statusEl.textContent = 'Permission denied by user';
    
    chrome.runtime.sendMessage({
        type: "MICROPHONE_PERMISSION_RESULT", 
        granted: false, 
        error: "User denied permission"
    });
    
    setTimeout(() => window.close(), 1500);
};


================================================
FILE: vibe_surf/chrome_extension/scripts/session-manager.js
================================================
// Session Manager - Handles session lifecycle and activity monitoring
// Manages session creation, activity polling, and history

class VibeSurfSessionManager {
  constructor(apiClient) {
    this.apiClient = apiClient;
    this.currentSession = null;
    this.activityLogs = [];
    this.pollingInterval = null;
    this.pollingFrequency = 300; // 300ms for faster response
    this.isPolling = false;
    this.eventListeners = new Map();
    
    this.bindMethods();
  }

  bindMethods() {
    this.handleActivityUpdate = this.handleActivityUpdate.bind(this);
    this.pollActivity = this.pollActivity.bind(this);
  }

  // Event system for UI updates
  on(event, callback) {
    if (!this.eventListeners.has(event)) {
      this.eventListeners.set(event, []);
    }
    this.eventListeners.get(event).push(callback);
  }

  off(event, callback) {
    if (this.eventListeners.has(event)) {
      const callbacks = this.eventListeners.get(event);
      const index = callbacks.indexOf(callback);
      if (index > -1) {
        callbacks.splice(index, 1);
      }
    }
  }

  emit(event, data) {
    if (this.eventListeners.has(event)) {
      this.eventListeners.get(event).forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error(`[SessionManager] Event callback error for ${event}:`, error);
        }
      });
    }
  }

  // Session management
  async createSession(prefix = 'vibesurf_') {
    const sessionId = await this.apiClient.generateSessionId(prefix);
    
    this.currentSession = {
      id: sessionId,
      createdAt: new Date().toISOString(),
      status: 'active',
      taskHistory: [],
      currentTask: null
    };

    // Clear previous activity logs
    this.activityLogs = [];


    // Store session in background
    await this.storeSessionData();

    this.emit('sessionCreated', { sessionId, session: this.currentSession });
    
    return sessionId;
  }

  async loadSession(sessionId) {
    try {
      // Stop current polling
      this.stopActivityPolling();


      // Load session data from background
      const response = await chrome.runtime.sendMessage({
        type: 'GET_SESSION_DATA',
        data: { sessionId }
      });


      // Check if response is valid and has sessionData
      if (response && response.sessionData) {
        this.currentSession = {
          id: sessionId,
          ...response.sessionData
        };

        // Load activity logs from backend
        await this.loadSessionActivity();

        this.emit('sessionLoaded', { sessionId, session: this.currentSession });
        
        return true;
      } else if (response && response.error) {
        console.error('[SessionManager] Background error:', response.error);
        this.emit('sessionError', { error: response.error, sessionId });
        return false;
      } else {
        // Session not found in storage - create a new session with this ID
        
        this.currentSession = {
          id: sessionId,
          createdAt: new Date().toISOString(),
          status: 'active',
          taskHistory: [],
          currentTask: null
        };

        // Clear activity logs for new session
        this.activityLogs = [];

        // Try to load any existing activity logs from backend
        await this.loadSessionActivity();

        // Store the new session
        await this.storeSessionData();

        this.emit('sessionLoaded', { sessionId, session: this.currentSession });
        
        return true;
      }
    } catch (error) {
      console.error('[SessionManager] Failed to load session:', error);
      this.emit('sessionError', { error: error.message, sessionId });
      return false;
    }
  }

  async loadSessionActivity() {
    if (!this.currentSession) {
      console.warn('[SessionManager] No current session for activity loading');
      return;
    }

    try {
      const response = await this.apiClient.getSessionActivity(this.currentSession.id);
      
      
      // Check multiple possible response formats
      let activityLogs = null;
      if (response && response.data && response.data.activity_logs) {
        activityLogs = response.data.activity_logs;
      } else if (response && response.activity_logs) {
        activityLogs = response.activity_logs;
      } else if (response && Array.isArray(response)) {
        activityLogs = response;
      }
      
      if (activityLogs && Array.isArray(activityLogs)) {
        this.activityLogs = activityLogs;
        
        
        // Add timestamps to logs that don't have them
        this.activityLogs.forEach(log => {
          if (!log.timestamp) {
            log.timestamp = new Date().toISOString();
          }
        });
        
        this.emit('activityLogsLoaded', {
          sessionId: this.currentSession.id,
          logs: this.activityLogs
        });
      } else {
        // No existing activity logs
        this.activityLogs = [];
        this.lastMessageIndex = 0;
      }
    } catch (error) {
      console.error('[SessionManager] âŒ Failed to load session activity:', error);
      console.error('[SessionManager] Error details:', {
        message: error.message,
        stack: error.stack,
        sessionId: this.currentSession?.id
      });
      // Reset to safe defaults
      this.activityLogs = [];
    }
  }

  getCurrentSession() {
    return this.currentSession;
  }

  getCurrentSessionId() {
    return this.currentSession?.id || null;
  }

  // Task management
  async submitTask(taskData) {
    if (!this.currentSession) {
      throw new Error('No active session. Please create a session first.');
    }

    try {
      // Stop any existing polling before starting new task
      this.stopActivityPolling();
      
      // Reset activity logs for new task to ensure proper index synchronization
      this.activityLogs = [];
      
      // Sync with server logs to get the correct starting state
      try {
        await this.syncActivityLogsFromServer();
      } catch (error) {
        this.activityLogs = [];
      }

      const taskPayload = {
        session_id: this.currentSession.id,
        ...taskData
      };

      const response = await this.apiClient.submitTask(taskPayload);
      
      // Check if the response indicates LLM connection failure
      if (response && response.success === false && response.error === 'llm_connection_failed') {
        console.log('[SessionManager] LLM connection failed, emitting taskError');
        this.emit('taskError', {
          error: response,
          sessionId: this.currentSession.id
        });
        throw new Error(response.message || 'LLM connection failed');
      }
      
      // Update current session with task info
      this.currentSession.currentTask = {
        taskId: response.task_id,
        description: taskData.task_description,
        llmProfile: taskData.llm_profile_name,
        status: 'submitted',
        submittedAt: new Date().toISOString()
      };

      // Store updated session
      await this.storeSessionData();

      // Start polling after task submission and sync
      this.startActivityPolling();

      this.emit('taskSubmitted', {
        sessionId: this.currentSession.id,
        task: this.currentSession.currentTask,
        response
      });

      return response;
    } catch (error) {
      console.error('[SessionManager] Task submission failed:', error);
      this.emit('taskError', { error: error.message, sessionId: this.currentSession.id });
      throw error;
    }
  }

  async pauseTask(reason = 'User requested pause') {
    try {
      const response = await this.apiClient.pauseTask(reason);
      
      if (this.currentSession?.currentTask) {
        this.currentSession.currentTask.status = 'paused';
        this.currentSession.currentTask.pausedAt = new Date().toISOString();
        await this.storeSessionData();
      }

      // Stop polling when task is paused
      this.stopActivityPolling();

      this.emit('taskPaused', { sessionId: this.currentSession?.id, response });
      
      return response;
    } catch (error) {
      console.error('[SessionManager] Task pause failed:', error);
      this.emit('taskError', { error: error.message, action: 'pause' });
      throw error;
    }
  }

  async resumeTask(reason = 'User requested resume') {
    try {
      const response = await this.apiClient.resumeTask(reason);
      
      if (this.currentSession?.currentTask) {
        this.currentSession.currentTask.status = 'running';
        this.currentSession.currentTask.resumedAt = new Date().toISOString();
        await this.storeSessionData();
      }

      // Sync activity logs before resuming polling to ensure index consistency
      try {
        await this.syncActivityLogsFromServer();
      } catch (error) {
        // Continue with existing logs if sync fails
      }

      // Restart polling when task is resumed
      this.startActivityPolling();

      this.emit('taskResumed', { sessionId: this.currentSession?.id, response });
      
      return response;
    } catch (error) {
      console.error('[SessionManager] Task resume failed:', error);
      this.emit('taskError', { error: error.message, action: 'resume' });
      throw error;
    }
  }

  async stopTask(reason = 'User requested stop') {
    try {
      const response = await this.apiClient.stopTask(reason);
      
      if (this.currentSession?.currentTask) {
        this.currentSession.currentTask.status = 'stopped';
        this.currentSession.currentTask.stoppedAt = new Date().toISOString();
        await this.storeSessionData();
      }

      // Stop polling when task is stopped
      this.stopActivityPolling();
      
      // Sync final activity logs to capture any termination messages
      try {
        await this.syncActivityLogsFromServer();
      } catch (error) {
        // Continue if sync fails
      }

      this.emit('taskStopped', { sessionId: this.currentSession?.id, response });
      
      return response;
    } catch (error) {
      console.error('[SessionManager] Task stop failed:', error);
      this.emit('taskError', { error: error.message, action: 'stop' });
      throw error;
    }
  }

  async addNewTaskToPaused(newTaskDescription) {
    try {
      const response = await this.apiClient.addNewTask(newTaskDescription);
      
      this.emit('newTaskAdded', {
        sessionId: this.currentSession?.id,
        newTask: newTaskDescription,
        response
      });
      
      return response;
    } catch (error) {
      console.error('[SessionManager] Add new task failed:', error);
      this.emit('taskError', { error: error.message, action: 'add_new_task' });
      throw error;
    }
  }

  // Activity polling
  startActivityPolling() {
    if (this.isPolling) {
      return;
    }

    
    this.isPolling = true;
    // Use arrow function to preserve 'this' context
    this.pollingInterval = setInterval(() => {
      this.pollActivity();
    }, this.pollingFrequency);
    
    this.emit('pollingStarted', { sessionId: this.currentSession?.id });
  }

  stopActivityPolling() {
    if (this.pollingInterval) {
      clearInterval(this.pollingInterval);
      this.pollingInterval = null;
    }
    
    this.isPolling = false;
    this.emit('pollingStopped', { sessionId: this.currentSession?.id });
  }

  async pollActivity() {
    if (!this.currentSession) {
      this.stopActivityPolling();
      return;
    }

    try {
      const requestIndex = this.activityLogs.length;
      
      console.log(`[SessionManager] ğŸ”„ Polling activity at index ${requestIndex}, current logs: ${this.activityLogs.length}`);
      
      // Poll for new activity at the next expected index
      const response = await this.apiClient.pollSessionActivity(
        this.currentSession.id,
        requestIndex
      );

      // Check both possible response formats
      const activityLog = response?.activity_log || response?.data?.activity_log;
      const totalAvailable = response?.total_available || response?.data?.total_available;

      if (response && activityLog) {
        const prevActivityLog = this.activityLogs.length > 0 ? this.activityLogs[this.activityLogs.length - 1] : null;

        const isNewLog = !prevActivityLog || !this.areLogsEqual(prevActivityLog, activityLog);
        
        if (isNewLog) {
          // New activity log received
          const newLog = { ...activityLog };
          
          // Add timestamp if not present - this should now be handled by UI
          if (!newLog.timestamp) {
            newLog.timestamp = new Date().toISOString();
          }
          
          this.activityLogs.push(newLog);

          console.log(`[SessionManager] âœ… New activity received: ${newLog.agent_name} - ${newLog.agent_status}`);

          await this.handleActivityUpdate(newLog);

          this.emit('newActivity', {
            sessionId: this.currentSession.id,
            activity: newLog,
            allLogs: this.activityLogs
          });

          // Check if task is completed or terminated
          const terminalStatuses = ['done'];
          
          if (terminalStatuses.includes(newLog.agent_status?.toLowerCase())) {
            this.stopActivityPolling();
            
            if (this.currentSession.currentTask) {
              this.currentSession.currentTask.status = newLog.agent_status;
              this.currentSession.currentTask.completedAt = new Date().toISOString();
              await this.storeSessionData();
            }

            this.emit('taskCompleted', {
              sessionId: this.currentSession.id,
              status: newLog.agent_status,
              finalActivity: newLog
            });
          }
        } else {
          console.log(`[SessionManager] ğŸ”„ Duplicate log detected, skipping`);
        }
      } else {
        // No new activity at this index
        console.log(`[SessionManager] ğŸ”„ No new activity at index ${requestIndex}, waiting...`);
        
        // Check if we're behind based on server total
        if (totalAvailable && totalAvailable > this.activityLogs.length) {
          console.log(`[SessionManager] ğŸ”„ Syncing logs: have ${this.activityLogs.length}, server has ${totalAvailable}`);
          await this.syncActivityLogs();
        }
      }
    } catch (error) {
      // Enhanced error logging for debugging
      console.error(`[SessionManager] âŒ Activity polling error at index ${this.activityLogs.length}:`, {
        error: error.message,
        sessionId: this.currentSession?.id,
        currentLogsLength: this.activityLogs.length,
        stack: error.stack
      });
      
      // Only emit polling errors for non-timeout/not-found errors
      if (!error.message.includes('timeout') && !error.message.includes('No activity log found')) {
        this.emit('pollingError', { error: error.message, sessionId: this.currentSession?.id });
      }
    }
  }

  areLogsEqual(log1, log2) {
    if (!log1 || !log2) return false;
    
    return log1.agent_name === log2.agent_name &&
           log1.agent_status === log2.agent_status &&
           log1.agent_msg === log2.agent_msg;
  }

  async syncActivityLogs() {
    if (!this.currentSession) return;
    
    try {
      
      // Get all activity logs from server
      const response = await this.apiClient.getSessionActivity(this.currentSession.id);
      
      // Check both possible response formats
      const activityLogs = response?.activity_logs || response?.data?.activity_logs;
      
      if (activityLogs) {
        const serverLogs = activityLogs;
        const missingLogs = serverLogs.slice(this.activityLogs.length);
        
        if (missingLogs.length > 0) {
          
          for (const log of missingLogs) {
            // Add timestamp if not present - this should now be handled by UI
            if (!log.timestamp) {
              log.timestamp = new Date().toISOString();
            }
            
            this.activityLogs.push(log);
            
            this.emit('newActivity', {
              sessionId: this.currentSession.id,
              activity: log,
              allLogs: this.activityLogs
            });
          }
        }
      }
    } catch (error) {
      console.error(`[SessionManager] âŒ Failed to sync activity logs:`, error);
    }
  }

  async syncActivityLogsFromServer() {
    if (!this.currentSession) return;
    
    try {
      console.log(`[SessionManager] ğŸ”„ Syncing all activity logs from server for session: ${this.currentSession.id}`);
      
      // Get all activity logs from server
      const response = await this.apiClient.getSessionActivity(this.currentSession.id);
      
      // Check both possible response formats
      const serverLogs = response?.activity_logs || response?.data?.activity_logs || [];
      
      if (Array.isArray(serverLogs)) {
        // å®Œå…¨åŒæ­¥ï¼šç”¨æœåŠ¡å™¨ç«¯çš„logsæ›¿æ¢æœ¬åœ°logs
        const previousCount = this.activityLogs.length;
        
        // Add timestamp to logs that don't have them
        const processedLogs = serverLogs.map(log => ({
          ...log,
          timestamp: log.timestamp || new Date().toISOString()
        }));
        
        this.activityLogs = processedLogs;
        
        console.log(`[SessionManager] âœ… Activity logs synced: ${previousCount} -> ${this.activityLogs.length} logs`);
        
        // è§¦å‘æ—¥å¿—åŠ è½½äº‹ä»¶ï¼Œè®©UIæ›´æ–°
        this.emit('activityLogsLoaded', {
          sessionId: this.currentSession.id,
          logs: this.activityLogs
        });
      } else {
        console.log(`[SessionManager] ğŸ“ No activity logs found on server for session: ${this.currentSession.id}`);
        this.activityLogs = [];
      }
    } catch (error) {
      console.error(`[SessionManager] âŒ Failed to sync activity logs from server:`, error);
      // ä¸æŠ›å‡ºé”™è¯¯ï¼Œå…è®¸ä»»åŠ¡æäº¤ç»§ç»­è¿›è¡Œ
    }
  }

  async handleActivityUpdate(activityLog) {
    // Update current task status based on activity
    if (this.currentSession?.currentTask && activityLog.agent_status) {
      this.currentSession.currentTask.status = activityLog.agent_status;
      await this.storeSessionData();
    }

    // Store activity in background for persistence
    await chrome.runtime.sendMessage({
      type: 'STORE_SESSION_DATA',
      data: {
        sessionId: this.currentSession.id,
        lastActivity: activityLog,
        activityCount: this.activityLogs.length
      }
    });
  }

  // History management
  async getSessionHistory() {
    try {
      const response = await chrome.runtime.sendMessage({
        type: 'GET_SESSION_DATA'
      });

      return response.sessions || [];
    } catch (error) {
      console.error('[SessionManager] Failed to get session history:', error);
      return [];
    }
  }

  async getSessionTasks(sessionId) {
    try {
      const response = await this.apiClient.getSessionTasks(sessionId);
      return response.data?.tasks || [];
    } catch (error) {
      console.error('[SessionManager] Failed to get session tasks:', error);
      return [];
    }
  }

  // Storage helpers
  async storeSessionData() {
    if (!this.currentSession) return;

    try {
      await chrome.runtime.sendMessage({
        type: 'STORE_SESSION_DATA',
        data: {
          sessionId: this.currentSession.id,
          ...this.currentSession,
          activityLogs: this.activityLogs,
          lastUpdated: new Date().toISOString()
        }
      });
    } catch (error) {
      console.error('[SessionManager] Failed to store session data:', error);
    }
  }

  // File management for session
  async uploadFiles(files) {
    if (!this.currentSession) {
      throw new Error('No active session for file upload');
    }

    try {
      const response = await this.apiClient.uploadFiles(files, this.currentSession.id);
      
      this.emit('filesUploaded', {
        sessionId: this.currentSession.id,
        files: response.files
      });

      return response;
    } catch (error) {
      console.error('[SessionManager] File upload failed:', error);
      this.emit('fileUploadError', { error: error.message, sessionId: this.currentSession.id });
      throw error;
    }
  }

  // Cleanup
  destroy() {
    // Prevent multiple cleanup calls
    if (this.isDestroying) {
      console.log('[SessionManager] Cleanup already in progress, skipping...');
      return;
    }
    
    this.isDestroying = true;
    console.log('[SessionManager] Destroying session manager...');
    
    try {
      this.stopActivityPolling();
      this.eventListeners.clear();
      
      // Clear any ongoing requests
      if (this.pollingTimer) {
        clearTimeout(this.pollingTimer);
        this.pollingTimer = null;
      }
      
      // Reset state
      this.currentSession = null;
      this.currentTaskId = null;
      this.activityLogs = [];
      this.isPolling = false;
      
      console.log('[SessionManager] Session manager cleanup complete');
    } catch (error) {
      console.error('[SessionManager] Error during destroy:', error);
    } finally {
      this.isDestroying = false;
    }
  }

  // Status helpers
  isSessionActive() {
    return this.currentSession && this.currentSession.status === 'active';
  }

  hasActiveTask() {
    return this.currentSession?.currentTask && 
           ['submitted', 'running', 'paused'].includes(this.currentSession.currentTask.status);
  }

  getTaskStatus() {
    return this.currentSession?.currentTask?.status || null;
  }

  getActivityLogs() {
    return [...this.activityLogs]; // Return copy
  }

  getLatestActivity() {
    return this.activityLogs[this.activityLogs.length - 1] || null;
  }

}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfSessionManager = VibeSurfSessionManager;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/user-settings-storage.js
================================================
// User Settings Storage Manager - Unified local storage management for user settings
// Supports Chrome extension storage API and localStorage with unified settings interface

class VibeSurfUserSettingsStorage {
  constructor() {
    this.storageKeys = {
      // Chrome storage keys
      userSettings: 'vibesurf-user-settings',
      
      // LocalStorage keys (for backward compatibility)
      theme: 'vibesurf-theme',
      defaultAsr: 'vibesurf-default-asr',
      defaultTts: 'vibesurf-default-tts',
      
      // New setting keys
      selectedLlmProfile: 'vibesurf-selected-llm-profile',
      selectedAgentMode: 'vibesurf-selected-agent-mode'
    };
    
    // Default settings
    this.defaultSettings = {
      // General settings
      theme: 'auto',
      defaultAsr: '',
      defaultTts: '',
      
      // UI state settings
      selectedLlmProfile: '',
      selectedAgentMode: 'thinking',
      
      // Additional settings can be extended here
      lastSessionId: '',
      rememberSelections: true,
      autoSaveSettings: true
    };
    
    this.eventListeners = new Map();
    this.isInitialized = false;
  }

  // Initialize the storage manager
  async initialize() {
    try {
      console.log('[UserSettingsStorage] Initializing user settings storage...');
      
      // Check if this is first run and migrate existing settings if needed
      await this.migrateExistingSettings();
      
      // Ensure all default settings exist
      await this.ensureDefaultSettings();
      
      this.isInitialized = true;
      console.log('[UserSettingsStorage] User settings storage initialized successfully');
      
      this.emit('initialized', await this.getAllSettings());
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to initialize:', error);
      throw error;
    }
  }

  // Migrate existing localStorage settings to unified storage
  async migrateExistingSettings() {
    try {
      console.log('[UserSettingsStorage] Checking for existing settings to migrate...');
      
      const existingSettings = {};
      
      // Check and migrate existing localStorage settings
      const localStorageKeys = [
        { old: this.storageKeys.theme, new: 'theme' },
        { old: this.storageKeys.defaultAsr, new: 'defaultAsr' },
        { old: this.storageKeys.defaultTts, new: 'defaultTts' }
      ];
      
      localStorageKeys.forEach(({ old, new: newKey }) => {
        const value = localStorage.getItem(old);
        if (value !== null) {
          existingSettings[newKey] = value;
          console.log(`[UserSettingsStorage] Found existing ${newKey}:`, value);
        }
      });
      
      // If there are existing settings, merge them with current Chrome storage
      if (Object.keys(existingSettings).length > 0) {
        console.log('[UserSettingsStorage] Migrating existing settings:', existingSettings);
        
        // Get current Chrome storage settings to preserve them
        const currentSettings = await this.getAllSettings();
        
        // Merge localStorage settings with existing Chrome storage (don't overwrite)
        const mergedSettings = { ...currentSettings };
        
        // Only migrate localStorage values if they don't already exist in Chrome storage
        Object.keys(existingSettings).forEach(key => {
          if (!(key in mergedSettings)) {
            mergedSettings[key] = existingSettings[key];
            console.log(`[UserSettingsStorage] Migrated ${key}:`, existingSettings[key]);
          }
        });
        
        // Save merged settings only if something was actually migrated
        if (Object.keys(mergedSettings).length !== Object.keys(currentSettings).length) {
          await this.saveSettings(mergedSettings);
          console.log('[UserSettingsStorage] Migration completed successfully');
        }
      }
    } catch (error) {
      console.warn('[UserSettingsStorage] Failed to migrate existing settings:', error);
    }
  }

  // Ensure all default settings exist
  async ensureDefaultSettings() {
    try {
      const currentSettings = await this.getAllSettings();
      const mergedSettings = { ...this.defaultSettings, ...currentSettings };
      
      // Only save if there are missing settings
      if (Object.keys(currentSettings).length !== Object.keys(mergedSettings).length) {
        await this.saveSettings(mergedSettings);
        console.log('[UserSettingsStorage] Default settings ensured');
      }
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to ensure default settings:', error);
    }
  }

  // Get all settings from storage
  async getAllSettings() {
    try {
      if (chrome && chrome.storage && chrome.storage.local) {
        // Use Chrome storage API with proper promise wrapping
        const result = await new Promise((resolve, reject) => {
          chrome.storage.local.get(this.storageKeys.userSettings, (result) => {
            if (chrome.runtime.lastError) {
              reject(new Error(chrome.runtime.lastError.message));
            } else {
              resolve(result);
            }
          });
        });
        return result[this.storageKeys.userSettings] || {};
      } else {
        
        // Fallback to localStorage
        const stored = localStorage.getItem(this.storageKeys.userSettings);
        const settings = stored ? JSON.parse(stored) : {};
        return settings;
      }
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to get all settings:', error);
      // Fallback to localStorage if Chrome storage fails
      try {
        const stored = localStorage.getItem(this.storageKeys.userSettings);
        const settings = stored ? JSON.parse(stored) : {};
        return settings;
      } catch (fallbackError) {
        console.error('[UserSettingsStorage] localStorage fallback also failed:', fallbackError);
        return {};
      }
    }
  }

  // Save settings to storage
  async saveSettings(settings) {
    try {
      if (chrome && chrome.storage && chrome.storage.local) {
        // Get current settings first to merge
        const currentSettings = await this.getAllSettings();
        const mergedSettings = { ...currentSettings, ...settings };
        
        // Use Chrome storage API with proper promise wrapping
        await new Promise((resolve, reject) => {
          chrome.storage.local.set({ [this.storageKeys.userSettings]: mergedSettings }, () => {
            if (chrome.runtime.lastError) {
              reject(new Error(chrome.runtime.lastError.message));
            } else {
              resolve();
            }
          });
        });
        
        this.emit('settingsChanged', mergedSettings);
        return mergedSettings;
      } else {
        // Fallback to localStorage
        const currentSettings = await this.getAllSettings();
        const mergedSettings = { ...currentSettings, ...settings };
        localStorage.setItem(this.storageKeys.userSettings, JSON.stringify(mergedSettings));
        this.emit('settingsChanged', mergedSettings);
        return mergedSettings;
      }
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to save settings:', error);
      // Fallback to localStorage if Chrome storage fails
      try {
        const currentSettings = await this.getAllSettings();
        const mergedSettings = { ...currentSettings, ...settings };
        localStorage.setItem(this.storageKeys.userSettings, JSON.stringify(mergedSettings));
        this.emit('settingsChanged', mergedSettings);
        return mergedSettings;
      } catch (fallbackError) {
        console.error('[UserSettingsStorage] localStorage fallback save also failed:', fallbackError);
        throw error; // Throw original error
      }
    }
  }

  // Get a specific setting value
  async getSetting(key) {
    try {
      const allSettings = await this.getAllSettings();
      return allSettings[key] !== undefined ? allSettings[key] : this.defaultSettings[key];
    } catch (error) {
      console.error(`[UserSettingsStorage] Failed to get setting ${key}:`, error);
      return this.defaultSettings[key];
    }
  }

  // Set a specific setting value
  async setSetting(key, value) {
    try {
      const allSettings = await this.getAllSettings();
      allSettings[key] = value;
      await this.saveSettings(allSettings);
      
      console.log(`[UserSettingsStorage] Setting updated: ${key} = ${value}`);
      this.emit('settingChanged', { key, value, allSettings });
    } catch (error) {
      console.error(`[UserSettingsStorage] Failed to set setting ${key}:`, error);
      throw error;
    }
  }

  // Update multiple settings at once
  async updateSettings(updates) {
    try {
      const allSettings = await this.getAllSettings();
      const updatedSettings = { ...allSettings, ...updates };
      await this.saveSettings(updatedSettings);
      
      console.log('[UserSettingsStorage] Multiple settings updated:', updates);
      this.emit('settingsUpdated', { updates, allSettings: updatedSettings });
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to update settings:', error);
      throw error;
    }
  }

  // Remove a setting
  async removeSetting(key) {
    try {
      const allSettings = await this.getAllSettings();
      delete allSettings[key];
      await this.saveSettings(allSettings);
      
      console.log(`[UserSettingsStorage] Setting removed: ${key}`);
      this.emit('settingRemoved', { key, allSettings });
    } catch (error) {
      console.error(`[UserSettingsStorage] Failed to remove setting ${key}:`, error);
      throw error;
    }
  }

  // Clear all settings (reset to defaults)
  async clearAllSettings() {
    try {
      await this.saveSettings(this.defaultSettings);
      console.log('[UserSettingsStorage] All settings cleared, reset to defaults');
      this.emit('settingsCleared', this.defaultSettings);
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to clear settings:', error);
      throw error;
    }
  }

  // Specific methods for commonly used settings

  // Theme settings
  async getTheme() {
    return await this.getSetting('theme');
  }

  async setTheme(theme) {
    await this.setSetting('theme', theme);
    // Also update localStorage for backward compatibility
    localStorage.setItem(this.storageKeys.theme, theme);
  }

  // LLM Profile settings
  async getSelectedLlmProfile() {
    return await this.getSetting('selectedLlmProfile');
  }

  async setSelectedLlmProfile(profileName) {
    await this.setSetting('selectedLlmProfile', profileName);
  }

  // Agent Mode settings
  async getSelectedAgentMode() {
    return await this.getSetting('selectedAgentMode');
  }

  async setSelectedAgentMode(mode) {
    await this.setSetting('selectedAgentMode', mode);
    console.log('[UserSettingsStorage] Setting updated: selectedAgentMode =', mode);
  }

  // Default Voice settings
  async getDefaultAsr() {
    return await this.getSetting('defaultAsr');
  }

  async setDefaultAsr(asrProfile) {
    await this.setSetting('defaultAsr', asrProfile);
    // Also update localStorage for backward compatibility
    localStorage.setItem(this.storageKeys.defaultAsr, asrProfile);
  }

  async getDefaultTts() {
    return await this.getSetting('defaultTts');
  }

  async setDefaultTts(ttsProfile) {
    await this.setSetting('defaultTts', ttsProfile);
    // Also update localStorage for backward compatibility
    localStorage.setItem(this.storageKeys.defaultTts, ttsProfile);
  }

  // Event system for components to listen to setting changes
  on(event, callback) {
    if (!this.eventListeners.has(event)) {
      this.eventListeners.set(event, []);
    }
    this.eventListeners.get(event).push(callback);
  }

  off(event, callback) {
    if (this.eventListeners.has(event)) {
      const listeners = this.eventListeners.get(event);
      const index = listeners.indexOf(callback);
      if (index > -1) {
        listeners.splice(index, 1);
      }
    }
  }

  emit(event, data) {
    if (this.eventListeners.has(event)) {
      this.eventListeners.get(event).forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error(`[UserSettingsStorage] Event callback error for ${event}:`, error);
        }
      });
    }
  }

  // Export settings for backup
  async exportSettings() {
    try {
      const allSettings = await this.getAllSettings();
      return {
        version: '1.0',
        timestamp: new Date().toISOString(),
        settings: allSettings
      };
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to export settings:', error);
      throw error;
    }
  }

  // Import settings from backup
  async importSettings(exportedData) {
    try {
      if (!exportedData || !exportedData.settings) {
        throw new Error('Invalid settings data');
      }
      
      await this.saveSettings(exportedData.settings);
      console.log('[UserSettingsStorage] Settings imported successfully');
      return true;
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to import settings:', error);
      throw error;
    }
  }

  // Get storage info
  async getStorageInfo() {
    try {
      const allSettings = await this.getAllSettings();
      const settingsSize = JSON.stringify(allSettings).length;
      
      return {
        isInitialized: this.isInitialized,
        settingsCount: Object.keys(allSettings).length,
        estimatedSize: settingsSize,
        storageType: (chrome && chrome.storage && chrome.storage.local) ? 'chrome.storage.local' : 'localStorage',
        lastUpdated: allSettings.lastUpdated || null
      };
    } catch (error) {
      console.error('[UserSettingsStorage] Failed to get storage info:', error);
      return null;
    }
  }

  // Destroy the storage manager
  destroy() {
    this.eventListeners.clear();
    this.isInitialized = false;
    console.log('[UserSettingsStorage] Storage manager destroyed');
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfUserSettingsStorage = VibeSurfUserSettingsStorage;
}


================================================
FILE: vibe_surf/chrome_extension/scripts/voice-recorder.js
================================================
// Voice Recording Manager - Handles voice input functionality
// Provides recording capabilities and integration with ASR API

class VibeSurfVoiceRecorder {
  constructor(apiClient) {
    this.apiClient = apiClient;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
    this.recordingStartTime = null;
    this.maxRecordingDuration = 60000; // 30 seconds max
    this.recordingTimeout = null;
    this.durationInterval = null;
    this.onDurationUpdate = null;
    
    // Recording state callbacks
    this.onRecordingStart = null;
    this.onRecordingStop = null;
    this.onTranscriptionComplete = null;
    this.onTranscriptionError = null;
    
    console.log('[VoiceRecorder] Voice recorder initialized');
  }

  // Check if browser supports media recording
  isSupported() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia && window.MediaRecorder);
  }

  // Simplified permission request for Chrome extension
  async requestMicrophonePermission() {
    try {
      console.log('[VoiceRecorder] Requesting microphone permission...');
      
      // For Chrome extensions, try direct permission first
      if (typeof chrome !== 'undefined' && chrome.runtime && chrome.runtime.id) {
        try {
          // Try direct getUserMedia first (works if permission already granted)
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          stream.getTracks().forEach(track => track.stop());
          console.log('[VoiceRecorder] Direct permission granted');
          return true;
        } catch (directError) {
          console.log('[VoiceRecorder] Direct permission failed, using iframe method');
          return new Promise((resolve) => {
            this.requestMicrophonePermissionViaIframe(resolve);
          });
        }
      }
      
      // Fallback: Direct permission request for non-extension contexts
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());
      console.log('[VoiceRecorder] Permission granted');
      return true;
      
    } catch (error) {
      console.error('[VoiceRecorder] Microphone permission denied:', error);
      
      let errorMessage = 'Microphone permission denied';
      if (error.name === 'NotAllowedError') {
        errorMessage = 'Microphone access was denied. Please allow access and try again.';
      } else if (error.name === 'NotFoundError') {
        errorMessage = 'No microphone found. Please connect a microphone.';
      } else if (error.name === 'NotReadableError') {
        errorMessage = 'Microphone is in use by another application.';
      }
      
      const permissionError = new Error(errorMessage);
      permissionError.name = 'MicrophonePermissionError';
      permissionError.originalError = error;
      throw permissionError;
    }
  }

  // Simplified iframe permission request
  async requestMicrophonePermissionViaIframe(resolve) {
    console.log('[VoiceRecorder] Using iframe injection method');
    
    try {
      const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
      if (!tabs || tabs.length === 0) {
        throw new Error('No active tab found');
      }
      
      const activeTab = tabs[0];
      
      // Check if we can inject into this tab
      if (activeTab.url.startsWith('chrome://') ||
          activeTab.url.startsWith('chrome-extension://')) {
        console.log('[VoiceRecorder] Cannot inject into restricted tab, using tab method');
        return this.showMicrophonePermissionTab(resolve);
      }
      
      // Inject iframe
      const response = await chrome.tabs.sendMessage(activeTab.id, {
        type: 'INJECT_MICROPHONE_PERMISSION_IFRAME'
      });
      
      if (response && response.success) {
        // Listen for permission result
        const messageHandler = (message) => {
          if (message.type === 'MICROPHONE_PERMISSION_RESULT' && message.source === 'iframe') {
            chrome.runtime.onMessage.removeListener(messageHandler);
            resolve(message.granted || message.success || false);
          }
        };
        
        chrome.runtime.onMessage.addListener(messageHandler);
        
        // Timeout cleanup
        setTimeout(() => {
          chrome.runtime.onMessage.removeListener(messageHandler);
          resolve(false);
        }, 30000);
      } else {
        throw new Error('Failed to inject iframe');
      }
      
    } catch (error) {
      console.error('[VoiceRecorder] Iframe method failed:', error);
      this.showMicrophonePermissionTab(resolve);
    }
  }
  
  // Simplified tab permission request
  showMicrophonePermissionTab(resolve) {
    console.log('[VoiceRecorder] Using tab method for permission');
    
    try {
      const permissionUrl = chrome.runtime.getURL('permission-request.html');
      
      chrome.tabs.create({ url: permissionUrl, active: true }, (tab) => {
        if (chrome.runtime.lastError) {
          console.error('[VoiceRecorder] Failed to create tab:', chrome.runtime.lastError);
          resolve(false);
          return;
        }
        
        // Listen for permission result
        const messageHandler = (message) => {
          if (message.type === 'MICROPHONE_PERMISSION_RESULT') {
            chrome.runtime.onMessage.removeListener(messageHandler);
            chrome.tabs.remove(tab.id).catch(() => {});
            resolve(message.granted || false);
          }
        };
        
        chrome.runtime.onMessage.addListener(messageHandler);
        
        // Timeout cleanup
        setTimeout(() => {
          chrome.runtime.onMessage.removeListener(messageHandler);
          chrome.tabs.remove(tab.id).catch(() => {});
          resolve(false);
        }, 30000);
      });
      
    } catch (error) {
      console.error('[VoiceRecorder] Tab method failed:', error);
      resolve(false);
    }
  }


  // Start voice recording
  async startRecording() {
    if (this.isRecording) {
      console.warn('[VoiceRecorder] Already recording');
      return false;
    }

    if (!this.isSupported()) {
      console.error('[VoiceRecorder] Voice recording not supported in this browser');
      throw new Error('Voice recording is not supported in your browser');
    }

    try {
      console.log('[VoiceRecorder] Starting voice recording...');
      
      // Check for ASR profiles BEFORE starting recording
      const asrProfiles = await this.apiClient.getASRProfiles(true);
      if (!asrProfiles.profiles || asrProfiles.profiles.length === 0) {
        console.log('[VoiceRecorder] No ASR profiles found, showing configuration modal');
        this.handleNoVoiceProfileError();
        return false;
      }
      
      console.log(`[VoiceRecorder] Found ${asrProfiles.profiles.length} ASR profile(s)`);
      
      // Get microphone stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100
        }
      });

      // Create MediaRecorder
      const options = {
        mimeType: 'audio/webm;codecs=opus'
      };

      // Fallback for browsers that don't support webm
      if (!MediaRecorder.isTypeSupported(options.mimeType)) {
        options.mimeType = 'audio/ogg;codecs=opus';
        if (!MediaRecorder.isTypeSupported(options.mimeType)) {
          options.mimeType = 'audio/wav';
        }
      }

      this.mediaRecorder = new MediaRecorder(stream, options);
      this.audioChunks = [];
      this.recordingStartTime = Date.now();

      // Set up event handlers
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
        }
      };

      this.mediaRecorder.onstop = () => {
        this.handleRecordingStop();
      };

      this.mediaRecorder.onerror = (event) => {
        console.error('[VoiceRecorder] MediaRecorder error:', event.error);
        this.stopRecording();
        this.handleRecordingError(event.error);
      };

      // Start recording
      this.mediaRecorder.start();
      this.isRecording = true;

      // Set up duration updates
      this.startDurationUpdates();

      // Set up maximum recording duration
      this.recordingTimeout = setTimeout(() => {
        if (this.isRecording) {
          console.log('[VoiceRecorder] Maximum recording duration reached, stopping automatically');
          this.stopRecording();
        }
      }, this.maxRecordingDuration);

      // Notify callback
      if (this.onRecordingStart) {
        this.onRecordingStart();
      }

      console.log('[VoiceRecorder] Voice recording started');
      return true;

    } catch (error) {
      console.error('[VoiceRecorder] Failed to start recording:', error);
      this.handleRecordingError(error);
      throw error;
    }
  }

  // Stop voice recording
  stopRecording() {
    if (!this.isRecording || !this.mediaRecorder) {
      console.warn('[VoiceRecorder] Not currently recording');
      return false;
    }

    try {
      console.log('[VoiceRecorder] Stopping voice recording...');
      
      // Clear the timeout
      if (this.recordingTimeout) {
        clearTimeout(this.recordingTimeout);
        this.recordingTimeout = null;
      }

      // Stop the MediaRecorder
      this.mediaRecorder.stop();
      
      // Stop all tracks in the stream
      const stream = this.mediaRecorder.stream;
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      this.isRecording = false;
      console.log('[VoiceRecorder] Voice recording stopped');

      return true;

    } catch (error) {
      console.error('[VoiceRecorder] Error stopping recording:', error);
      this.handleRecordingError(error);
      return false;
    }
  }

  // Handle recording stop event
  async handleRecordingStop() {
    try {
      if (this.audioChunks.length === 0) {
        console.warn('[VoiceRecorder] No audio data recorded');
        this.handleRecordingError(new Error('No audio data recorded'));
        return;
      }

      // Create audio blob
      const audioBlob = new Blob(this.audioChunks, { 
        type: this.mediaRecorder.mimeType 
      });

      const recordingDuration = Date.now() - this.recordingStartTime;
      console.log(`[VoiceRecorder] Recorded ${audioBlob.size} bytes in ${recordingDuration}ms`);

      // Notify callback
      if (this.onRecordingStop) {
        this.onRecordingStop(audioBlob, recordingDuration);
      }

      // Transcribe the audio
      await this.transcribeAudio(audioBlob);

    } catch (error) {
      console.error('[VoiceRecorder] Error handling recording stop:', error);
      this.handleRecordingError(error);
    }
  }

  // Start duration updates
  startDurationUpdates() {
    this.stopDurationUpdates(); // Clear any existing interval
    
    this.durationInterval = setInterval(() => {
      const duration = this.getRecordingDuration();
      const formattedDuration = this.formatDuration(duration);
      
      if (this.onDurationUpdate) {
        this.onDurationUpdate(formattedDuration, duration);
      }
    }, 1000); // Update every second
  }

  // Stop duration updates
  stopDurationUpdates() {
    if (this.durationInterval) {
      clearInterval(this.durationInterval);
      this.durationInterval = null;
    }
  }

  // Format duration in MM:SS format
  formatDuration(milliseconds) {
    const seconds = Math.floor(milliseconds / 1000);
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;
  }

  // Transcribe audio using ASR API
  async transcribeAudio(audioBlob) {
    try {
      console.log('[VoiceRecorder] Transcribing audio...');
      
      // Get available ASR profiles
      const asrProfiles = await this.apiClient.getASRProfiles(true);
      
      if (!asrProfiles.profiles || asrProfiles.profiles.length === 0) {
        // Show voice profile required modal instead of generic error
        this.handleNoVoiceProfileError();
        return;
      }

      // Use the first available ASR profile
      const voiceProfileName = asrProfiles.profiles[0].voice_profile_name;
      console.log(`[VoiceRecorder] Using ASR profile: ${voiceProfileName}`);

      // Call the ASR API
      const result = await this.apiClient.transcribeAudio(audioBlob, voiceProfileName);
      
      if (result.success && result.recognized_text) {
        console.log(`[VoiceRecorder] Transcription successful: "${result.recognized_text}"`);
        
        // Notify callback with transcription result
        if (this.onTranscriptionComplete) {
          this.onTranscriptionComplete(result.recognized_text, result);
        }
      } else {
        throw new Error(result.message || 'Transcription failed');
      }

    } catch (error) {
      console.error('[VoiceRecorder] Transcription error:', error);
      this.handleTranscriptionError(error);
    }
  }

  // Handle recording errors
  handleRecordingError(error) {
    this.isRecording = false;
    this.cleanup();
    
    const errorMessage = error.message || 'Voice recording failed';
    console.error('[VoiceRecorder] Recording error:', errorMessage);
    
    if (this.onTranscriptionError) {
      this.onTranscriptionError(errorMessage, 'recording');
    }
  }

  // Handle transcription errors
  handleTranscriptionError(error) {
    const errorMessage = error.message || 'Audio transcription failed';
    console.error('[VoiceRecorder] Transcription error:', errorMessage);
    
    if (this.onTranscriptionError) {
      this.onTranscriptionError(errorMessage, 'transcription');
    }
  }

  // Handle no voice profile error with modal
  handleNoVoiceProfileError() {
    console.log('[VoiceRecorder] No voice profiles configured');
    
    // Send message to UI manager to show voice profile required modal
    if (typeof window !== 'undefined' && window.vibeSurfUIManager) {
      try {
        window.vibeSurfUIManager.showVoiceProfileRequiredModal('configure');
      } catch (error) {
        console.error('[VoiceRecorder] Failed to show voice profile modal:', error);
        // Fallback to generic error handling
        this.handleTranscriptionError(new Error('No active ASR profiles found. Please configure an ASR profile in Settings > Voice.'));
      }
    } else {
      // Fallback to generic error handling
      this.handleTranscriptionError(new Error('No active ASR profiles found. Please configure an ASR profile in Settings > Voice.'));
    }
  }

  // Cleanup resources
  cleanup() {
    if (this.recordingTimeout) {
      clearTimeout(this.recordingTimeout);
      this.recordingTimeout = null;
    }

    if (this.mediaRecorder) {
      if (this.mediaRecorder.state !== 'inactive') {
        try {
          this.mediaRecorder.stop();
        } catch (error) {
          console.warn('[VoiceRecorder] Error stopping MediaRecorder during cleanup:', error);
        }
      }

      // Stop all tracks in the stream
      const stream = this.mediaRecorder.stream;
      if (stream) {
        stream.getTracks().forEach(track => {
          try {
            track.stop();
          } catch (error) {
            console.warn('[VoiceRecorder] Error stopping track during cleanup:', error);
          }
        });
      }

      this.mediaRecorder = null;
    }

    this.audioChunks = [];
    this.isRecording = false;
    this.recordingStartTime = null;
  }

  // Get recording duration
  getRecordingDuration() {
    if (!this.isRecording || !this.recordingStartTime) {
      return 0;
    }
    return Date.now() - this.recordingStartTime;
  }

  // Check if currently recording
  isCurrentlyRecording() {
    return this.isRecording;
  }

  // Check if voice recording should be disabled due to missing ASR profiles
  async isVoiceRecordingAvailable() {
    try {
      const asrProfiles = await this.apiClient.getASRProfiles(true);
      return asrProfiles.profiles && asrProfiles.profiles.length > 0;
    } catch (error) {
      console.error('[VoiceRecorder] Error checking ASR profiles availability:', error);
      return false;
    }
  }

  // Set callbacks
  setCallbacks(callbacks) {
    if (callbacks.onRecordingStart) this.onRecordingStart = callbacks.onRecordingStart;
    if (callbacks.onRecordingStop) this.onRecordingStop = callbacks.onRecordingStop;
    if (callbacks.onTranscriptionComplete) this.onTranscriptionComplete = callbacks.onTranscriptionComplete;
    if (callbacks.onTranscriptionError) this.onTranscriptionError = callbacks.onTranscriptionError;
  }
}

// Export for use in other modules
if (typeof window !== 'undefined') {
  window.VibeSurfVoiceRecorder = VibeSurfVoiceRecorder;
}


================================================
FILE: vibe_surf/chrome_extension/styles/activity.css
================================================
/* Activity Log Styles - VibeSurf Extension */

/* Activity Section */
.activity-section {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.activity-log {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-lg);
  background-color: var(--bg-primary);
  min-height: 0; /* Allow proper flexbox sizing */
  display: flex;
  flex-direction: column;
}

.activity-log::-webkit-scrollbar {
  width: 6px;
}

.activity-log::-webkit-scrollbar-track {
  background: var(--bg-secondary);
}

.activity-log::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 3px;
}

.activity-log::-webkit-scrollbar-thumb:hover {
  background: var(--border-hover);
}

/* Welcome Message */
.welcome-message {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: flex-start;
  text-align: center;
  padding: var(--spacing-lg) var(--spacing-2xl);
  color: var(--text-muted);
  min-height: 0; /* Allow shrinking */
  max-height: none; /* Remove any height constraints */
  overflow: visible; /* Ensure content is not clipped */
}

.welcome-text h4 {
  font-size: var(--font-size-lg);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
  margin-bottom: var(--spacing-sm);
}

.welcome-text p {
  font-size: var(--font-size-sm);
  line-height: 1.6;
  margin-bottom: var(--spacing-xl);
}

/* Quick Tasks */
.quick-tasks {
  display: grid;
  grid-template-columns: 1fr;
  gap: var(--spacing-md);
  width: 100%;
  max-width: 400px;
  min-height: 0; /* Ensure all items are visible */
  overflow: visible; /* Ensure content is not clipped */
  grid-auto-rows: auto; /* Let rows size automatically */
}

.task-suggestion {
  display: flex;
  align-items: flex-start;
  gap: var(--spacing-md);
  padding: var(--spacing-lg);
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-lg);
  cursor: pointer;
  transition: all var(--transition-fast);
  text-align: left;
}

.task-suggestion:hover {
  border-color: var(--primary-color);
  background: var(--bg-hover);
  transform: translateY(-2px);
  box-shadow: var(--shadow-md);
}

.task-icon {
  font-size: 24px;
  flex-shrink: 0;
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: var(--primary-color);
  border-radius: var(--radius-md);
  color: white;
}

.task-content {
  flex: 1;
}

.task-title {
  font-weight: var(--font-weight-semibold);
  color: var(--text-primary);
  margin-bottom: var(--spacing-xs);
  font-size: var(--font-size-base);
}

.task-description {
  font-size: var(--font-size-sm);
  color: var(--text-secondary);
  line-height: 1.4;
}

/* Activity Log Items - Chat Style */
.activity-item {
  margin-bottom: var(--spacing-lg);
  width: 100%;
  display: flex;
  flex-direction: column;
}

.message-container {
  display: flex;
  flex-direction: column;
  max-width: 85%;
  min-width: 0; /* Allow shrinking */
  animation: fadeIn 0.3s ease-out;
}

.user-container {
  align-self: flex-end;
  align-items: flex-end;
}

.agent-container {
  align-self: flex-start;
  align-items: flex-start;
}

.message-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--spacing-xs);
  font-size: var(--font-size-xs);
  color: var(--text-muted);
}

.user-container .message-header {
  flex-direction: row-reverse;
  gap: var(--spacing-md);
}

.agent-container .message-header {
  flex-direction: row;
  gap: var(--spacing-md);
}

.agent-name {
  font-weight: var(--font-weight-medium);
  color: var(--text-secondary);
  flex-shrink: 0;
}

.user-container .agent-name {
  color: var(--primary-color);
}

.message-metadata {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  font-size: 10px;
  flex-shrink: 0;
}

.message-time {
  font-size: 10px;
  color: var(--text-muted);
}

.message-metrics {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.metric-item {
  font-size: 10px;
  color: var(--text-muted);
  background-color: var(--bg-tertiary);
  padding: 1px 4px;
  border-radius: var(--radius-sm);
  font-weight: var(--font-weight-normal);
}

.user-container .metric-item {
  background-color: rgba(255, 255, 255, 0.2);
  color: rgba(255, 255, 255, 0.9);
}

/* Message Bubbles */
.message-bubble {
  padding: var(--spacing-md);
  border-radius: var(--radius-lg);
  position: relative;
  word-wrap: break-word;
  box-shadow: var(--shadow-sm);
  min-width: 0; /* Allow shrinking */
  max-width: 100%;
  overflow-wrap: break-word;
}

/* Copy Message Button */
.copy-message-btn {
  position: absolute;
  bottom: 8px;
  right: 8px;
  background: rgba(0, 0, 0, 0.7);
  border: none;
  border-radius: var(--radius-sm);
  padding: 6px;
  cursor: pointer;
  opacity: 0;
  visibility: hidden;
  transition: all var(--transition-fast);
  display: flex;
  align-items: center;
  justify-content: center;
  color: white;
  font-size: 12px;
  z-index: 10;
  backdrop-filter: blur(4px);
}

.copy-message-btn:hover {
  background: rgba(0, 0, 0, 0.8);
  transform: scale(1.05);
}

.copy-message-btn:active {
  transform: scale(0.95);
}

.copy-message-btn svg {
  width: 12px;
  height: 12px;
}

/* Show copy button on message bubble hover */
.message-bubble:hover .copy-message-btn {
  opacity: 1;
  visibility: visible;
}

/* User bubble copy button styling */
.user-bubble .copy-message-btn {
  background: rgba(255, 255, 255, 0.2);
  color: white;
}

.user-bubble .copy-message-btn:hover {
  background: rgba(255, 255, 255, 0.3);
}

.user-bubble {
  background-color: var(--primary-color);
  color: white;
  border-bottom-right-radius: var(--radius-sm);
}

.agent-bubble {
  background-color: var(--bg-secondary);
  color: var(--text-primary);
  border: 1px solid var(--border-color);
  border-bottom-left-radius: var(--radius-sm);
}

.message-bubble.working {
  border-left: 3px solid var(--warning-color);
  background-color: rgba(255, 193, 7, 0.05);
}

.message-bubble.result,
.message-bubble.done {
  border-left: 3px solid var(--accent-color);
  background-color: rgba(40, 167, 69, 0.05);
}

.message-bubble.error {
  border-left: 3px solid var(--danger-color);
  background-color: rgba(220, 53, 69, 0.05);
}

/* Message Status */
.message-status {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  margin-bottom: var(--spacing-sm);
  font-size: var(--font-size-xs);
  font-weight: var(--font-weight-medium);
}

.user-bubble .message-status {
  color: rgba(255, 255, 255, 0.9);
}

.agent-bubble .message-status {
  color: var(--text-secondary);
}

.status-indicator {
  font-size: 12px;
}

.status-text {
  text-transform: capitalize;
}

/* Message Content */
.message-content {
  line-height: 1.6;
  font-size: var(--font-size-sm);
  min-width: 0; /* Allow shrinking */
  overflow-wrap: break-word;
}

.user-bubble .message-content {
  color: white;
}

.agent-bubble .message-content {
  color: var(--text-primary);
}

/* Content formatting */
.message-content h1,
.message-content h2,
.message-content h3 {
  margin: var(--spacing-sm) 0;
  font-weight: var(--font-weight-semibold);
}

.message-content h1 {
  font-size: var(--font-size-lg);
}

.message-content h2 {
  font-size: var(--font-size-base);
}

.message-content h3 {
  font-size: var(--font-size-sm);
}

.message-content ul {
  margin: var(--spacing-sm) 0;
  padding-left: var(--spacing-lg);
}

.message-content li {
  margin-bottom: var(--spacing-xs);
}

.message-content .code-block {
  background-color: var(--bg-tertiary);
  padding: var(--spacing-sm);
  border-radius: var(--radius-sm);
  overflow-x: auto;
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: var(--font-size-xs);
  margin: var(--spacing-sm) 0;
  border: 1px solid var(--border-color);
  max-width: 100%;
  white-space: pre-wrap;
  word-break: break-all;
}

/* Custom scrollbar for code blocks */
.message-content .code-block::-webkit-scrollbar {
  height: 6px;
}

.message-content .code-block::-webkit-scrollbar-track {
  background: var(--bg-secondary);
  border-radius: 3px;
}

.message-content .code-block::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 3px;
}

.message-content .code-block::-webkit-scrollbar-thumb:hover {
  background: var(--border-hover);
}

.user-bubble .code-block {
  background-color: rgba(255, 255, 255, 0.1);
  border-color: rgba(255, 255, 255, 0.2);
}

.message-content .inline-code {
  background-color: var(--bg-tertiary);
  padding: 2px 4px;
  border-radius: var(--radius-sm);
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: var(--font-size-xs);
  word-break: break-all;
  overflow-wrap: break-word;
}

.user-bubble .inline-code {
  background-color: rgba(255, 255, 255, 0.2);
}

.message-content .json-content {
  background-color: var(--bg-tertiary);
  padding: var(--spacing-sm);
  border-radius: var(--radius-sm);
  overflow-x: auto;
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: var(--font-size-xs);
  margin: var(--spacing-sm) 0;
  border: 1px solid var(--border-color);
  max-width: 100%;
  white-space: pre-wrap;
  word-break: break-all;
}

/* Custom scrollbar for JSON content */
.message-content .json-content::-webkit-scrollbar {
  height: 6px;
}

.message-content .json-content::-webkit-scrollbar-track {
  background: var(--bg-secondary);
  border-radius: 3px;
}

.message-content .json-content::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 3px;
}

.message-content .json-content::-webkit-scrollbar-thumb:hover {
  background: var(--border-hover);
}

.user-bubble .json-content {
  background-color: rgba(255, 255, 255, 0.1);
  border-color: rgba(255, 255, 255, 0.2);
}

.message-content a {
  color: inherit;
  text-decoration: underline;
}

.user-bubble .message-content a {
  color: rgba(255, 255, 255, 0.9);
}

.agent-bubble .message-content a {
  color: var(--primary-color);
}

.message-content a:hover {
  opacity: 0.8;
}

/* Status indicators */
.status-indicator {
  display: inline-flex;
  align-items: center;
  gap: var(--spacing-xs);
  font-size: var(--font-size-xs);
  font-weight: var(--font-weight-medium);
}

.status-dot {
  width: 6px;
  height: 6px;
  border-radius: 50%;
  background-color: var(--border-color);
}

.status-dot.active {
  background-color: var(--accent-color);
  animation: pulse 2s infinite;
}

.status-dot.error {
  background-color: var(--danger-color);
}

/* Enhanced Markdown Styles */
.message-content .task-item {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  margin: var(--spacing-xs) 0;
  padding: var(--spacing-xs) 0;
  line-height: 1.4;
}

.message-content .task-item input[type="checkbox"] {
  margin: 0;
  cursor: default;
  accent-color: var(--primary-color);
}

.message-content .markdown-table {
  border-collapse: collapse;
  width: 100%;
  margin: var(--spacing-md) 0;
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  overflow: hidden;
  font-size: var(--font-size-sm);
}

.message-content .markdown-table th,
.message-content .markdown-table td {
  padding: var(--spacing-sm) var(--spacing-md);
  text-align: left;
  border-bottom: 1px solid var(--border-color);
}

.message-content .markdown-table th {
  background-color: var(--bg-tertiary);
  font-weight: var(--font-weight-semibold);
  color: var(--text-primary);
}

.user-bubble .markdown-table th {
  background-color: rgba(255, 255, 255, 0.1);
  color: white;
}

.message-content .markdown-table tr:hover {
  background-color: var(--bg-hover);
}

.user-bubble .markdown-table tr:hover {
  background-color: rgba(255, 255, 255, 0.05);
}

.message-content .markdown-quote {
  border-left: 4px solid var(--primary-color);
  padding-left: var(--spacing-md);
  margin: var(--spacing-md) 0;
  color: var(--text-secondary);
  font-style: italic;
  background-color: var(--bg-secondary);
  padding: var(--spacing-md);
  border-radius: var(--radius-md);
}

.user-bubble .markdown-quote {
  border-left-color: rgba(255, 255, 255, 0.8);
  background-color: rgba(255, 255, 255, 0.1);
  color: rgba(255, 255, 255, 0.9);
}

.message-content h1,
.message-content h2,
.message-content h3,
.message-content h4,
.message-content h5,
.message-content h6 {
  margin: var(--spacing-md) 0 var(--spacing-sm) 0;
  color: var(--text-primary);
  line-height: 1.3;
}

.user-bubble .message-content h1,
.user-bubble .message-content h2,
.user-bubble .message-content h3,
.user-bubble .message-content h4,
.user-bubble .message-content h5,
.user-bubble .message-content h6 {
  color: white;
}

.message-content h1 { font-size: var(--font-size-xl); font-weight: var(--font-weight-bold); }
.message-content h2 { font-size: var(--font-size-lg); font-weight: var(--font-weight-semibold); }
.message-content h3 { font-size: var(--font-size-base); font-weight: var(--font-weight-semibold); }
.message-content h4 { font-size: var(--font-size-sm); font-weight: var(--font-weight-medium); }

.message-content ul,
.message-content ol {
  margin: var(--spacing-sm) 0;
  padding-left: var(--spacing-xl);
}

.message-content li {
  margin: var(--spacing-xs) 0;
  line-height: 1.5;
}

.message-content p {
  margin: var(--spacing-sm) 0;
  line-height: 1.6;
}

.message-content a {
  color: var(--primary-color);
  text-decoration: none;
  border-bottom: 1px solid transparent;
  transition: border-color var(--transition-fast);
}

.message-content a:hover {
  border-bottom-color: var(--primary-color);
}

.user-bubble .message-content a {
  color: rgba(255, 255, 255, 0.9);
  border-bottom-color: transparent;
}

.user-bubble .message-content a:hover {
  border-bottom-color: rgba(255, 255, 255, 0.9);
}

.message-content hr {
  border: none;
  border-top: 1px solid var(--border-color);
  margin: var(--spacing-lg) 0;
}

.user-bubble .message-content hr {
  border-top-color: rgba(255, 255, 255, 0.3);
}

.message-content strong {
  font-weight: var(--font-weight-semibold);
}

.message-content em {
  font-style: italic;
}

/* Suggestion Tasks Styles */
.suggestion-tasks-container {
  margin: var(--spacing-lg) 0;
  padding: var(--spacing-md);
  background: linear-gradient(135deg, rgba(0, 122, 204, 0.02), rgba(0, 122, 204, 0.05));
  border: 1px solid rgba(0, 122, 204, 0.1);
  border-radius: var(--radius-xl);
  position: relative;
  overflow: visible;
  display: block;
  width: 100%;
  box-sizing: border-box;
  min-height: auto;
}

.suggestion-tasks-container::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 3px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  border-radius: var(--radius-xl) var(--radius-xl) 0 0;
}

.suggestion-tasks-header {
  text-align: center;
  margin-bottom: var(--spacing-md);
  padding: 0;
  display: block;
  width: 100%;
}

.suggestion-tasks-header h4 {
  font-size: var(--font-size-xl);
  font-weight: var(--font-weight-bold);
  color: var(--text-primary);
  margin: 0;
  display: block;
  text-align: center;
  line-height: 1.3;
  white-space: normal;
  overflow: visible;
  word-wrap: break-word;
}

.suggestion-tasks-header p {
  font-size: var(--font-size-sm);
  color: var(--text-secondary);
  margin: 0;
  opacity: 0.8;
  line-height: 1.4;
  display: block;
}

.suggestion-cards {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-sm);
  width: 100%;
  min-height: auto;
}

.suggestion-task-card {
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
  padding: var(--spacing-md);
  background: var(--bg-primary);
  border: 2px solid var(--border-color);
  border-radius: var(--radius-lg);
  cursor: pointer;
  transition: all var(--transition-fast);
  position: relative;
  overflow: visible;
  width: 100%;
  min-height: 50px;
  box-sizing: border-box;
  margin-bottom: 0;
}

.suggestion-task-card:last-child {
  margin-bottom: 0;
}

.suggestion-task-card::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 4px;
  background: var(--primary-color);
  transform: scaleX(0);
  transform-origin: left;
  transition: transform var(--transition-fast);
}

.suggestion-task-card:hover {
  border-color: var(--primary-color);
  background: var(--bg-hover);
  transform: translateY(-2px);
  box-shadow: var(--shadow-lg);
}

.suggestion-task-card:hover::before {
  transform: scaleX(1);
}

.suggestion-task-card:active {
  transform: translateY(-1px);
  box-shadow: var(--shadow-md);
}

.suggestion-card-icon {
  flex-shrink: 0;
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: linear-gradient(135deg, var(--primary-color), var(--accent-color));
  border-radius: var(--radius-md);
  color: white;
  transition: transform var(--transition-fast);
}

.suggestion-task-card:hover .suggestion-card-icon {
  transform: scale(1.1);
}

.suggestion-card-content {
  flex: 1;
  min-width: 0;
  display: flex;
  flex-direction: column;
  justify-content: center;
}

.suggestion-task-text {
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  line-height: 1.5;
  font-weight: var(--font-weight-medium);
  word-wrap: break-word;
  overflow-wrap: break-word;
  white-space: normal;
  text-overflow: clip;
}

.suggestion-tasks-section {
  margin: var(--spacing-lg) 0;
  width: 100%;
}

.suggestion-tasks-header {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  margin-bottom: var(--spacing-md);
  padding: 0 var(--spacing-sm);
  min-height: 24px;
  width: 100%;
  overflow: visible;
}

.suggestion-tasks-header h4 {
  font-size: var(--font-size-base);
  font-weight: var(--font-weight-bold);
  color: var(--text-primary);
  margin: 0;
  white-space: nowrap;
  overflow: visible;
  text-overflow: clip;
}

.suggestion-cards-container {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-md);
  width: 100%;
  padding: 0 var(--spacing-sm);
}

.suggestion-card-arrow {
  flex-shrink: 0;
  width: 24px;
  height: 24px;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--text-muted);
  transition: all var(--transition-fast);
}

.suggestion-task-card:hover .suggestion-card-arrow {
  color: var(--primary-color);
  transform: translateX(4px);
}

/* Animation for suggestion cards */
.suggestion-tasks-container.fade-in {
  animation: suggestionFadeIn 0.5s ease-out;
}

@keyframes suggestionFadeIn {
  from {
    opacity: 0;
    transform: translateY(20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Responsive design for suggestion cards */
@media (max-width: 480px) {
  .suggestion-tasks-container {
    margin: var(--spacing-lg) 0;
    padding: var(--spacing-md);
  }
  
  .suggestion-task-card {
    padding: var(--spacing-md);
    gap: var(--spacing-sm);
  }
  
  .suggestion-card-icon {
    width: 32px;
    height: 32px;
  }
  
  .suggestion-card-icon svg {
    width: 14px;
    height: 14px;
  }
  
  .suggestion-task-text {
    font-size: var(--font-size-xs);
  }
  
  .suggestion-tasks-header h4 {
    font-size: var(--font-size-base);
  }
}

/* Loading state for suggestion cards */
.suggestion-task-card.loading {
  pointer-events: none;
  opacity: 0.6;
}

.suggestion-task-card.loading .suggestion-card-icon {
  animation: pulse 1.5s ease-in-out infinite;
}

@keyframes pulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: 0.7;
  }
}

/* Success state when card is clicked */
.suggestion-task-card.success {
  border-color: var(--accent-color);
  background: rgba(40, 167, 69, 0.05);
}

.suggestion-task-card.success .suggestion-card-icon {
  background: var(--accent-color);
}


================================================
FILE: vibe_surf/chrome_extension/styles/animations.css
================================================
/* Animations CSS - Transitions, Keyframes, and Interactive States */

/* Fade Animations */
.fade-in {
  animation: fadeIn var(--transition-normal) ease-out;
}

.fade-out {
  animation: fadeOut var(--transition-normal) ease-out;
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes fadeOut {
  from {
    opacity: 1;
    transform: translateY(0);
  }
  to {
    opacity: 0;
    transform: translateY(-10px);
  }
}

/* Slide Animations */
.slide-in-right {
  animation: slideInRight var(--transition-normal) ease-out;
}

.slide-in-left {
  animation: slideInLeft var(--transition-normal) ease-out;
}

.slide-in-up {
  animation: slideInUp var(--transition-normal) ease-out;
}

.slide-in-down {
  animation: slideInDown var(--transition-normal) ease-out;
}

@keyframes slideInRight {
  from {
    opacity: 0;
    transform: translateX(20px);
  }
  to {
    opacity: 1;
    transform: translateX(0);
  }
}

@keyframes slideInLeft {
  from {
    opacity: 0;
    transform: translateX(-20px);
  }
  to {
    opacity: 1;
    transform: translateX(0);
  }
}

@keyframes slideInUp {
  from {
    opacity: 0;
    transform: translateY(20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes slideInDown {
  from {
    opacity: 0;
    transform: translateY(-20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Scale Animations */
.scale-in {
  animation: scaleIn var(--transition-normal) ease-out;
}

.scale-out {
  animation: scaleOut var(--transition-normal) ease-out;
}

@keyframes scaleIn {
  from {
    opacity: 0;
    transform: scale(0.95);
  }
  to {
    opacity: 1;
    transform: scale(1);
  }
}

@keyframes scaleOut {
  from {
    opacity: 1;
    transform: scale(1);
  }
  to {
    opacity: 0;
    transform: scale(0.95);
  }
}

/* Loading Animations */
.loading-dots {
  animation: loadingDots 1.5s infinite ease-in-out;
}

.loading-dots::after {
  content: '';
  animation: loadingDotsAfter 1.5s infinite ease-in-out;
}

@keyframes loadingDots {
  0%, 20% {
    content: '.';
  }
  40% {
    content: '..';
  }
  60%, 100% {
    content: '...';
  }
}

/* Bounce Animation */
.bounce {
  animation: bounce 0.6s ease-in-out;
}

@keyframes bounce {
  0%, 20%, 60%, 100% {
    transform: translateY(0);
  }
  40% {
    transform: translateY(-10px);
  }
  80% {
    transform: translateY(-5px);
  }
}

/* Shake Animation */
.shake {
  animation: shake 0.5s ease-in-out;
}

@keyframes shake {
  0%, 100% {
    transform: translateX(0);
  }
  10%, 30%, 50%, 70%, 90% {
    transform: translateX(-3px);
  }
  20%, 40%, 60%, 80% {
    transform: translateX(3px);
  }
}

/* Typing Animation */
.typing-indicator {
  display: inline-flex;
  align-items: center;
  gap: 2px;
}

.typing-dot {
  width: 4px;
  height: 4px;
  border-radius: 50%;
  background-color: var(--text-muted);
  animation: typingDot 1.5s infinite ease-in-out;
}

.typing-dot:nth-child(1) {
  animation-delay: 0s;
}

.typing-dot:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-dot:nth-child(3) {
  animation-delay: 0.4s;
}

@keyframes typingDot {
  0%, 60%, 100% {
    opacity: 0.3;
    transform: scale(0.8);
  }
  30% {
    opacity: 1;
    transform: scale(1);
  }
}

/* Progress Bar Animation */
.progress-bar {
  width: 100%;
  height: 4px;
  background-color: var(--bg-tertiary);
  border-radius: 2px;
  overflow: hidden;
  position: relative;
}

.progress-bar-fill {
  height: 100%;
  background-color: var(--primary-color);
  border-radius: 2px;
  transition: width var(--transition-slow) ease-out;
}

.progress-bar-indeterminate {
  position: relative;
  background-color: var(--bg-tertiary);
  overflow: hidden;
}

.progress-bar-indeterminate::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background-color: var(--primary-color);
  animation: progressIndeterminate 2s infinite linear;
}

@keyframes progressIndeterminate {
  0% {
    left: -100%;
  }
  100% {
    left: 100%;
  }
}

/* Hover Effects */
.hover-lift {
  transition: all var(--transition-fast);
}

.hover-lift:hover {
  transform: translateY(-2px);
  box-shadow: var(--shadow-md);
}

.hover-scale {
  transition: transform var(--transition-fast);
}

.hover-scale:hover {
  transform: scale(1.02);
}

.hover-glow {
  transition: box-shadow var(--transition-fast);
}

.hover-glow:hover {
  box-shadow: 0 0 20px rgba(0, 122, 204, 0.3);
}

/* Focus Effects */
.focus-ring {
  transition: all var(--transition-fast);
}

.focus-ring:focus {
  outline: none;
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.focus-ring:focus-visible {
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.3);
}

/* State Transitions */
.state-transition {
  transition: all var(--transition-normal);
}

/* Smooth scroll */
.smooth-scroll {
  scroll-behavior: smooth;
}

/* Text animations */
.text-shimmer {
  background: linear-gradient(
    90deg,
    var(--text-muted) 0%,
    var(--text-primary) 50%,
    var(--text-muted) 100%
  );
  background-size: 200% 100%;
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  animation: shimmer 2s infinite ease-in-out;
}

@keyframes shimmer {
  0% {
    background-position: -200% 0;
  }
  100% {
    background-position: 200% 0;
  }
}

/* Ripple Effect */
.ripple {
  position: relative;
  overflow: hidden;
}

.ripple::before {
  content: '';
  position: absolute;
  top: 50%;
  left: 50%;
  width: 0;
  height: 0;
  border-radius: 50%;
  background-color: rgba(255, 255, 255, 0.5);
  transform: translate(-50%, -50%);
  transition: width 0.6s, height 0.6s;
}

.ripple:active::before {
  width: 300px;
  height: 300px;
}

/* Breathing animation for active states */
.breathing {
  animation: breathing 3s infinite ease-in-out;
}

@keyframes breathing {
  0%, 100% {
    opacity: 0.8;
  }
  50% {
    opacity: 1;
  }
}

/* Slide animation for panels */
.slide-panel-enter {
  transform: translateX(100%);
  opacity: 0;
}

.slide-panel-enter-active {
  transform: translateX(0);
  opacity: 1;
  transition: all var(--transition-slow) ease-out;
}

.slide-panel-exit {
  transform: translateX(0);
  opacity: 1;
}

.slide-panel-exit-active {
  transform: translateX(100%);
  opacity: 0;
  transition: all var(--transition-slow) ease-in;
}

/* Stagger animation for list items */
.stagger-item {
  opacity: 0;
  transform: translateY(20px);
  animation: staggerIn var(--transition-normal) ease-out forwards;
}

.stagger-item:nth-child(1) { animation-delay: 0ms; }
.stagger-item:nth-child(2) { animation-delay: 50ms; }
.stagger-item:nth-child(3) { animation-delay: 100ms; }
.stagger-item:nth-child(4) { animation-delay: 150ms; }
.stagger-item:nth-child(5) { animation-delay: 200ms; }
.stagger-item:nth-child(n+6) { animation-delay: 250ms; }

@keyframes staggerIn {
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Loading skeleton animation */
.skeleton {
  background: linear-gradient(
    90deg,
    var(--bg-secondary) 25%,
    var(--bg-tertiary) 50%,
    var(--bg-secondary) 75%
  );
  background-size: 200% 100%;
  animation: skeleton 1.5s infinite ease-in-out;
  border-radius: var(--radius-sm);
}

@keyframes skeleton {
  0% {
    background-position: -200% 0;
  }
  100% {
    background-position: 200% 0;
  }
}

/* Micro-interactions */
.micro-bounce {
  transition: transform 0.1s ease-out;
}

.micro-bounce:active {
  transform: scale(0.98);
}

.micro-rotate {
  transition: transform var(--transition-fast);
}

.micro-rotate:hover {
  transform: rotate(5deg);
}

/* Reduced motion preferences */
@media (prefers-reduced-motion: reduce) {
  *,
  *::before,
  *::after {
    animation-duration: 0.01ms !important;
    animation-iteration-count: 1 !important;
    transition-duration: 0.01ms !important;
  }
  
  .smooth-scroll {
    scroll-behavior: auto;
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/base.css
================================================
/* Base Styles - VibeSurf Extension */

/* Reset and Base Styles */
* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
  font-size: var(--font-size-base);
  line-height: 1.5;
  color: var(--text-primary);
  background-color: var(--bg-primary);
  overflow: hidden;
}

/* Utility Classes */
.hidden {
  display: none !important;
}

.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border: 0;
}

/* Animation for new content */
@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.fade-in {
  animation: fadeIn 0.3s ease-out;
}

@keyframes pulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: 0.5;
  }
}

/* Accessibility Enhancements */
@media (prefers-reduced-motion: reduce) {
  * {
    animation-duration: 0.01ms !important;
    animation-iteration-count: 1 !important;
    transition-duration: 0.01ms !important;
  }
}

/* High Contrast Mode Support */
@media (prefers-contrast: high) {
  :root {
    --border-color: #000000;
    --text-muted: #333333;
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/components.css
================================================
/* Components CSS - Modals, Forms, and Complex Components */

/* Modal Styles - Base modal container */
.modal {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 1000;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: var(--spacing-lg);
}

/* Hidden state for static modals */
.modal.hidden {
  display: none;
}

/* Legacy modal overlay for existing modals */
.modal-overlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  backdrop-filter: blur(4px);
}

/* Dynamic modal overlay for modal-manager.js */
.modal-overlay.dynamic-modal {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 10000;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: var(--spacing-lg);
  background-color: rgba(0, 0, 0, 0.5);
  backdrop-filter: blur(4px);
  opacity: 0;
  visibility: hidden;
  transition: all var(--transition-fast);
}

.modal-overlay.dynamic-modal.show {
  opacity: 1;
  visibility: visible;
}

.modal-content {
  position: relative;
  width: 100%;
  max-width: 500px;
  max-height: 90vh;
  background-color: var(--bg-primary);
  border-radius: var(--radius-xl);
  box-shadow: var(--shadow-lg);
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

/* Modal Body - for dynamic modals from modal-manager.js */
.modal-body {
  flex: 1;
  padding: var(--spacing-lg) var(--spacing-xl);
  overflow-y: auto;
  display: flex;
  flex-direction: column;
  align-items: center;
  text-align: center;
  gap: var(--spacing-md);
}

.modal-body .warning-icon {
  font-size: 2rem;
  margin-bottom: var(--spacing-sm);
}

.modal-body p {
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  line-height: 1.6;
  margin: 0;
}

/* Modal Footer - for dynamic modals from modal-manager.js */
.modal-footer {
  display: flex;
  gap: var(--spacing-sm);
  justify-content: center;
  padding: var(--spacing-lg) var(--spacing-xl) var(--spacing-xl);
  border-top: 1px solid var(--border-color);
  background-color: var(--bg-secondary);
}

.modal-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-xl) var(--spacing-xl) var(--spacing-lg);
  border-bottom: 1px solid var(--border-color);
}

.modal-header h3 {
  font-size: var(--font-size-xl);
  font-weight: var(--font-weight-semibold);
  color: var(--text-primary);
}

.modal-close {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border: none;
  border-radius: var(--radius-md);
  background-color: transparent;
  color: var(--text-secondary);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.modal-close:hover {
  background-color: var(--bg-hover);
  color: var(--text-primary);
}

/* Modal Close Button - alternative class for dynamic modals */
.modal-close-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border: none;
  border-radius: var(--radius-md);
  background-color: transparent;
  color: var(--text-secondary);
  cursor: pointer;
  transition: all var(--transition-fast);
  font-size: 18px;
  line-height: 1;
}

.modal-close-btn:hover {
  background-color: var(--bg-hover);
  color: var(--text-primary);
}

/* History Modal */
.history-list {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-lg) var(--spacing-xl);
  max-height: 60vh;
}

.history-item {
  padding: var(--spacing-lg);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-lg);
  margin-bottom: var(--spacing-md);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.history-item:hover {
  border-color: var(--primary-color);
  background-color: var(--bg-active);
}

.history-item:last-child {
  margin-bottom: 0;
}

.history-item-header {
  display: flex;
  align-items: center;
  justify-content: between;
  margin-bottom: var(--spacing-sm);
}

.history-session-id {
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: var(--font-size-xs);
  background-color: var(--bg-tertiary);
  padding: 2px 6px;
  border-radius: var(--radius-sm);
  color: var(--text-secondary);
}

.history-timestamp {
  font-size: var(--font-size-xs);
  color: var(--text-muted);
  margin-left: auto;
}

.history-task {
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  line-height: 1.5;
  margin-bottom: var(--spacing-sm);
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
}

.history-status {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  font-size: var(--font-size-xs);
  font-weight: var(--font-weight-medium);
}

/* Settings Modal */
.settings-content {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-lg) var(--spacing-xl);
  max-height: 70vh;
}

.settings-section {
  margin-bottom: var(--spacing-2xl);
}

.settings-section:last-child {
  margin-bottom: 0;
}

.settings-section h4 {
  font-size: var(--font-size-lg);
  font-weight: var(--font-weight-semibold);
  color: var(--text-primary);
  margin-bottom: var(--spacing-lg);
}

.profiles-list {
  space-y: var(--spacing-md);
  margin-bottom: var(--spacing-lg);
}

.profile-item {
  padding: var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-lg);
  background-color: var(--bg-secondary);
  margin-bottom: var(--spacing-md);
}

.profile-item:last-child {
  margin-bottom: 0;
}

.profile-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--spacing-sm);
}

.profile-name {
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
  flex: 1;
}

.profile-badges {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  margin-right: var(--spacing-sm);
}

.profile-status {
  font-size: var(--font-size-xs);
  padding: 2px 6px;
  border-radius: var(--radius-sm);
  font-weight: var(--font-weight-medium);
}

.profile-status.active {
  background-color: rgba(40, 167, 69, 0.1);
  color: var(--accent-color);
}

.profile-status.inactive {
  background-color: rgba(108, 117, 125, 0.1);
  color: var(--text-secondary);
}

.profile-actions {
  display: flex;
  gap: var(--spacing-xs);
}

.profile-btn {
  padding: var(--spacing-xs) var(--spacing-sm);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-sm);
  background-color: var(--bg-primary);
  color: var(--text-secondary);
  font-size: var(--font-size-xs);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.profile-btn:hover {
  border-color: var(--border-hover);
  color: var(--text-primary);
}

.profile-btn.danger {
  color: var(--danger-color);
  border-color: var(--danger-color);
}

.profile-btn.danger:hover {
  background-color: rgba(220, 53, 69, 0.1);
}

.profile-details {
  font-size: var(--font-size-sm);
  color: var(--text-secondary);
  line-height: 1.5;
}

.profile-details .detail-item {
  margin-bottom: var(--spacing-xs);
}

.profile-details .detail-label {
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
}

.add-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 100%;
  padding: var(--spacing-md);
  border: 2px dashed var(--border-color);
  border-radius: var(--radius-lg);
  background-color: transparent;
  color: var(--text-secondary);
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.add-btn:hover {
  border-color: var(--primary-color);
  color: var(--primary-color);
  background-color: rgba(0, 122, 204, 0.05);
}

.config-item {
  margin-bottom: var(--spacing-lg);
}

.config-item:last-child {
  margin-bottom: 0;
}

.config-item label {
  display: block;
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
  margin-bottom: var(--spacing-sm);
}

.config-item input {
  width: 100%;
  padding: var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  background-color: var(--bg-primary);
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  transition: border-color var(--transition-fast);
}

.config-item input:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.config-item input::placeholder {
  color: var(--text-muted);
}

/* Form Components */
.form-group {
  margin-bottom: var(--spacing-lg);
}

.form-label {
  display: block;
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
  margin-bottom: var(--spacing-sm);
}

.form-input {
  width: 100%;
  padding: var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  background-color: var(--bg-primary);
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  transition: all var(--transition-fast);
}

.form-input:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.form-input::placeholder {
  color: var(--text-muted);
}

.form-textarea {
  resize: vertical;
  min-height: 100px;
}

.form-select {
  appearance: none;
  background-image: url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 20 20'%3e%3cpath stroke='%236b7280' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='m6 8 4 4 4-4'/%3e%3c/svg%3e");
  background-position: right 8px center;
  background-repeat: no-repeat;
  background-size: 16px;
  padding-right: 32px;
}

/* Form input error state */
.form-input.form-error {
  border-color: var(--danger-color) !important;
  box-shadow: 0 0 0 3px rgba(220, 53, 69, 0.1) !important;
}

.form-input.form-error:focus {
  border-color: var(--danger-color) !important;
  box-shadow: 0 0 0 3px rgba(220, 53, 69, 0.2) !important;
}

/* Form error message */
.form-error-message {
  font-size: var(--font-size-xs);
  color: var(--danger-color);
  margin-top: var(--spacing-xs);
  display: block;
  font-weight: var(--font-weight-medium);
  animation: slideDown 0.2s ease-out;
}

.profile-name-error {
  background-color: rgba(220, 53, 69, 0.05);
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-sm);
  border: 1px solid rgba(220, 53, 69, 0.2);
}

@keyframes slideDown {
  from {
    opacity: 0;
    transform: translateY(-10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Legacy form error class for backwards compatibility */
.form-error {
  font-size: var(--font-size-xs);
  color: var(--danger-color);
  margin-top: var(--spacing-xs);
}

.form-help {
  font-size: var(--font-size-xs);
  color: var(--text-muted);
  margin-top: var(--spacing-xs);
}

/* Loading Overlay */
.loading-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(255, 255, 255, 0.9);
  backdrop-filter: blur(4px);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  z-index: 2000;
}

.loading-spinner {
  width: 40px;
  height: 40px;
  border: 3px solid var(--border-color);
  border-top: 3px solid var(--primary-color);
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: var(--spacing-lg);
}

.loading-text {
  font-size: var(--font-size-sm);
  color: var(--text-secondary);
  font-weight: var(--font-weight-medium);
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

/* Button Groups */
.btn-group {
  display: flex;
  gap: var(--spacing-sm);
}

.btn-group.full-width {
  width: 100%;
}

.btn-group.full-width > * {
  flex: 1;
}

/* Badge Components */
.badge {
  display: inline-flex;
  align-items: center;
  padding: 2px 8px;
  border-radius: var(--radius-sm);
  font-size: var(--font-size-xs);
  font-weight: var(--font-weight-medium);
  line-height: 1;
}

.badge.primary {
  background-color: rgba(0, 122, 204, 0.1);
  color: var(--primary-color);
}

.badge.success {
  background-color: rgba(40, 167, 69, 0.1);
  color: var(--accent-color);
}

.badge.danger {
  background-color: rgba(220, 53, 69, 0.1);
  color: var(--danger-color);
}

.badge.warning {
  background-color: rgba(255, 193, 7, 0.1);
  color: var(--warning-color);
}

.badge.neutral {
  background-color: var(--bg-tertiary);
  color: var(--text-secondary);
}

/* Button Styles for Dynamic Modals */
.btn-primary, .btn-secondary {
  padding: 12px 24px;
  border: none;
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  cursor: pointer;
  transition: all var(--transition-fast);
  min-width: 100px;
}

.btn-primary {
  background-color: var(--primary-color);
  color: white;
}

.btn-primary:hover {
  background-color: var(--primary-hover);
  transform: translateY(-1px);
  box-shadow: 0 4px 16px rgba(0, 122, 204, 0.3);
}

.btn-secondary {
  background-color: var(--bg-primary);
  color: var(--text-primary);
  border: 1px solid var(--border-color);
}

.btn-secondary:hover {
  background-color: var(--bg-hover);
  border-color: var(--border-hover);
}

.btn-primary:disabled, .btn-secondary:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  transform: none !important;
  box-shadow: none !important;
}

/* Tooltip */
.tooltip {
  position: relative;
}

.tooltip::before {
  content: attr(data-tooltip);
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  background-color: var(--text-primary);
  color: var(--bg-primary);
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-sm);
  font-size: var(--font-size-xs);
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
  transition: opacity var(--transition-fast);
  margin-bottom: 4px;
  z-index: 1000;
}

.tooltip::after {
  content: '';
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  border: 4px solid transparent;
  border-top-color: var(--text-primary);
  opacity: 0;
  pointer-events: none;
  transition: opacity var(--transition-fast);
  z-index: 1000;
}

.tooltip:hover::before,
.tooltip:hover::after {
  opacity: 1;
}

/* Empty State */
.empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: var(--spacing-2xl);
  text-align: center;
  color: var(--text-muted);
}

.empty-state-icon {
  font-size: 3rem;
  margin-bottom: var(--spacing-lg);
  opacity: 0.5;
}

.empty-state-title {
  font-size: var(--font-size-lg);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
  margin-bottom: var(--spacing-sm);
}

.empty-state-description {
  font-size: var(--font-size-sm);
  line-height: 1.6;
  max-width: 300px;
}

/* Dynamic Modal Styles for modal-manager.js */
.modal-overlay.dynamic-modal .modal.warning-modal .modal-content {
  border: 2px solid var(--warning-color);
  animation: modalSlideIn 0.3s ease-out;
}

.modal-overlay.dynamic-modal .modal.warning-modal .modal-header {
  background-color: rgba(255, 193, 7, 0.1);
  border-bottom: 1px solid rgba(255, 193, 7, 0.2);
}

.modal-overlay.dynamic-modal .modal.warning-modal .modal-header h3 {
  color: var(--warning-color);
}

.modal-overlay.dynamic-modal .modal.warning-modal .warning-icon {
  color: var(--warning-color);
  filter: drop-shadow(0 2px 4px rgba(255, 193, 7, 0.3));
}

/* Confirm Modal enhancements */
.modal-overlay.dynamic-modal .modal.confirm-modal .modal-content {
  animation: modalSlideIn 0.3s ease-out;
}

/* Modal Animation */
@keyframes modalSlideIn {
  from {
    opacity: 0;
    transform: translateY(-20px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

/* Z-index management for different modal types */
.modal-overlay.dynamic-modal {
  z-index: 10000; /* Higher than static modals */
}

.modal-overlay.dynamic-modal.warning-modal {
  z-index: 15000; /* Highest priority for warning modals */
}

/* Legacy warning modal classes - keeping for backward compatibility but not used by modal-manager.js */
.warning-content {
  background-color: var(--bg-primary);
  border-radius: var(--radius-xl);
  box-shadow: var(--shadow-lg);
  max-width: 480px;
  width: 100%;
  max-height: 90vh;
  overflow: hidden;
  border: 2px solid var(--warning-color);
}

.warning-header {
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
  padding: var(--spacing-xl);
  background-color: rgba(255, 193, 7, 0.1);
  border-bottom: 1px solid rgba(255, 193, 7, 0.2);
}

.warning-header h3 {
  font-size: var(--font-size-lg);
  font-weight: var(--font-weight-semibold);
  color: var(--text-primary);
  margin: 0;
}

.warning-body {
  padding: var(--spacing-xl);
}

.warning-body p {
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  line-height: 1.6;
  margin: 0 0 var(--spacing-md) 0;
}

.warning-details {
  background-color: var(--bg-tertiary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  padding: var(--spacing-md);
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: var(--font-size-xs);
  color: var(--text-secondary);
  white-space: pre-wrap;
  margin: 0;
  overflow-x: auto;
}

.warning-actions {
  display: flex;
  gap: var(--spacing-sm);
  padding: var(--spacing-lg) var(--spacing-xl) var(--spacing-xl);
  border-top: 1px solid var(--border-color);
  justify-content: flex-end;
}

.warning-actions .btn {
  min-width: 100px;
}

.warning-actions .btn-secondary {
  background-color: var(--bg-secondary);
  color: var(--text-secondary);
  border: 1px solid var(--border-color);
}

.warning-actions .btn-secondary:hover {
  background-color: var(--bg-hover);
  color: var(--text-primary);
  border-color: var(--border-hover);
}

.warning-actions .btn-danger {
  background-color: var(--danger-color);
  color: white;
  border: 1px solid var(--danger-color);
}

.warning-actions .btn-danger:hover {
  background-color: #c82333;
  border-color: #bd2130;
}

/* Disabled state styles */
.form-input:disabled,
.form-select:disabled,
.form-textarea:disabled {
  background-color: var(--bg-tertiary);
  color: var(--text-muted);
  cursor: not-allowed;
  opacity: 0.6;
}

.btn:disabled {
  background-color: var(--bg-tertiary);
  color: var(--text-muted);
  border-color: var(--border-color);
  cursor: not-allowed;
  opacity: 0.6;
}

.btn:disabled:hover {
  background-color: var(--bg-tertiary);
  color: var(--text-muted);
  border-color: var(--border-color);
}

/* Voice recording disabled state */
.voice-disabled {
  opacity: 0.5;
  cursor: not-allowed !important;
  filter: grayscale(100%);
  position: relative;
}

.voice-disabled::after {
  content: 'ğŸ”‡';
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  font-size: 12px;
  opacity: 0.8;
}

.voice-disabled:hover {
  opacity: 0.5;
  filter: grayscale(100%);
}

/* Task running disabled state (existing) */
.task-running-disabled {
  opacity: 0.5;
  cursor: not-allowed !important;
  filter: grayscale(100%);
}

.task-running-disabled:hover {
  opacity: 0.5;
  filter: grayscale(100%);
}


================================================
FILE: vibe_surf/chrome_extension/styles/history-modal.css
================================================
/* History Modal Styles - VibeSurf Extension */

/* History Modal - Modern Design */
.history-modal-content {
  width: 92vw;
  max-width: 900px;
  min-height: 70vh;
  max-height: 85vh;
  display: flex;
  flex-direction: column;
  border-radius: 16px;
  box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
  background: var(--bg-primary);
  border: 1px solid var(--border-color);
  overflow: hidden;
}

.history-content {
  flex: 1;
  overflow: auto;
  display: flex;
  flex-direction: column;
  padding: 32px;
  padding-top: 24px;
}

.history-section {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: visible;
  min-height: 0;
}

.section-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0 0 24px 0;
  border-bottom: 2px solid var(--border-color);
  margin-bottom: 28px;
}

.section-header h4 {
  font-size: 24px;
  font-weight: 700;
  color: var(--text-primary);
  margin: 0;
  letter-spacing: -0.5px;
}

.section-subtitle {
  font-size: var(--font-size-sm);
  color: var(--text-muted);
  margin-top: 4px;
  font-weight: 500;
}

.back-btn {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 10px 20px;
  border: 2px solid var(--primary-color);
  border-radius: 12px;
  background-color: transparent;
  color: var(--primary-color);
  font-size: var(--font-size-sm);
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.back-btn:hover {
  background-color: var(--primary-color);
  color: white;
  transform: translateY(-1px);
  box-shadow: 0 4px 16px rgba(0, 122, 204, 0.2);
}

/* Recent Tasks List */
.recent-tasks-list {
  flex: 1;
  overflow-y: auto;
  margin-bottom: 28px;
  padding-right: 8px;
}

.recent-tasks-list::-webkit-scrollbar {
  width: 6px;
}

.recent-tasks-list::-webkit-scrollbar-track {
  background: var(--bg-tertiary);
  border-radius: 3px;
}

.recent-tasks-list::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 3px;
}

.recent-tasks-list::-webkit-scrollbar-thumb:hover {
  background: var(--text-secondary);
}

.recent-task-item {
  display: flex;
  align-items: flex-start;
  gap: 20px;
  padding: 24px;
  border: 1px solid var(--border-color);
  border-radius: 16px;
  background-color: var(--bg-secondary);
  margin-bottom: 20px;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
}

.recent-task-item::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 4px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  transform: scaleX(0);
  transition: transform 0.3s ease;
}

.recent-task-item:hover {
  border-color: var(--primary-color);
  background-color: var(--bg-hover);
  transform: translateY(-3px);
  box-shadow: 0 12px 32px rgba(0, 122, 204, 0.15);
}

.recent-task-item:hover::before {
  transform: scaleX(1);
}

.task-status-icon {
  width: 48px;
  height: 48px;
  border-radius: 12px;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 20px;
  flex-shrink: 0;
  position: relative;
}

.task-status-icon::before {
  content: '';
  position: absolute;
  inset: -2px;
  border-radius: 14px;
  padding: 2px;
  background: linear-gradient(45deg, transparent 30%, rgba(255, 255, 255, 0.3) 50%, transparent 70%);
  mask: linear-gradient(#fff 0 0) content-box, linear-gradient(#fff 0 0);
  mask-composite: exclude;
  opacity: 0;
  transition: opacity 0.3s ease;
}

.task-status-icon:hover::before {
  opacity: 1;
}

.task-status-icon.completed {
  background: linear-gradient(135deg, rgba(40, 167, 69, 0.1), rgba(40, 167, 69, 0.2));
  color: var(--accent-color);
  border: 1px solid rgba(40, 167, 69, 0.3);
}

.task-status-icon.running {
  background: linear-gradient(135deg, rgba(255, 193, 7, 0.1), rgba(255, 193, 7, 0.2));
  color: var(--warning-color);
  border: 1px solid rgba(255, 193, 7, 0.3);
}

.task-status-icon.error {
  background: linear-gradient(135deg, rgba(220, 53, 69, 0.1), rgba(220, 53, 69, 0.2));
  color: var(--danger-color);
  border: 1px solid rgba(220, 53, 69, 0.3);
}

.task-status-icon.pending {
  background: linear-gradient(135deg, rgba(108, 117, 125, 0.1), rgba(108, 117, 125, 0.2));
  color: var(--text-muted);
  border: 1px solid rgba(108, 117, 125, 0.3);
}

.task-item-content {
  flex: 1;
  min-width: 0;
}

.task-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 12px;
}

.session-id-badge {
  font-family: 'SF Mono', Monaco, 'Cascadia Code', monospace;
  font-size: 11px;
  background: var(--bg-tertiary);
  color: var(--text-secondary);
  padding: 6px 12px;
  border-radius: 8px;
  border: 1px solid var(--border-color);
  font-weight: 600;
  letter-spacing: 0.5px;
}

.task-timestamp {
  font-size: 12px;
  color: var(--text-muted);
  font-weight: 500;
}

.task-description {
  font-size: 15px;
  color: var(--text-primary);
  margin-bottom: 16px;
  line-height: 1.5;
  overflow: hidden;
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  font-weight: 400;
}

.task-meta {
  display: flex;
  align-items: center;
  gap: 12px;
  font-size: 13px;
  color: var(--text-muted);
}

.task-status-text {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 4px 10px;
  border-radius: 8px;
  font-weight: 600;
  font-size: 12px;
}

.task-status-text.completed {
  background: linear-gradient(135deg, rgba(40, 167, 69, 0.1), rgba(40, 167, 69, 0.15));
  color: var(--accent-color);
  border: 1px solid rgba(40, 167, 69, 0.2);
}

.task-status-text.running {
  background: linear-gradient(135deg, rgba(255, 193, 7, 0.1), rgba(255, 193, 7, 0.15));
  color: var(--warning-color);
  border: 1px solid rgba(255, 193, 7, 0.2);
}

.task-status-text.error {
  background: linear-gradient(135deg, rgba(220, 53, 69, 0.1), rgba(220, 53, 69, 0.15));
  color: var(--danger-color);
  border: 1px solid rgba(220, 53, 69, 0.2);
}

.task-status-text.pending {
  background: linear-gradient(135deg, rgba(108, 117, 125, 0.1), rgba(108, 117, 125, 0.15));
  color: var(--text-muted);
  border: 1px solid rgba(108, 117, 125, 0.2);
}

.view-more-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 12px;
  width: 100%;
  padding: 20px;
  border: 2px dashed var(--border-color);
  border-radius: 16px;
  background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
  color: var(--text-secondary);
  font-size: 15px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
}

.view-more-btn::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(0, 122, 204, 0.1), transparent);
  transition: left 0.5s;
}

.view-more-btn:hover {
  border-color: var(--primary-color);
  background: linear-gradient(135deg, var(--bg-hover), var(--bg-secondary));
  color: var(--primary-color);
  transform: translateY(-2px);
  box-shadow: 0 8px 24px rgba(0, 122, 204, 0.15);
}

.view-more-btn:hover::before {
  left: 100%;
}

/* Search and Filter Bar */
.search-filter-bar {
  display: flex;
  gap: 20px;
  margin-bottom: 28px;
}

.search-input {
  flex: 1;
  padding: 14px 20px;
  border: 2px solid var(--border-color);
  border-radius: 12px;
  background-color: var(--bg-primary);
  font-size: 15px;
  color: var(--text-primary);
  font-weight: 500;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.search-input::placeholder {
  color: var(--text-muted);
  font-weight: 400;
}

.search-input:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 4px rgba(0, 122, 204, 0.1);
  transform: translateY(-1px);
}

.filter-select {
  padding: 14px 18px;
  border: 2px solid var(--border-color);
  border-radius: 12px;
  background-color: var(--bg-primary);
  font-size: 15px;
  color: var(--text-primary);
  font-weight: 500;
  cursor: pointer;
  min-width: 140px;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.filter-select:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 4px rgba(0, 122, 204, 0.1);
  transform: translateY(-1px);
}

/* All Sessions List */
.sessions-list {
  flex: 1;
  overflow-y: auto;
  margin-bottom: 28px;
  padding-right: 8px;
}

.sessions-list::-webkit-scrollbar {
  width: 6px;
}

.sessions-list::-webkit-scrollbar-track {
  background: var(--bg-tertiary);
  border-radius: 3px;
}

.sessions-list::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 3px;
}

.sessions-list::-webkit-scrollbar-thumb:hover {
  background: var(--text-secondary);
}

.session-item {
  display: flex;
  align-items: center;
  gap: 20px;
  padding: 20px;
  border: 1px solid var(--border-color);
  border-radius: 12px;
  background-color: var(--bg-primary);
  margin-bottom: 16px;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
}

.session-item::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 3px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  transform: scaleX(0);
  transition: transform 0.3s ease;
}

.session-item:hover {
  border-color: var(--primary-color);
  background-color: var(--bg-hover);
  transform: translateY(-2px);
  box-shadow: 0 8px 24px rgba(0, 122, 204, 0.15);
}

.session-item:hover::before {
  transform: scaleX(1);
}

.session-info {
  flex: 1;
  min-width: 0;
}

.session-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 8px;
}

.session-id {
  font-family: 'SF Mono', Monaco, 'Cascadia Code', monospace;
  font-size: 14px;
  font-weight: 600;
  color: var(--text-primary);
  letter-spacing: 0.3px;
}

.session-time {
  font-size: 12px;
  color: var(--text-muted);
  font-weight: 500;
}

.session-details {
  display: flex;
  align-items: center;
  gap: 16px;
  font-size: 13px;
  color: var(--text-secondary);
}

.task-count {
  display: flex;
  align-items: center;
  gap: 6px;
  font-weight: 600;
}

.session-status {
  display: flex;
  align-items: center;
  gap: 8px;
  font-weight: 500;
}

/* Pagination Controls */
.pagination-controls {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 16px;
  padding: 24px 0;
  border-top: 2px solid var(--border-color);
  margin-top: auto;
}

.page-btn {
  padding: 12px 20px;
  border: 2px solid var(--border-color);
  border-radius: 10px;
  background-color: var(--bg-primary);
  color: var(--text-primary);
  font-size: 14px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.page-btn:hover:not(:disabled) {
  border-color: var(--primary-color);
  background-color: var(--primary-color);
  color: white;
  transform: translateY(-1px);
  box-shadow: 0 4px 16px rgba(0, 122, 204, 0.2);
}

.page-btn:disabled {
  background-color: var(--bg-tertiary);
  color: var(--text-muted);
  cursor: not-allowed;
  opacity: 0.4;
}

.page-btn:disabled:hover {
  transform: none;
  box-shadow: none;
}

.page-info {
  font-size: 14px;
  color: var(--text-secondary);
  font-weight: 600;
}

/* Empty State - Enhanced */
.empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 60px 40px;
  text-align: center;
  color: var(--text-muted);
  min-height: 300px;
  background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
  border-radius: 16px;
  border: 2px dashed var(--border-color);
  margin: 20px 0;
}

.empty-state-icon {
  font-size: 72px;
  margin-bottom: 24px;
  opacity: 0.6;
  filter: grayscale(0.3);
}

.empty-state-title {
  font-size: 20px;
  font-weight: 600;
  color: var(--text-secondary);
  margin-bottom: 16px;
  letter-spacing: -0.3px;
}

.empty-state-description {
  font-size: 15px;
  line-height: 1.6;
  opacity: 0.8;
  max-width: 320px;
}

/* Loading States */
.loading-skeleton {
  background: linear-gradient(90deg, 
    var(--bg-tertiary) 25%, 
    rgba(0, 122, 204, 0.1) 50%, 
    var(--bg-tertiary) 75%);
  background-size: 200% 100%;
  animation: shimmer 2s infinite ease-in-out;
  border-radius: 12px;
}

@keyframes shimmer {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}

.skeleton-task-item {
  height: 96px;
  border-radius: 16px;
  margin-bottom: 20px;
  position: relative;
  overflow: hidden;
}

.skeleton-task-item::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 4px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  opacity: 0.3;
}

.skeleton-session-item {
  height: 76px;
  border-radius: 12px;
  margin-bottom: 16px;
  position: relative;
  overflow: hidden;
}

.skeleton-session-item::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 3px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  opacity: 0.3;
}

/* Enhanced Status Dots */
.history-modal .status-dot {
  width: 10px;
  height: 10px;
  border-radius: 50%;
  display: inline-block;
  position: relative;
  margin-right: 4px;
}

.history-modal .status-dot::before {
  content: '';
  position: absolute;
  top: -3px;
  left: -3px;
  right: -3px;
  bottom: -3px;
  border-radius: 50%;
  opacity: 0.3;
}

.history-modal .status-dot.completed, 
.history-modal .status-dot.done {
  background: var(--accent-color);
}

.history-modal .status-dot.completed::before, 
.history-modal .status-dot.done::before {
  background: var(--accent-color);
}

.history-modal .status-dot.running {
  background: var(--warning-color);
  animation: pulse-dot 2s infinite;
}

.history-modal .status-dot.running::before {
  background: var(--warning-color);
  animation: pulse-ring 2s infinite;
}

.history-modal .status-dot.failed, 
.history-modal .status-dot.error {
  background: var(--danger-color);
}

.history-modal .status-dot.failed::before, 
.history-modal .status-dot.error::before {
  background: var(--danger-color);
}

.history-modal .status-dot.paused {
  background: var(--text-muted);
}

.history-modal .status-dot.paused::before {
  background: var(--text-muted);
}

.history-modal .status-dot.active {
  background: var(--accent-color);
  animation: pulse-dot 2s infinite;
}

.history-modal .status-dot.active::before {
  background: var(--accent-color);
  animation: pulse-ring 2s infinite;
}

@keyframes pulse-dot {
  0%, 100% {
    opacity: 1;
    transform: scale(1);
  }
  50% {
    opacity: 0.7;
    transform: scale(0.9);
  }
}

@keyframes pulse-ring {
  0% {
    transform: scale(0.8);
    opacity: 0.8;
  }
  50% {
    transform: scale(1.2);
    opacity: 0.3;
  }
  100% {
    transform: scale(1.6);
    opacity: 0;
  }
}

/* Enhanced Scrollbar Styles for History Modal */
.recent-tasks-list::-webkit-scrollbar,
.sessions-list::-webkit-scrollbar {
  width: 8px;
}

.recent-tasks-list::-webkit-scrollbar-track,
.sessions-list::-webkit-scrollbar-track {
  background: var(--bg-tertiary);
  border-radius: 4px;
  margin: 8px 0;
}

.recent-tasks-list::-webkit-scrollbar-thumb,
.sessions-list::-webkit-scrollbar-thumb {
  background: linear-gradient(135deg, var(--border-color), var(--text-muted));
  border-radius: 4px;
  border: 1px solid var(--bg-tertiary);
}

.recent-tasks-list::-webkit-scrollbar-thumb:hover,
.sessions-list::-webkit-scrollbar-thumb:hover {
  background: linear-gradient(135deg, var(--text-secondary), var(--primary-color));
}

/* Focus Styles for Keyboard Navigation */
.recent-task-item:focus,
.session-item:focus,
.view-more-btn:focus,
.back-btn:focus {
  outline: 3px solid var(--primary-color) !important;
  outline-offset: 2px !important;
}

/* High Contrast Mode Support */
@media (prefers-contrast: high) {
  .recent-task-item,
  .session-item {
    border-width: 2px !important;
  }
  
  .task-status-icon,
  .task-status-text {
    border-width: 2px !important;
  }
  
  .history-modal .status-dot {
    border: 2px solid currentColor !important;
  }
}

/* Dark Mode Support (Future Enhancement) */
@media (prefers-color-scheme: dark) {
  .loading-skeleton {
    background: linear-gradient(90deg, 
      rgba(255, 255, 255, 0.1) 25%, 
      rgba(0, 122, 204, 0.2) 50%, 
      rgba(255, 255, 255, 0.1) 75%) !important;
  }
  
  .recent-tasks-list::-webkit-scrollbar-thumb,
  .sessions-list::-webkit-scrollbar-thumb {
    background: linear-gradient(135deg, rgba(255, 255, 255, 0.2), rgba(0, 122, 204, 0.4)) !important;
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/input.css
================================================
/* Input and Control Styles - VibeSurf Extension */

/* Control Panel */
.control-panel {
  padding: var(--spacing-md);
  background-color: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
  display: flex;
  gap: var(--spacing-sm);
  align-items: center;
}

.control-btn {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  padding: var(--spacing-sm) var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  background-color: var(--bg-primary);
  color: var(--text-primary);
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.control-btn:hover {
  border-color: var(--border-hover);
  background-color: var(--bg-hover);
}

.control-btn:active {
  transform: scale(0.98);
}

.cancel-btn {
  color: var(--danger-color);
  border-color: var(--danger-color);
}

.cancel-btn:hover {
  background-color: rgba(220, 53, 69, 0.1);
}

.resume-btn {
  color: var(--accent-color);
  border-color: var(--accent-color);
}

.resume-btn:hover {
  background-color: rgba(40, 167, 69, 0.1);
}

.terminate-btn {
  color: var(--danger-color);
  border-color: var(--danger-color);
}

.terminate-btn:hover {
  background-color: rgba(220, 53, 69, 0.1);
}

/* Control Panel Error State */
.control-panel.error-state {
  background-color: rgba(220, 53, 69, 0.05);
  border-left: 4px solid var(--danger-color);
  border-color: rgba(220, 53, 69, 0.2);
  box-shadow: 0 2px 8px rgba(220, 53, 69, 0.1);
}

.control-panel.error-state::before {
  content: 'âš ï¸ Error occurred - task controls remain available';
  display: block;
  font-size: var(--font-size-xs);
  color: var(--danger-color);
  margin-bottom: var(--spacing-sm);
  padding: var(--spacing-xs) var(--spacing-sm);
  background-color: rgba(220, 53, 69, 0.1);
  border-radius: var(--radius-sm);
  border: 1px solid rgba(220, 53, 69, 0.2);
  font-weight: var(--font-weight-medium);
}

.control-panel.error-state .control-btn {
  border-color: rgba(220, 53, 69, 0.3);
  background-color: rgba(255, 255, 255, 0.9);
}

.control-panel.error-state .control-btn:hover {
  background-color: var(--bg-primary);
  box-shadow: 0 2px 4px rgba(220, 53, 69, 0.2);
}

/* Input Section */
.input-section {
  background-color: var(--bg-primary);
  border-top: 1px solid var(--border-color);
  padding: var(--spacing-md);
}

.input-container {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-sm);
}

.input-main {
  display: flex;
  flex-direction: column;
}

.textarea-container {
  position: relative;
  display: flex;
  flex-direction: column;
}

.input-footer {
  display: flex;
  align-items: center;
  justify-content: flex-start;
  margin-top: calc(var(--spacing-xs) / 2);
}

.llm-select,
.agent-mode-select {
  padding: var(--spacing-sm) var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  background-color: var(--bg-primary);
  color: var(--text-primary);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: border-color var(--transition-fast);
  min-width: 120px;
}

.llm-select.compact {
  max-width: 140px;
  font-size: var(--font-size-xs);
  padding: var(--spacing-xs) var(--spacing-sm);
}

.agent-mode-select.compact {
  max-width: 70px;
  font-size: var(--font-size-xs);
  padding: var(--spacing-xs) var(--spacing-sm);
}

.llm-select:focus,
.agent-mode-select:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.task-input {
  width: 100%;
  padding: var(--spacing-md);
  padding-right: 80px; /* Make space for buttons */
  border: 1px solid var(--border-color);
  border-radius: var(--radius-lg);
  background-color: var(--bg-primary);
  color: var(--text-primary);
  font-size: var(--font-size-sm);
  font-family: inherit;
  line-height: 1.5;
  resize: vertical;
  transition: all var(--transition-fast);
  min-height: 44px;
  max-height: 200px;
  overflow-y: auto;
}

.task-input:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.task-input::placeholder {
  color: var(--text-muted);
}

.input-actions {
  position: absolute;
  right: var(--spacing-sm);
  bottom: var(--spacing-sm);
  display: flex;
  gap: var(--spacing-xs);
  z-index: 1;
}

.send-btn {
  background-color: var(--primary-color);
  border-color: var(--primary-color);
  color: white;
}

.send-btn:hover {
  background-color: var(--primary-hover);
  border-color: var(--primary-hover);
}

.send-btn:disabled {
  background-color: var(--border-color);
  border-color: var(--border-color);
  color: var(--text-muted);
  cursor: not-allowed;
  transform: none;
  opacity: 0.5;
}

/* Task Running Disabled State */
.task-running-disabled {
  opacity: 0.5 !important;
  cursor: not-allowed !important;
  pointer-events: none !important;
  position: relative;
}

.task-running-disabled::after {
  content: 'ğŸ”’';
  position: absolute;
  top: 50%;
  right: 8px;
  transform: translateY(-50%);
  font-size: 12px;
  opacity: 0.7;
  pointer-events: none;
}

/* Specific styles for different elements when disabled */
button.task-running-disabled {
  background-color: #f0f0f0 !important;
  border-color: #ddd !important;
  color: #999 !important;
}

input.task-running-disabled,
textarea.task-running-disabled,
select.task-running-disabled {
  background-color: #f9f9f9 !important;
  border-color: #ddd !important;
  color: #999 !important;
}

/* Task status indicator in control panel */
.control-panel .task-status-indicator {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 8px 12px;
  background: rgba(255, 193, 7, 0.1);
  border: 1px solid rgba(255, 193, 7, 0.3);
  border-radius: 6px;
  font-size: 12px;
  color: #856404;
  margin-bottom: 8px;
}

.control-panel .task-status-indicator::before {
  content: 'âš ï¸';
  font-size: 14px;
}

/* Warning modal styles removed - consolidated in components.css to avoid conflicts */

/* File Upload Styles */
.uploaded-files-container {
  margin-top: var(--spacing-sm);
  margin-bottom: var(--spacing-sm);
  padding: var(--spacing-sm);
  background-color: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  max-height: 120px;
  overflow-y: auto;
  display: none; /* Hidden by default */
}

.uploaded-files-container .files-items {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-xs);
}

.uploaded-files-container .file-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-xs) var(--spacing-sm);
  background-color: var(--bg-primary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-sm);
  font-size: var(--font-size-sm);
}

.uploaded-files-container .file-name {
  flex: 1;
  color: var(--text-primary);
  font-weight: var(--font-weight-medium);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  margin-right: var(--spacing-sm);
}

.uploaded-files-container .file-remove-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 20px;
  height: 20px;
  border: none;
  border-radius: var(--radius-sm);
  background-color: var(--danger-color);
  color: white;
  font-size: 14px;
  font-weight: bold;
  cursor: pointer;
  transition: all var(--transition-fast);
  flex-shrink: 0;
}

.uploaded-files-container .file-remove-btn:hover {
  background-color: #c82333;
  transform: scale(1.1);
}

.uploaded-files-container .file-remove-btn:active {
  transform: scale(0.95);
}

.uploaded-files-container .error-message {
  padding: var(--spacing-sm);
  background-color: rgba(220, 53, 69, 0.1);
  border: 1px solid var(--danger-color);
  border-radius: var(--radius-sm);
  color: var(--danger-color);
  font-size: var(--font-size-sm);
  text-align: center;
}

/* Tab Selection Dropdown */
.tab-selector-dropdown {
  position: fixed !important;
  z-index: 1000;
  background: var(--bg-primary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  max-height: 300px;
  overflow: hidden;
  display: none;
}

.tab-selector-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) var(--spacing-md);
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
}

.tab-selector-title {
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
}

.tab-selector-close {
  width: 24px;
  height: 24px;
  border: none;
  background: none;
  font-size: 18px;
  color: var(--text-muted);
  cursor: pointer;
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all var(--transition-fast);
}

.tab-selector-close:hover {
  background: var(--bg-hover);
  color: var(--text-primary);
}

.tab-selector-content {
  max-height: 200px;
  overflow-y: auto;
  padding: var(--spacing-xs);
}

.tab-selector-controls {
  padding: var(--spacing-xs) var(--spacing-sm);
  border-bottom: 1px solid var(--border-color);
  margin-bottom: var(--spacing-xs);
}

.select-all-option {
  font-weight: var(--font-weight-medium);
  color: var(--primary-color);
}

.tab-options-list {
  display: flex;
  flex-direction: column;
  gap: 2px;
}

.tab-option {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  padding: var(--spacing-xs) var(--spacing-sm);
  cursor: pointer;
  border-radius: var(--radius-sm);
  transition: background-color var(--transition-fast);
}

.tab-option:hover {
  background: var(--bg-hover);
}

.tab-option.active-tab {
  background: rgba(144, 238, 144, 0.3);
  border: 1px solid rgba(144, 238, 144, 0.6);
  box-shadow: 0 2px 4px rgba(144, 238, 144, 0.2);
}

.tab-option.active-tab .tab-name {
  color: #2d5a2d;
  font-weight: var(--font-weight-medium);
}

.tab-option.active-tab .tab-id {
  color: #1a4d1a;
  font-weight: var(--font-weight-medium);
}

.tab-radio {
  width: 16px;
  height: 16px;
  cursor: pointer;
}

.tab-name {
  flex: 1;
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/* Remove footer and buttons for single-select auto-confirm */

/* Tab selection indicator in textarea */
.task-input.has-selected-tabs {
  border-color: var(--primary-color);
  background: rgba(0, 122, 204, 0.02);
}

/* Selected tabs display */
.selected-tabs-indicator {
  position: absolute;
  bottom: 8px;
  left: 8px;
  background: var(--primary-color);
  color: white;
  padding: 2px 6px;
  border-radius: var(--radius-sm);
  font-size: 10px;
  font-weight: var(--font-weight-medium);
  pointer-events: none;
  z-index: 2;
}

/* Voice Recording Button Styles */
.voice-record-btn {
  background-color: var(--bg-primary);
  border-color: var(--border-color);
  color: var(--text-primary);
  transition: all var(--transition-fast);
}

.voice-record-btn:hover {
  border-color: var(--primary-color);
  background-color: var(--bg-hover);
  color: var(--primary-color);
}

.voice-record-btn.recording {
  background-color: var(--danger-color);
  border-color: var(--danger-color);
  color: white;
  animation: pulse-recording 1.5s infinite;
}

.voice-record-btn.recording:hover {
  background-color: #c82333;
  border-color: #c82333;
}

/* Recording pulse animation */
@keyframes pulse-recording {
  0% {
    box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(220, 53, 69, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(220, 53, 69, 0);
  }
}

/* Disabled state for voice recording */
.voice-record-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  background-color: var(--border-color);
  border-color: var(--border-color);
  color: var(--text-muted);
  animation: none;
}

/* Task running disabled state for voice recording */
.voice-record-btn.task-running-disabled {
  opacity: 0.6;
  cursor: not-allowed;
  background-color: var(--bg-tertiary);
  border-color: var(--border-color);
  color: var(--text-muted);
  animation: none;
  position: relative;
}

.voice-record-btn.task-running-disabled:hover {
  background-color: var(--bg-tertiary);
  border-color: var(--border-color);
  color: var(--text-muted);
  transform: none;
}

/* Add lock icon for task running disabled state */
.voice-record-btn.task-running-disabled::before {
  content: 'ğŸ”’';
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  font-size: 16px;
  opacity: 0.8;
  z-index: 1;
}

.voice-record-btn.task-running-disabled .recording-status {
  display: none;
}

/* Recording status indicator */
.recording-status {
  position: absolute;
  top: -8px;
  right: -8px;
  width: 8px;
  height: 8px;
  background-color: var(--danger-color);
  border-radius: 50%;
  animation: pulse-dot 1s infinite;
}

@keyframes pulse-dot {
  0% {
    transform: scale(0.8);
    opacity: 1;
  }
  50% {
    transform: scale(1.2);
    opacity: 0.7;
  }
  100% {
    transform: scale(0.8);
    opacity: 1;
  }
}

/* Voice recording tooltip */
.voice-record-btn::after {
  content: attr(data-tooltip);
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  background-color: var(--bg-secondary);
  color: var(--text-primary);
  padding: 4px 8px;
  border-radius: var(--radius-sm);
  font-size: 12px;
  white-space: nowrap;
  opacity: 0;
  visibility: hidden;
  transition: all var(--transition-fast);
  pointer-events: none;
  border: 1px solid var(--border-color);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  z-index: 1000;
}

.voice-record-btn:hover::after {
  opacity: 1;
  visibility: visible;
  transform: translateX(-50%) translateY(-4px);
}

/* Recording duration display */
.recording-duration {
  position: absolute;
  top: -20px;
  left: 50%;
  transform: translateX(-50%);
  background-color: var(--danger-color);
  color: white;
  padding: 2px 6px;
  border-radius: var(--radius-sm);
  font-size: 10px;
  font-weight: var(--font-weight-medium);
  white-space: nowrap;
  opacity: 0;
  visibility: hidden;
  transition: all var(--transition-fast);
  pointer-events: none;
}

.voice-record-btn.recording .recording-duration {
  opacity: 1;
  visibility: visible;
}

/* Skill Selection Dropdown */
.skill-selector-dropdown {
  position: fixed !important;
  z-index: 1000;
  background: var(--bg-primary);
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  max-height: 300px;
  overflow: hidden;
  display: none;
}

.skill-selector-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) var(--spacing-md);
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
}

.skill-selector-title {
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  color: var(--text-primary);
}

.skill-selector-close {
  width: 24px;
  height: 24px;
  border: none;
  background: none;
  font-size: 18px;
  color: var(--text-muted);
  cursor: pointer;
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all var(--transition-fast);
}

.skill-selector-close:hover {
  background: var(--bg-hover);
  color: var(--text-primary);
}

.skill-selector-content {
  max-height: 200px;
  overflow-y: auto;
  padding: var(--spacing-xs);
}

.skill-options-list {
  display: flex;
  flex-direction: column;
  gap: 2px;
}

.skill-option {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  padding: var(--spacing-xs) var(--spacing-sm);
  cursor: pointer;
  border-radius: var(--radius-sm);
  transition: background-color var(--transition-fast);
}

.skill-option:hover {
  background: var(--bg-hover);
}

.skill-option.selected {
  background: rgba(0, 122, 204, 0.1);
  border: 1px solid rgba(0, 122, 204, 0.3);
}

.skill-name {
  flex: 1;
  font-size: var(--font-size-sm);
  color: var(--text-primary);
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

.skill-description {
  font-size: var(--font-size-xs);
  color: var(--text-muted);
  margin-top: 2px;
}

/* Skill selection indicator in textarea */
.task-input.has-selected-skills {
  border-color: var(--accent-color);
  background: rgba(40, 167, 69, 0.02);
}

/* Selected skills display */
.selected-skills-indicator {
  position: absolute;
  bottom: 8px;
  left: 8px;
  background: var(--accent-color);
  color: white;
  padding: 2px 6px;
  border-radius: var(--radius-sm);
  font-size: 10px;
  font-weight: var(--font-weight-medium);
  pointer-events: none;
  z-index: 2;
}


================================================
FILE: vibe_surf/chrome_extension/styles/layout.css
================================================
/* Layout Styles - VibeSurf Extension */

/* Container */
.vibesurf-container {
  display: flex;
  flex-direction: column;
  height: 100vh;
  width: 100%;
  min-width: 320px;
  background-color: var(--bg-primary);
  resize: horizontal;
  overflow: hidden;
}

/* Header */
.header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-lg);
  background-color: var(--bg-primary);
  border-bottom: 1px solid var(--border-color);
  min-height: 56px;
}

.header-left {
  display: flex;
  align-items: center;
}

.logo {
  display: flex;
  align-items: center;
  color: var(--primary-color);
  font-weight: var(--font-weight-semibold);
}

.logo-brand {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  color: var(--primary-color);
  font-weight: var(--font-weight-semibold);
}

/* Social Links */
.social-links {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  margin-left: var(--spacing-sm);
}

.social-link {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 20px;
  height: 20px;
  border-radius: var(--radius-sm);
  color: var(--text-muted);
  text-decoration: none;
  transition: all var(--transition-fast);
  opacity: 0.7;
}

.social-link:hover {
  color: var(--text-secondary);
  background-color: var(--bg-hover);
  opacity: 1;
  transform: scale(1.1);
}

.social-link:active {
  transform: scale(0.95);
}

.social-link[data-platform="github"]:hover {
  color: #333;
}

.social-link[data-platform="discord"]:hover {
  color: #5865F2;
}

.social-link[data-platform="x"]:hover {
  color: #000;
}

/* Dark mode adjustments for social links */
@media (prefers-color-scheme: dark) {
  .social-link[data-platform="github"]:hover {
    color: #f0f6fc;
  }
  
  .social-link[data-platform="x"]:hover {
    color: #fff;
  }
}

.logo-image {
  width: 24px;
  height: 24px;
  border-radius: var(--radius-sm);
  object-fit: contain;
  flex-shrink: 0;
}

.logo-content {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-xs);
}

.logo-text {
  font-size: var(--font-size-lg);
  line-height: 1.2;
}

.session-info {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  font-size: var(--font-size-xs);
  color: var(--text-secondary);
}

.session-label {
  font-weight: var(--font-weight-medium);
}

#session-id {
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  background-color: var(--bg-tertiary);
  padding: 2px 4px;
  border-radius: var(--radius-sm);
  max-width: 120px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

.copy-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 20px;
  height: 20px;
  border: none;
  border-radius: var(--radius-sm);
  background-color: transparent;
  color: var(--text-muted);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.copy-btn:hover {
  background-color: var(--bg-hover);
  color: var(--text-secondary);
}

.copy-btn:active {
  background-color: var(--bg-active);
  transform: scale(0.95);
}

.header-right {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.icon-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border: none;
  border-radius: var(--radius-md);
  background-color: transparent;
  color: var(--text-secondary);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.icon-btn:hover {
  background-color: var(--bg-hover);
  color: var(--text-primary);
}

.icon-btn:active {
  background-color: var(--bg-active);
  transform: scale(0.95);
}

/* Main Content */
.main-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  background-color: var(--bg-primary);
}

/* Common Button Styles */
.action-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border: 1px solid var(--border-color);
  border-radius: var(--radius-md);
  background-color: var(--bg-secondary);
  color: var(--text-secondary);
  cursor: pointer;
  transition: all var(--transition-fast);
  opacity: 0.8;
}

.action-btn:hover {
  border-color: var(--border-hover);
  background-color: var(--bg-hover);
  color: var(--text-primary);
  opacity: 1;
  transform: scale(1.05);
}

.action-btn:active {
  transform: scale(0.95);
}

.action-btn:disabled {
  background-color: var(--border-color);
  border-color: var(--border-color);
  color: var(--text-muted);
  cursor: not-allowed;
  transform: none;
  opacity: 0.5;
}


================================================
FILE: vibe_surf/chrome_extension/styles/responsive.css
================================================
/* Responsive Styles - VibeSurf Extension */

/* Small Screen Adjustments */
@media (max-width: 320px) {
  .header {
    padding: var(--spacing-md);
  }
  
  .activity-log {
    padding: var(--spacing-md);
  }
  
  .input-section {
    padding: var(--spacing-md);
  }
}

/* Tablet and Medium Screen Adjustments */
@media (max-width: 768px) {
  .history-modal-content {
    width: 96vw !important;
    max-height: 92vh !important;
    border-radius: 12px !important;
    margin: 16px !important;
  }
  
  .history-content {
    padding: 24px !important;
    padding-top: 20px !important;
  }
  
  .section-header {
    padding-bottom: 20px !important;
    margin-bottom: 24px !important;
  }
  
  .section-header h4 {
    font-size: 20px !important;
  }
  
  .search-filter-bar {
    flex-direction: column !important;
    gap: 16px !important;
  }
  
  .search-input, .filter-select {
    padding: 12px 16px !important;
    font-size: 14px !important;
  }
  
  .recent-task-item {
    padding: 20px !important;
    gap: 16px !important;
    margin-bottom: 16px !important;
  }
  
  .task-status-icon {
    width: 40px !important;
    height: 40px !important;
    font-size: 18px !important;
  }
  
  .task-header {
    flex-direction: column !important;
    align-items: flex-start !important;
    gap: 8px !important;
  }
  
  .session-item {
    padding: 16px !important;
    gap: 16px !important;
    margin-bottom: 12px !important;
  }
  
  .session-header {
    flex-direction: column !important;
    align-items: flex-start !important;
    gap: 6px !important;
  }
  
  .pagination-controls {
    flex-direction: column !important;
    gap: 16px !important;
    text-align: center !important;
    padding: 20px 0 !important;
  }
  
  .page-btn {
    padding: 10px 16px !important;
    font-size: 13px !important;
  }
  
  .view-more-btn {
    padding: 16px !important;
    font-size: 14px !important;
  }
}

/* Mobile Screen Adjustments */
@media (max-width: 480px) {
  .history-modal-content {
    width: 98vw !important;
    max-height: 95vh !important;
    border-radius: 8px !important;
    margin: 8px !important;
  }
  
  .history-content {
    padding: 20px !important;
    padding-top: 16px !important;
  }
  
  .section-header h4 {
    font-size: 18px !important;
  }
  
  .back-btn {
    padding: 8px 16px !important;
    font-size: 13px !important;
  }
  
  .recent-task-item {
    padding: 16px !important;
    gap: 12px !important;
    border-radius: 12px !important;
  }
  
  .task-status-icon {
    width: 36px !important;
    height: 36px !important;
    font-size: 16px !important;
    border-radius: 10px !important;
  }
  
  .session-id-badge {
    font-size: 10px !important;
    padding: 4px 8px !important;
  }
  
  .task-timestamp, .session-time {
    font-size: 11px !important;
  }
  
  .task-description {
    font-size: 14px !important;
    -webkit-line-clamp: 3 !important;
    margin-bottom: 12px !important;
  }
  
  .task-meta, .session-details {
    font-size: 12px !important;
    gap: 8px !important;
  }
  
  .task-status-text {
    padding: 3px 8px !important;
    font-size: 11px !important;
  }
  
  .session-item {
    padding: 14px !important;
    gap: 12px !important;
    border-radius: 10px !important;
  }
  
  .session-id {
    font-size: 13px !important;
  }
  
  .empty-state {
    padding: 40px 20px !important;
    min-height: 240px !important;
  }
  
  .empty-state-icon {
    font-size: 56px !important;
    margin-bottom: 20px !important;
  }
  
  .empty-state-title {
    font-size: 18px !important;
    margin-bottom: 12px !important;
  }
  
  .empty-state-description {
    font-size: 14px !important;
    max-width: 280px !important;
  }
  
  .task-item-content {
    min-width: 0;
  }
  
  .task-description {
    -webkit-line-clamp: 3;
  }
}

/* Extra Small Mobile Screens */
@media (max-width: 360px) {
  .history-content {
    padding: 16px !important;
  }
  
  .recent-task-item {
    flex-direction: column !important;
    align-items: flex-start !important;
    text-align: left !important;
  }
  
  .task-status-icon {
    align-self: flex-end !important;
    margin-bottom: 8px !important;
  }
  
  .task-header {
    width: 100% !important;
  }
  
  .session-item {
    flex-direction: column !important;
    align-items: flex-start !important;
  }
  
  .session-header {
    width: 100% !important;
  }
}

/* Accessibility Enhancements */
@media (prefers-reduced-motion: reduce) {
  .recent-task-item,
  .session-item,
  .view-more-btn,
  .back-btn,
  .page-btn,
  .task-suggestion,
  .icon-btn,
  .action-btn,
  .control-btn {
    transition: none !important;
  }
  
  .recent-task-item:hover,
  .session-item:hover,
  .task-suggestion:hover {
    transform: none !important;
  }
  
  .loading-skeleton {
    animation: none !important;
    background: var(--bg-tertiary) !important;
  }
  
  .history-modal .status-dot.running,
  .history-modal .status-dot.active,
  .status-dot.active {
    animation: none !important;
  }
  
  .history-modal .status-dot.running::before,
  .history-modal .status-dot.active::before {
    animation: none !important;
    opacity: 0 !important;
  }
  
  @keyframes pulse {
    0%, 100% {
      opacity: 1;
    }
    50% {
      opacity: 1;
    }
  }
}

/* High Contrast Mode Support */
@media (prefers-contrast: high) {
  :root {
    --border-color: #000000;
    --text-muted: #333333;
    --bg-secondary: #f0f0f0;
    --bg-tertiary: #e0e0e0;
  }
  
  .recent-task-item,
  .session-item,
  .message-bubble,
  .control-btn,
  .action-btn {
    border-width: 2px !important;
  }
  
  .task-status-icon,
  .task-status-text {
    border-width: 2px !important;
  }
  
  .history-modal .status-dot,
  .status-dot {
    border: 2px solid currentColor !important;
  }
  
  .message-bubble {
    box-shadow: none !important;
  }
}

/* Print Styles */
@media print {
  .header-right,
  .control-panel,
  .input-section,
  .action-btn,
  .icon-btn {
    display: none !important;
  }
  
  .vibesurf-container {
    height: auto !important;
    overflow: visible !important;
  }
  
  .activity-log {
    overflow: visible !important;
    padding: 0 !important;
  }
  
  .message-bubble {
    border: 1px solid #000 !important;
    box-shadow: none !important;
    break-inside: avoid;
  }
  
  .activity-item {
    break-inside: avoid;
    page-break-inside: avoid;
  }
}

/* Large Screen Optimizations */
@media (min-width: 1200px) {
  .vibesurf-container {
    max-width: 1200px;
    margin: 0 auto;
  }
  
  .message-container {
    max-width: 85%;
  }
  
  .history-modal-content {
    max-width: 1000px;
  }
  
  .quick-tasks {
    grid-template-columns: repeat(2, 1fr);
    max-width: 600px;
  }
}

/* Ultra-wide Screen Adjustments */
@media (min-width: 1600px) {
  .message-container {
    max-width: 70%;
  }
  
  .quick-tasks {
    grid-template-columns: repeat(3, 1fr);
    max-width: 800px;
  }
}

/* Touch Device Optimizations */
@media (hover: none) and (pointer: coarse) {
  .icon-btn,
  .action-btn,
  .control-btn {
    min-height: 44px;
    min-width: 44px;
  }
  
  .task-suggestion {
    padding: var(--spacing-xl);
  }
  
  .copy-btn {
    min-height: 32px;
    min-width: 32px;
  }
  
  .recent-task-item,
  .session-item {
    padding: var(--spacing-xl);
  }
}

/* Landscape Mobile Adjustments */
@media (max-height: 500px) and (orientation: landscape) {
  .header {
    padding: var(--spacing-sm) var(--spacing-lg);
    min-height: 48px;
  }
  
  .input-section {
    padding: var(--spacing-sm) var(--spacing-md);
  }
  
  .welcome-message {
    padding: var(--spacing-md) var(--spacing-lg);
  }
  
  .quick-tasks {
    grid-template-columns: repeat(2, 1fr);
    gap: var(--spacing-sm);
  }
  
  .task-suggestion {
    padding: var(--spacing-md);
  }
}

/* Reduced Data Mode */
@media (prefers-reduced-data: reduce) {
  .loading-skeleton {
    background: var(--bg-tertiary) !important;
    animation: none !important;
  }
  
  .task-status-icon::before,
  .recent-task-item::before,
  .session-item::before {
    display: none !important;
  }
  
  .view-more-btn::before {
    display: none !important;
  }
}

/* Focus Visible Support */
@media (prefers-reduced-motion: no-preference) {
  .recent-task-item:focus-visible,
  .session-item:focus-visible,
  .view-more-btn:focus-visible,
  .back-btn:focus-visible,
  .task-suggestion:focus-visible,
  .icon-btn:focus-visible,
  .action-btn:focus-visible,
  .control-btn:focus-visible {
    outline: 3px solid var(--primary-color);
    outline-offset: 2px;
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-environment.css
================================================
/* Environment Variables Management */

/* Environment Variables Tab */
.env-variables-container {
  display: flex;
  flex-direction: column;
  gap: 16px;
}

.env-var-item {
  display: flex;
  gap: 12px;
  align-items: flex-start;
  padding: 16px;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: 8px;
}

.env-var-key,
.env-var-value {
  flex: 1;
}

.readonly-input {
  background-color: var(--bg-tertiary) !important;
  color: var(--text-muted) !important;
  cursor: not-allowed !important;
  opacity: 0.7;
}

.readonly-input:focus {
  outline: none !important;
  box-shadow: none !important;
  border-color: var(--border-color) !important;
}

.env-var-actions {
  display: flex;
  gap: 4px;
}

.env-var-btn {
  width: 32px;
  height: 32px;
  border: none;
  background: transparent;
  color: var(--text-muted);
  cursor: pointer;
  border-radius: 6px;
  transition: all 0.2s ease;
  display: flex;
  align-items: center;
  justify-content: center;
}

.env-var-btn:hover {
  background: var(--bg-hover);
  color: var(--text-primary);
}

.env-var-btn.delete:hover {
  background: rgba(220, 53, 69, 0.1);
  color: var(--danger-color);
}

.add-env-var-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 8px;
  width: 100%;
  padding: 16px;
  border: 1px dashed var(--border-color);
  border-radius: 8px;
  background: transparent;
  color: var(--text-secondary);
  font-size: 14px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s ease;
}

.add-env-var-btn:hover {
  border-color: var(--primary-color);
  background: rgba(0, 122, 204, 0.05);
  color: var(--primary-color);
}

/* Environment Variables Action Center */
.env-var-actions-center {
  display: flex;
  justify-content: center;
  margin-top: 32px;
  padding-top: 24px;
  border-top: 1px solid var(--border-color);
}

.save-env-vars-btn-modern {
  display: flex;
  align-items: center;
  gap: 12px;
  padding: 16px 32px;
  background: linear-gradient(135deg, var(--primary-color), var(--primary-hover));
  color: white;
  border: none;
  border-radius: 12px;
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  box-shadow: 0 4px 12px rgba(0, 122, 204, 0.2);
  position: relative;
  overflow: hidden;
}

.save-env-vars-btn-modern::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
  transition: left 0.5s ease;
}

.save-env-vars-btn-modern:hover {
  transform: translateY(-2px);
  box-shadow: 0 8px 24px rgba(0, 122, 204, 0.3);
  background: linear-gradient(135deg, var(--primary-hover), var(--primary-color));
}

.save-env-vars-btn-modern:hover::before {
  left: 100%;
}

.save-env-vars-btn-modern:active {
  transform: translateY(0);
  box-shadow: 0 4px 12px rgba(0, 122, 204, 0.2);
}

.save-env-vars-btn-modern .btn-icon {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 20px;
  height: 20px;
}

.save-env-vars-btn-modern .btn-text {
  font-weight: 600;
  letter-spacing: 0.5px;
}

.save-env-vars-btn-modern:disabled {
  opacity: 0.6;
  cursor: not-allowed;
  transform: none !important;
  box-shadow: 0 4px 12px rgba(0, 122, 204, 0.1) !important;
}

.save-env-vars-btn-modern:disabled::before {
  display: none;
}

/* Compact Button for General Tab */
.save-env-vars-btn-compact {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 8px 16px;
  background: linear-gradient(135deg, var(--primary-color), var(--primary-hover));
  color: white;
  border: none;
  border-radius: 8px;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  box-shadow: 0 2px 8px rgba(0, 122, 204, 0.15);
  position: relative;
  overflow: hidden;
}

.save-env-vars-btn-compact::before {
  content: '';
  position: absolute;
  top: 0;
  left: -100%;
  width: 100%;
  height: 100%;
  background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
  transition: left 0.5s ease;
}

.save-env-vars-btn-compact:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 16px rgba(0, 122, 204, 0.25);
  background: linear-gradient(135deg, var(--primary-hover), var(--primary-color));
}

.save-env-vars-btn-compact:hover::before {
  left: 100%;
}

.save-env-vars-btn-compact:active {
  transform: translateY(0);
  box-shadow: 0 2px 8px rgba(0, 122, 204, 0.15);
}

.save-env-vars-btn-compact .btn-icon {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 16px;
  height: 16px;
}

.save-env-vars-btn-compact .btn-text {
  font-weight: 500;
  letter-spacing: 0.3px;
}

.save-env-vars-btn-compact:disabled {
  opacity: 0.6;
  cursor: not-allowed;
  transform: none !important;
  box-shadow: 0 2px 8px rgba(0, 122, 204, 0.1) !important;
}

.save-env-vars-btn-compact:disabled::before {
  display: none;
}

/* General Tab Sections Layout */
.general-sections {
  display: flex;
  flex-direction: column;
  gap: 20px;
}

.general-section {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: 12px;
  padding: 20px;
  transition: all 0.3s ease;
}

.general-section:hover {
  border-color: var(--border-hover);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
}

.general-section .section-header {
  margin-bottom: 16px;
  padding-bottom: 12px;
  border-bottom: 1px solid var(--border-color);
}

.general-section .section-header h3 {
  font-size: 18px;
  font-weight: 600;
  color: var(--text-primary);
  margin: 0 0 8px 0;
  display: flex;
  align-items: center;
  gap: 8px;
}

.general-section .section-description {
  font-size: 14px;
  color: var(--text-secondary);
  margin: 0;
  line-height: 1.5;
}

.general-settings-content {
  display: flex;
  flex-direction: column;
  gap: 16px;
}

/* Responsive adjustments for general sections */
@media (max-width: 768px) {
  .general-section {
    padding: 16px;
  }
  
  .general-sections {
    gap: 16px;
  }
  
  .general-section .section-header h3 {
    font-size: 16px;
  }
  
  .general-section .section-header {
    margin-bottom: 12px;
    padding-bottom: 8px;
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-forms.css
================================================
/* Form Components and Validation */

/* Profile Form Modal */
.profile-form-modal {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 10000;
  backdrop-filter: blur(8px);
}

.profile-form-container {
  background: var(--bg-primary);
  border-radius: 16px;
  box-shadow: 0 20px 40px rgba(0, 0, 0, 0.2);
  width: 90vw;
  max-width: 600px;
  max-height: 90vh;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.profile-form-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 24px 32px;
  border-bottom: 1px solid var(--border-color);
  background: linear-gradient(135deg, var(--bg-primary), var(--bg-secondary));
}

.profile-form-header h3 {
  font-size: 20px;
  font-weight: 600;
  color: var(--text-primary);
  margin: 0;
}

.profile-form-close {
  width: 32px;
  height: 32px;
  border: none;
  background: transparent;
  color: var(--text-secondary);
  cursor: pointer;
  border-radius: 8px;
  transition: all 0.2s ease;
  display: flex;
  align-items: center;
  justify-content: center;
}

.profile-form-close:hover {
  background: var(--bg-hover);
  color: var(--text-primary);
}

.profile-form-content {
  flex: 1;
  padding: 32px;
  overflow-y: auto;
}

/* Form Groups and Labels */
.form-group {
  margin-bottom: 24px;
}

.form-label {
  display: block;
  font-size: 14px;
  font-weight: 600;
  color: var(--text-primary);
  margin-bottom: 8px;
}

.form-label.required::after {
  content: ' *';
  color: var(--danger-color);
}

/* Form Inputs */
.form-input,
.form-select,
.form-textarea {
  width: 100%;
  padding: 12px 16px;
  border: 1px solid var(--border-color);
  border-radius: 8px;
  background: var(--bg-primary);
  font-size: 14px;
  color: var(--text-primary);
  transition: all 0.3s ease;
  font-family: inherit;
}

.form-input:focus,
.form-select:focus,
.form-textarea:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 3px rgba(0, 122, 204, 0.1);
}

.form-textarea {
  resize: vertical;
  min-height: 80px;
}

.form-help {
  font-size: 12px;
  color: var(--text-muted);
  margin-top: 4px;
  line-height: 1.4;
}

/* API Key Field with Toggle */
.api-key-field {
  position: relative;
}

.api-key-toggle {
  position: absolute;
  right: 12px;
  top: 50%;
  transform: translateY(-50%);
  background: none;
  border: none;
  color: var(--text-muted);
  cursor: pointer;
  padding: 4px;
  border-radius: 4px;
  transition: all 0.2s ease;
}

.api-key-toggle:hover {
  color: var(--text-primary);
  background: var(--bg-hover);
}

.api-key-input {
  padding-right: 40px;
}

/* Form Actions */
.profile-form-actions {
  display: flex;
  gap: 12px;
  justify-content: flex-end;
  padding: 24px 32px;
  border-top: 1px solid var(--border-color);
  background: var(--bg-secondary);
}

.form-btn {
  padding: 12px 24px;
  border: none;
  border-radius: 8px;
  font-size: 14px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  min-width: 100px;
}

.form-btn.primary {
  background: var(--primary-color);
  color: white;
}

.form-btn.primary:hover {
  background: var(--primary-hover);
  transform: translateY(-1px);
  box-shadow: 0 4px 16px rgba(0, 122, 204, 0.3);
}

.form-btn.secondary {
  background: var(--bg-primary);
  color: var(--text-primary);
  border: 1px solid var(--border-color);
}

.form-btn.secondary:hover {
  background: var(--bg-hover);
  border-color: var(--border-hover);
}

.form-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  transform: none !important;
  box-shadow: none !important;
}

/* Success/Error Messages */
.form-message {
  padding: 12px 16px;
  border-radius: 8px;
  margin-bottom: 16px;
  font-size: 14px;
  font-weight: 500;
}

.form-message.success {
  background: rgba(40, 167, 69, 0.1);
  color: var(--accent-color);
  border: 1px solid rgba(40, 167, 69, 0.3);
}

.form-message.error {
  background: rgba(220, 53, 69, 0.1);
  color: var(--danger-color);
  border: 1px solid rgba(220, 53, 69, 0.3);
}

/* Form Validation */
.form-input.error,
.form-select.error,
.form-textarea.error {
  border-color: var(--danger-color);
  box-shadow: 0 0 0 3px rgba(220, 53, 69, 0.1);
}

.form-error {
  font-size: 12px;
  color: var(--danger-color);
  margin-top: 4px;
  display: flex;
  align-items: center;
  gap: 4px;
}

.form-error::before {
  content: 'âš ï¸';
  font-size: 10px;
}

/* JSON Validation Styles */
.json-input {
  font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
  font-size: 13px;
  line-height: 1.5;
  tab-size: 2;
}

.json-input.json-valid {
  border-color: var(--accent-color);
  box-shadow: 0 0 0 3px rgba(40, 167, 69, 0.1);
}

.json-input.json-invalid {
  border-color: var(--danger-color);
  box-shadow: 0 0 0 3px rgba(220, 53, 69, 0.1);
}

.json-validation-feedback {
  margin-top: 8px;
  font-size: 13px;
  font-weight: 500;
  min-height: 20px;
  display: flex;
  align-items: center;
  gap: 6px;
}

.json-success {
  color: var(--accent-color);
  display: flex;
  align-items: center;
  gap: 6px;
}

.json-success::before {
  content: 'âœ“';
  font-weight: bold;
  font-size: 14px;
}

.json-error {
  color: var(--danger-color);
  display: flex;
  align-items: center;
  gap: 6px;
}

.json-error::before {
  content: 'âœ—';
  font-weight: bold;
  font-size: 14px;
}

/* JSON syntax highlighting for better UX */
.json-input:focus {
  background: var(--bg-primary);
}

.json-input:invalid {
  border-color: var(--danger-color);
}

/* Enhanced form textarea for JSON */
.form-textarea.json-input {
  resize: vertical;
  min-height: 120px;
  white-space: pre;
  word-wrap: break-word;
}

/* JSON validation feedback animations */
.json-validation-feedback {
  opacity: 0;
  transform: translateY(-4px);
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.json-validation-feedback:not(:empty) {
  opacity: 1;
  transform: translateY(0);
}

/* Uploaded Files List Styles */
.uploaded-files-container {
  margin-top: 4px;
  margin-bottom: 4px;
  display: none;
}

.files-items {
  display: flex;
  flex-wrap: wrap;
  gap: 4px;
}

.file-item {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  padding: 2px 6px;
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: 12px;
  font-size: 11px;
  color: var(--text-secondary);
  max-width: 150px;
}

.file-item:hover {
  background: var(--bg-hover);
}

.file-name {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  flex: 1;
  min-width: 0;
}

.file-remove-btn {
  background: var(--text-muted);
  color: var(--bg-primary);
  border: none;
  border-radius: 50%;
  width: 14px;
  height: 14px;
  font-size: 10px;
  font-weight: bold;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: background-color 0.2s ease;
  flex-shrink: 0;
}

.file-remove-btn:hover {
  background: var(--text-secondary);
}

.file-remove-btn:active {
  transform: scale(0.9);
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-modal.css
================================================
/* Settings Modal Structure and Tab Navigation */

/* Settings Modal Container */
.settings-modal-content {
  width: 92vw;
  max-width: 1000px;
  min-height: 75vh;
  max-height: 90vh;
  display: flex;
  flex-direction: column;
  border-radius: 16px;
  box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
  background: var(--bg-primary);
  border: 1px solid var(--border-color);
  overflow: hidden;
}

/* Settings Header */
.settings-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 24px 32px;
  border-bottom: 2px solid var(--border-color);
  background: linear-gradient(135deg, var(--bg-primary), var(--bg-secondary));
}

.settings-header h3 {
  font-size: 24px;
  font-weight: 700;
  color: var(--text-primary);
  margin: 0;
  letter-spacing: -0.5px;
}

/* Settings Title with Logo */
.settings-title {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  font-size: 24px;
  font-weight: 700;
  color: var(--text-primary);
  margin: 0;
  letter-spacing: -0.5px;
}

.settings-logo {
  width: 28px;
  height: 28px;
  border-radius: var(--radius-sm);
  object-fit: contain;
  flex-shrink: 0;
}

/* Tab Navigation */
.settings-tabs {
  display: flex;
  background: var(--bg-secondary);
  border-bottom: 1px solid var(--border-color);
  overflow-x: auto;
}

.settings-tab {
  flex: 1;
  padding: 16px 24px;
  border: none;
  background: transparent;
  color: var(--text-secondary);
  font-size: 15px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  white-space: nowrap;
  min-width: 150px;
}

.settings-tab::before {
  content: '';
  position: absolute;
  bottom: 0;
  left: 0;
  right: 0;
  height: 3px;
  background: var(--primary-color);
  transform: scaleX(0);
  transition: transform 0.3s ease;
}

.settings-tab:hover {
  color: var(--text-primary);
  background: rgba(0, 122, 204, 0.05);
}

.settings-tab.active {
  color: var(--primary-color);
  background: var(--bg-primary);
}

.settings-tab.active::before {
  transform: scaleX(1);
}

/* Tab Content Container */
.settings-content {
  flex: 1;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.settings-tab-content {
  flex: 1;
  padding: 32px;
  overflow-y: auto;
  display: none;
}

.settings-tab-content.active {
  display: flex;
  flex-direction: column;
}

.settings-tab-content::-webkit-scrollbar {
  width: 8px;
}

.settings-tab-content::-webkit-scrollbar-track {
  background: var(--bg-tertiary);
  border-radius: 4px;
}

.settings-tab-content::-webkit-scrollbar-thumb {
  background: var(--border-color);
  border-radius: 4px;
}

.settings-tab-content::-webkit-scrollbar-thumb:hover {
  background: var(--text-secondary);
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-profiles.css
================================================
/* Profile Management Components */

/* Profile Cards */
.profiles-container {
  display: flex;
  flex-direction: column;
  gap: 16px;
  margin-bottom: 24px;
}

.profile-card {
  background: var(--bg-secondary);
  border: 1px solid var(--border-color);
  border-radius: 12px;
  padding: 20px;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
}

.profile-card::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  height: 4px;
  background: linear-gradient(90deg, var(--primary-color), var(--accent-color));
  transform: scaleX(0);
  transition: transform 0.3s ease;
}

.profile-card:hover {
  border-color: var(--primary-color);
  background: var(--bg-hover);
  transform: translateY(-2px);
  box-shadow: 0 8px 24px rgba(0, 122, 204, 0.15);
}

.profile-card:hover::before {
  transform: scaleX(1);
}

.profile-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 12px;
}

.profile-name {
  font-size: 18px;
  font-weight: 600;
  color: var(--text-primary);
  margin: 0;
}

.profile-badges {
  display: flex;
  align-items: center;
  gap: 8px;
}

.profile-badge {
  padding: 4px 10px;
  border-radius: 8px;
  font-size: 12px;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.profile-badge.default {
  background: linear-gradient(135deg, rgba(40, 167, 69, 0.1), rgba(40, 167, 69, 0.2));
  color: var(--accent-color);
  border: 1px solid rgba(40, 167, 69, 0.3);
}

.profile-badge.active {
  background: linear-gradient(135deg, rgba(0, 122, 204, 0.1), rgba(0, 122, 204, 0.2));
  color: var(--primary-color);
  border: 1px solid rgba(0, 122, 204, 0.3);
}

.profile-badge.inactive {
  background: linear-gradient(135deg, rgba(108, 117, 125, 0.1), rgba(108, 117, 125, 0.2));
  color: var(--text-muted);
  border: 1px solid rgba(108, 117, 125, 0.3);
}

.profile-details {
  font-size: 14px;
  color: var(--text-secondary);
  margin-bottom: 16px;
  line-height: 1.5;
}

.profile-meta {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
  gap: 12px;
  margin-bottom: 16px;
}

.profile-meta-item {
  display: flex;
  flex-direction: column;
}

.profile-meta-label {
  font-size: 12px;
  font-weight: 600;
  color: var(--text-muted);
  text-transform: uppercase;
  letter-spacing: 0.5px;
  margin-bottom: 4px;
}

.profile-meta-value {
  font-size: 14px;
  color: var(--text-primary);
  font-weight: 500;
  word-break: break-word;
}

.profile-actions {
  display: flex;
  gap: 8px;
  justify-content: flex-end;
}

.profile-btn {
  padding: 8px 16px;
  border: 1px solid var(--border-color);
  border-radius: 8px;
  background: var(--bg-primary);
  color: var(--text-primary);
  font-size: 13px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

.profile-btn:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

.profile-btn.edit {
  border-color: var(--primary-color);
  color: var(--primary-color);
}

.profile-btn.edit:hover {
  background: var(--primary-color);
  color: white;
}

.profile-btn.delete {
  border-color: var(--danger-color);
  color: var(--danger-color);
}

.profile-btn.delete:hover {
  background: var(--danger-color);
  color: white;
}

.profile-btn.toggle {
  border-color: var(--accent-color);
  color: var(--accent-color);
}

.profile-btn.toggle:hover {
  background: var(--accent-color);
  color: white;
}

/* Add Profile Button */
.add-profile-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 12px;
  width: 100%;
  padding: 20px;
  border: 2px dashed var(--border-color);
  border-radius: 12px;
  background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
  color: var(--text-secondary);
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  margin-top: 16px;
}

.add-profile-btn:hover {
  border-color: var(--primary-color);
  background: linear-gradient(135deg, var(--bg-hover), var(--bg-secondary));
  color: var(--primary-color);
  transform: translateY(-2px);
  box-shadow: 0 8px 24px rgba(0, 122, 204, 0.15);
}

.add-profile-btn svg {
  width: 20px;
  height: 20px;
}

/* Empty State */
.profiles-empty-state {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 60px 40px;
  text-align: center;
  color: var(--text-muted);
  background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
  border-radius: 16px;
  border: 2px dashed var(--border-color);
  margin-bottom: 24px;
}

.profiles-empty-icon {
  font-size: 64px;
  margin-bottom: 20px;
  opacity: 0.6;
}

.profiles-empty-title {
  font-size: 20px;
  font-weight: 600;
  color: var(--text-secondary);
  margin-bottom: 12px;
}

.profiles-empty-description {
  font-size: 15px;
  line-height: 1.6;
  opacity: 0.8;
  max-width: 320px;
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-responsive.css
================================================
/* Responsive Design for Settings */

/* Tablet and Small Desktop */
@media (max-width: 768px) {
  .settings-modal-content {
    width: 95vw;
    max-height: 95vh;
    border-radius: 12px;
  }
  
  .settings-header {
    padding: 20px 24px;
  }
  
  .settings-header h3 {
    font-size: 20px;
  }
  
  .settings-tab {
    padding: 14px 20px;
    font-size: 14px;
    min-width: 120px;
  }
  
  .settings-tab-content {
    padding: 24px;
  }
  
  .profile-card {
    padding: 16px;
  }
  
  .profile-name {
    font-size: 16px;
  }
  
  .profile-meta {
    grid-template-columns: 1fr;
  }
  
  .profile-actions {
    flex-direction: column;
  }
  
  .profile-btn {
    width: 100%;
    justify-content: center;
  }
  
  .profile-form-container {
    width: 95vw;
  }
  
  .profile-form-header {
    padding: 20px 24px;
  }
  
  .profile-form-content {
    padding: 24px;
  }
  
  .profile-form-actions {
    padding: 20px 24px;
    flex-direction: column;
  }
  
  .form-btn {
    width: 100%;
  }
}

/* Mobile */
@media (max-width: 480px) {
  .settings-header {
    padding: 16px 20px;
  }
  
  .settings-header h3 {
    font-size: 18px;
  }
  
  .settings-tab {
    padding: 12px 16px;
    font-size: 13px;
    min-width: 100px;
  }
  
  .settings-tab-content {
    padding: 20px;
  }
  
  .profile-card {
    padding: 12px;
  }
  
  .profile-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 12px;
  }
  
  .profile-badges {
    align-self: flex-end;
  }
  
  .env-var-item {
    flex-direction: column;
    gap: 8px;
  }
  
  .env-var-actions {
    align-self: flex-end;
  }
}

/* Accessibility */
@media (prefers-reduced-motion: reduce) {
  * {
    animation-duration: 0.01ms !important;
    animation-iteration-count: 1 !important;
    transition-duration: 0.01ms !important;
  }
}

/* High contrast mode */
@media (prefers-contrast: high) {
  .profile-card,
  .form-input,
  .form-select,
  .form-textarea {
    border-width: 2px;
  }
  
  .settings-tab::before {
    height: 4px;
  }
}

/* Dark mode support (for future enhancement) */
@media (prefers-color-scheme: dark) {
  .profile-form-modal {
    backdrop-filter: blur(12px);
  }
}


================================================
FILE: vibe_surf/chrome_extension/styles/settings-utilities.css
================================================
/* Utilities and Loading States */

/* Loading States */
.profiles-loading {
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 60px;
  color: var(--text-muted);
}

.loading-spinner {
  width: 32px;
  height: 32px;
  border: 3px solid var(--border-color);
  border-top: 3px solid var(--primary-color);
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-right: 16px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}


================================================
FILE: vibe_surf/chrome_extension/styles/variables.css
================================================
/* CSS Variables - VibeSurf Extension */

:root {
  /* Colors */
  --primary-color: #007acc;
  --primary-hover: #005a9b;
  --secondary-color: #f0f2f5;
  --accent-color: #28a745;
  --danger-color: #dc3545;
  --warning-color: #ffc107;
  --text-primary: #1a1a1a;
  --text-secondary: #6c757d;
  --text-muted: #9ca3af;
  --border-color: #e5e7eb;
  --border-hover: #d1d5db;
  --bg-primary: #ffffff;
  --bg-secondary: #f8f9fa;
  --bg-tertiary: #f1f3f4;
  --bg-hover: #f5f5f5;
  --bg-active: #e8f4fd;
  --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
  --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
  --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);

  /* Typography */
  --font-size-xs: 0.75rem;
  --font-size-sm: 0.875rem;
  --font-size-base: 0.875rem;
  --font-size-lg: 1rem;
  --font-size-xl: 1.125rem;
  --font-weight-normal: 400;
  --font-weight-medium: 500;
  --font-weight-semibold: 600;
  --font-weight-bold: 700;

  /* Spacing */
  --spacing-xs: 0.25rem;
  --spacing-sm: 0.5rem;
  --spacing-md: 0.75rem;
  --spacing-lg: 1rem;
  --spacing-xl: 1.5rem;
  --spacing-2xl: 2rem;

  /* Border Radius */
  --radius-sm: 0.25rem;
  --radius-md: 0.375rem;
  --radius-lg: 0.5rem;
  --radius-xl: 0.75rem;

  /* Transitions */
  --transition-fast: 150ms ease-in-out;
  --transition-normal: 200ms ease-in-out;
  --transition-slow: 300ms ease-in-out;
}

/* Dark Theme Variables */
[data-theme="dark"],
.dark-theme {
  /* Colors */
  --primary-color: #4fc3f7;
  --primary-hover: #29b6f6;
  --secondary-color: #2d2d2d;
  --accent-color: #66bb6a;
  --danger-color: #ef5350;
  --warning-color: #ffca28;
  --text-primary: #ffffff;
  --text-secondary: #b0b0b0;
  --text-muted: #808080;
  --border-color: #404040;
  --border-hover: #505050;
  --bg-primary: #1a1a1a;
  --bg-secondary: #2d2d2d;
  --bg-tertiary: #3a3a3a;
  --bg-hover: #404040;
  --bg-active: #2a3f5f;
  --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.3);
  --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.4), 0 2px 4px -1px rgba(0, 0, 0, 0.3);
  --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.4), 0 4px 6px -2px rgba(0, 0, 0, 0.3);
}

/* Auto Theme (follows system preference) */
@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]):not(.light-theme) {
    /* Colors */
    --primary-color: #4fc3f7;
    --primary-hover: #29b6f6;
    --secondary-color: #2d2d2d;
    --accent-color: #66bb6a;
    --danger-color: #ef5350;
    --warning-color: #ffca28;
    --text-primary: #ffffff;
    --text-secondary: #b0b0b0;
    --text-muted: #808080;
    --border-color: #404040;
    --border-hover: #505050;
    --bg-primary: #1a1a1a;
    --bg-secondary: #2d2d2d;
    --bg-tertiary: #3a3a3a;
    --bg-hover: #404040;
    --bg-active: #2a3f5f;
    --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.3);
    --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.4), 0 2px 4px -1px rgba(0, 0, 0, 0.3);
    --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.4), 0 4px 6px -2px rgba(0, 0, 0, 0.3);
  }
}


================================================
FILE: vibe_surf/llm/__init__.py
================================================
"""
Vibe Surf LLM implementations.

This module provides LLM implementations for vibe_surf, including:
- ChatOpenAICompatible: OpenAI-compatible implementation with Gemini schema fix support

Example usage:
    from vibe_surf.llm import ChatOpenAICompatible
    
    # Using with Azure OpenAI for Gemini models
    llm = ChatOpenAICompatible(
        model="gemini-2.5-pro",
        base_url="https://your-endpoint.openai.azure.com/",
        api_key="your-api-key",
        temperature=0,
    )
"""

from vibe_surf.llm.openai_compatible import ChatOpenAICompatible

__all__ = ['ChatOpenAICompatible']


================================================
FILE: vibe_surf/llm/openai_compatible.py
================================================
"""
OpenAI-compatible LLM implementation with Gemini schema fix support.

This module provides an OpenAI-compatible chat model that automatically applies
Gemini-specific schema fixes when using Gemini models through OpenAI-compatible APIs
like ChatAzureOpenAI, ChatOpenRouter, etc.

Example usage:
    from vibe_surf.llm.openai_compatible import ChatOpenAICompatible
    
    # Using with Azure OpenAI
    llm = ChatOpenAICompatible(
        model="gemini-2.5-pro",
        base_url="https://your-endpoint.openai.azure.com/",
        api_key="your-api-key",
        temperature=0,
    )
    
    # Using with OpenRouter
    llm = ChatOpenAICompatible(
        model="gemini-2.5-pro", 
        base_url="https://openrouter.ai/api/v1",
        api_key="your-openrouter-key",
        temperature=0,
    )
"""
import pdb
from dataclasses import dataclass
from typing import Any, TypeVar, overload
from pydantic import BaseModel

from browser_use.llm.openai.chat import ChatOpenAI
from browser_use.llm.messages import BaseMessage
from collections.abc import Iterable, Mapping
from dataclasses import dataclass, field
from typing import Any, Literal, TypeVar, overload

import httpx
from openai import APIConnectionError, APIStatusError, AsyncOpenAI, RateLimitError
from openai.types.chat import ChatCompletionContentPartTextParam
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.shared.chat_model import ChatModel
from openai.types.shared_params.reasoning_effort import ReasoningEffort
from openai.types.shared_params.response_format_json_schema import JSONSchema, ResponseFormatJSONSchema
from pydantic import BaseModel

from browser_use.llm.base import BaseChatModel
from browser_use.llm.exceptions import ModelProviderError
from browser_use.llm.messages import BaseMessage
from browser_use.llm.openai.serializer import OpenAIMessageSerializer
from browser_use.llm.schema import SchemaOptimizer
from browser_use.llm.views import ChatInvokeCompletion, ChatInvokeUsage

from json_repair import repair_json

T = TypeVar('T', bound=BaseModel)

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


@dataclass
class ChatOpenAICompatible(ChatOpenAI):
    """
    OpenAI-compatible chat model with automatic schema fix support for Gemini, Kimi, and Qwen models.
    
    This class extends browser_use's ChatOpenAI to automatically detect special models
    and apply the necessary schema fixes to work with OpenAI-compatible APIs.
    
    Supported models:
    - Gemini models: Removes 'additionalProperties', 'title', 'default' and resolves $ref
    - Kimi/Moonshot models: Removes 'min_items', 'max_items', 'minItems', 'maxItems', 'default' with anyOf
    - Qwen models: Ensures 'json' keyword is present in messages when using response_format
    
    The class automatically detects the model type and applies appropriate fixes.
    """

    max_completion_tokens: int | None = 8192

    def _is_gemini_model(self) -> bool:
        """Check if the current model is a Gemini model."""
        return str(self.model).lower().startswith('gemini')

    def _is_kimi_model(self) -> bool:
        """Check if the current model is a Kimi/Moonshot model."""
        model_str = str(self.model).lower()
        return 'kimi' in model_str or 'moonshot' in model_str

    def _is_deepseek_model(self) -> bool:
        """Check if the current model is a Kimi/Moonshot model."""
        model_str = str(self.model).lower()
        return 'deepseek' in model_str
    
    def _is_qwen_model(self) -> bool:
        """Check if the current model is a Qwen model."""
        model_str = str(self.model).lower()
        return 'qwen' in model_str

    def _fix_gemini_schema(self, schema: dict[str, Any]) -> dict[str, Any]:
        """
        Convert a Pydantic model to a Gemini-compatible schema.
        
        This function removes unsupported properties like 'additionalProperties' and resolves
        $ref references that Gemini doesn't support.
        
        Adapted from browser_use.llm.google.chat.ChatGoogle._fix_gemini_schema
        """

        # Handle $defs and $ref resolution
        if '$defs' in schema:
            defs = schema.pop('$defs')

            def resolve_refs(obj: Any) -> Any:
                if isinstance(obj, dict):
                    if '$ref' in obj:
                        ref = obj.pop('$ref')
                        ref_name = ref.split('/')[-1]
                        if ref_name in defs:
                            # Replace the reference with the actual definition
                            resolved = defs[ref_name].copy()
                            # Merge any additional properties from the reference
                            for key, value in obj.items():
                                if key != '$ref':
                                    resolved[key] = value
                            return resolve_refs(resolved)
                        return obj
                    else:
                        # Recursively process all dictionary values
                        return {k: resolve_refs(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [resolve_refs(item) for item in obj]
                return obj

            schema = resolve_refs(schema)

        # Remove unsupported properties
        def clean_schema(obj: Any) -> Any:
            if isinstance(obj, dict):
                # Remove unsupported properties
                cleaned = {}
                for key, value in obj.items():
                    if key not in ['additionalProperties', 'title', 'default']:
                        cleaned_value = clean_schema(value)
                        # Handle empty object properties - Gemini doesn't allow empty OBJECT types
                        if (
                                key == 'properties'
                                and isinstance(cleaned_value, dict)
                                and len(cleaned_value) == 0
                                and isinstance(obj.get('type', ''), str)
                                and obj.get('type', '').upper() == 'OBJECT'
                        ):
                            # Convert empty object to have at least one property
                            cleaned['properties'] = {'_placeholder': {'type': 'string'}}
                        else:
                            cleaned[key] = cleaned_value

                # If this is an object type with empty properties, add a placeholder
                if (
                        isinstance(cleaned.get('type', ''), str)
                        and cleaned.get('type', '').upper() == 'OBJECT'
                        and 'properties' in cleaned
                        and isinstance(cleaned['properties'], dict)
                        and len(cleaned['properties']) == 0
                ):
                    cleaned['properties'] = {'_placeholder': {'type': 'string'}}

                return cleaned
            elif isinstance(obj, list):
                return [clean_schema(item) for item in obj]
            return obj

        return clean_schema(schema)

    def _fix_kimi_schema(self, schema: dict[str, Any]) -> dict[str, Any]:
        """
        Convert a Pydantic model to a Kimi/Moonshot-compatible schema.
        
        This function removes unsupported keywords like 'min_items' that Moonshot API doesn't support.
        
        Args:
            schema: The original JSON schema
            
        Returns:
            A cleaned schema compatible with Moonshot API
        """

        def clean_schema(obj: Any) -> Any:
            if isinstance(obj, dict):
                cleaned = {}
                has_any_of = 'anyOf' in obj
                
                for key, value in obj.items():
                    # Remove unsupported keywords for Moonshot
                    if key in ['min_items', 'minItems']:
                        continue
                    # Remove 'default' when 'anyOf' is present (Moonshot restriction)
                    elif key == 'default' and has_any_of:
                        continue
                    # Remove other problematic keywords
                    elif key in ['title', 'additionalProperties']:
                        continue
                    else:
                        cleaned[key] = clean_schema(value)
                return cleaned
            elif isinstance(obj, list):
                return [clean_schema(item) for item in obj]
            return obj

        return clean_schema(schema)

    @overload
    async def ainvoke(self, messages: list[BaseMessage], output_format: None = None) -> ChatInvokeCompletion[str]:
        ...

    @overload
    async def ainvoke(self, messages: list[BaseMessage], output_format: type[T]) -> ChatInvokeCompletion[T]:
        ...

    async def ainvoke(
            self, messages: list[BaseMessage], output_format: type[T] | None = None
    ) -> ChatInvokeCompletion[T] | ChatInvokeCompletion[str]:
        """
        Invoke the model with the given messages.

        Args:
            messages: List of chat messages
            output_format: Optional Pydantic model class for structured output

        Returns:
            Either a string response or an instance of output_format
        """
        # If this is not a special model or no structured output is requested,
        # use the parent implementation directly
        if self._is_qwen_model() or self._is_kimi_model() or self._is_deepseek_model() :
            self.add_schema_to_system_prompt = True

        if not (self._is_gemini_model() or self._is_kimi_model() or self._is_qwen_model() or self._is_deepseek_model()) or output_format is None:
            return await super().ainvoke(messages, output_format)
        openai_messages = OpenAIMessageSerializer.serialize_messages(messages)

        try:
            model_params: dict[str, Any] = {}

            if self.temperature is not None:
                model_params['temperature'] = self.temperature

            if self.frequency_penalty is not None:
                model_params['frequency_penalty'] = self.frequency_penalty

            if self.max_completion_tokens is not None:
                model_params['max_completion_tokens'] = self.max_completion_tokens
                model_params['max_tokens'] = self.max_completion_tokens

            if self.top_p is not None:
                model_params['top_p'] = self.top_p

            if self.seed is not None:
                model_params['seed'] = self.seed

            if self.service_tier is not None:
                model_params['service_tier'] = self.service_tier

            if self.reasoning_models and any(str(m).lower() in str(self.model).lower() for m in self.reasoning_models):
                model_params['reasoning_effort'] = self.reasoning_effort
                del model_params['temperature']
                del model_params['frequency_penalty']

            if output_format is None:
                # Return string response
                response = await self.get_client().chat.completions.create(
                    model=self.model,
                    messages=openai_messages,
                    **model_params,
                )

                usage = self._get_usage(response)
                return ChatInvokeCompletion(
                    completion=response.choices[0].message.content or '',
                    usage=usage,
                )

            else:
                # Apply appropriate schema fix based on model type
                original_schema = SchemaOptimizer.create_optimized_json_schema(output_format)
                if self._is_gemini_model():
                    logger.debug(f"ğŸ”§ Applying Gemini schema fixes for model: {self.model}")
                    fixed_schema = self._fix_gemini_schema(original_schema)
                elif self._is_kimi_model():
                    logger.debug(f"ğŸ”§ Applying Kimi/Moonshot schema fixes for model: {self.model}")
                    fixed_schema = self._fix_kimi_schema(original_schema)
                else:
                    fixed_schema = original_schema
                response_format: JSONSchema = {
                    'name': 'agent_output',
                    'strict': True,
                    'schema': fixed_schema,
                }

                # Add JSON schema to system prompt if requested
                if self.add_schema_to_system_prompt and openai_messages and openai_messages[0]['role'] == 'system':
                    schema_text = f'\n<json_schema>\n{response_format}\n</json_schema>'
                    if isinstance(openai_messages[0]['content'], str):
                        openai_messages[0]['content'] += schema_text
                    elif isinstance(openai_messages[0]['content'], Iterable):
                        openai_messages[0]['content'] = list(openai_messages[0]['content']) + [
                            ChatCompletionContentPartTextParam(text=schema_text, type='text')
                        ]

                # Return structured response
                if self.add_schema_to_system_prompt:
                    response = await self.get_client().chat.completions.create(
                        model=self.model,
                        messages=openai_messages,
                        response_format={
                            'type': 'json_object'
                        },
                        **model_params,
                    )
                else:
                    response = await self.get_client().chat.completions.create(
                        model=self.model,
                        messages=openai_messages,
                        response_format=ResponseFormatJSONSchema(json_schema=response_format, type='json_schema'),
                        **model_params,
                    )

                if response.choices[0].message.content is None:
                    raise ModelProviderError(
                        message='Failed to parse structured output from model response',
                        status_code=500,
                        model=self.name,
                    )

                usage = self._get_usage(response)
                output_content = response.choices[0].message.content
                try:
                    parsed = output_format.model_validate_json(output_content)
                except Exception as e:
                    repair_content = repair_json(output_content)
                    parsed = output_format.model_validate_json(repair_content)

                return ChatInvokeCompletion(
                    completion=parsed,
                    usage=usage,
                )

        except RateLimitError as e:
            error_message = e.response.json().get('error', {})
            error_message = (
                error_message.get('message', 'Unknown model error') if isinstance(error_message,
                                                                                  dict) else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model=self.name,
            ) from e

        except APIConnectionError as e:
            raise ModelProviderError(message=str(e), model=self.name) from e

        except APIStatusError as e:
            try:
                error_message = e.response.json().get('error', {})
            except Exception:
                error_message = e.response.text
            error_message = (
                error_message.get('message', 'Unknown model error') if isinstance(error_message,
                                                                                  dict) else error_message
            )
            raise ModelProviderError(
                message=error_message,
                status_code=e.response.status_code,
                model=self.name,
            ) from e

        except Exception as e:
            raise ModelProviderError(message=str(e), model=self.name) from e



================================================
FILE: vibe_surf/tools/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/tools/browser_use_tools.py
================================================
import pdb
import os
import asyncio
import json
import enum
import base64
import mimetypes
import datetime
import aiohttp
import re
import urllib.parse
from pathvalidate import sanitize_filename
from typing import Optional, Type, Callable, Dict, Any, Union, Awaitable, TypeVar
from pydantic import BaseModel
from browser_use.tools.service import Tools
import logging
from browser_use.agent.views import ActionModel, ActionResult
from browser_use.utils import time_execution_sync
from browser_use.filesystem.file_system import FileSystem
from browser_use.browser import BrowserSession
from browser_use.browser.events import UploadFileEvent
from browser_use.observability import observe_debug
from browser_use.tools.views import (
    ClickElementAction,
    CloseTabAction,
    DoneAction,
    GetDropdownOptionsAction,
    GoToUrlAction,
    InputTextAction,
    NoParamsAction,
    ScrollAction,
    SearchAction,
    SelectDropdownOptionAction,
    SendKeysAction,
    StructuredOutputAction,
    SwitchTabAction,
    UploadFileAction,
)
from browser_use.llm.base import BaseChatModel
from browser_use.llm.messages import UserMessage, ContentPartTextParam, ContentPartImageParam, ImageURL
from browser_use.dom.service import EnhancedDOMTreeNode
from browser_use.browser.views import BrowserError
from browser_use.mcp.client import MCPClient

from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.tools.views import HoverAction, ExtractionAction, FileExtractionAction, DownloadMediaAction
from vibe_surf.tools.mcp_client import CustomMCPClient
from vibe_surf.tools.file_system import CustomFileSystem
from vibe_surf.logger import get_logger
from vibe_surf.tools.vibesurf_tools import VibeSurfTools

logger = get_logger(__name__)

Context = TypeVar('Context')

T = TypeVar('T', bound=BaseModel)


class BrowserUseTools(Tools, VibeSurfTools):
    def __init__(self,
                 exclude_actions: list[str] = [],
                 output_model: type[T] | None = None,
                 display_files_in_done_text: bool = True,
                 ):
        Tools.__init__(self, exclude_actions=exclude_actions, output_model=output_model,
                       display_files_in_done_text=display_files_in_done_text)
        self._register_browser_actions()
        self._register_file_actions()

    def _register_done_action(self, output_model: type[T] | None, display_files_in_done_text: bool = True):
        if output_model is not None:
            self.display_files_in_done_text = display_files_in_done_text

            @self.registry.action(
                'Complete task - with return text and if the task is finished (success=True) or not yet completely finished (success=False), because last step is reached',
                param_model=StructuredOutputAction[output_model],
            )
            async def done(params: StructuredOutputAction):
                # Exclude success from the output JSON since it's an internal parameter
                output_dict = params.data.model_dump()

                # Enums are not serializable, convert to string
                for key, value in output_dict.items():
                    if isinstance(value, enum.Enum):
                        output_dict[key] = value.value

                return ActionResult(
                    is_done=True,
                    success=params.success,
                    extracted_content=json.dumps(output_dict),
                    long_term_memory=f'Task completed. Success Status: {params.success}',
                )

        else:

            @self.registry.action(
                'Complete task - provide a summary of results for the user. Set success=True if task completed successfully, false otherwise. Text should be your response to the user summarizing results. Include files in files_to_display if you would like to display to the user or there files are important for the task result.',
                param_model=DoneAction,
            )
            async def done(params: DoneAction, file_system: CustomFileSystem):
                user_message = params.text

                len_text = len(params.text)
                len_max_memory = 100
                memory = f'Task completed: {params.success} - {params.text[:len_max_memory]}'
                if len_text > len_max_memory:
                    memory += f' - {len_text - len_max_memory} more characters'

                attachments = []
                if params.files_to_display:
                    if self.display_files_in_done_text:
                        file_msg = ''
                        for file_name in params.files_to_display:
                            if file_name == 'todo.md':
                                continue
                            file_content = await file_system.display_file(file_name)
                            if file_content:
                                file_msg += f'\n\n{file_name}:\n{file_content}'
                                attachments.append(file_name)
                        if file_msg:
                            user_message += '\n\nAttachments:'
                            user_message += file_msg
                        else:
                            logger.warning('Agent wanted to display files but none were found')
                    else:
                        for file_name in params.files_to_display:
                            if file_name == 'todo.md':
                                continue
                            file_content = await file_system.display_file(file_name)
                            if file_content:
                                attachments.append(file_name)

                attachments = [file_name for file_name in attachments]

                return ActionResult(
                    is_done=True,
                    success=params.success,
                    extracted_content=user_message,
                    long_term_memory=memory,
                    attachments=attachments,
                )

    def _register_browser_actions(self):
        """Register custom browser actions"""

        @self.registry.action('Upload file to interactive element with file path', param_model=UploadFileAction)
        async def upload_file_to_element(
                params: UploadFileAction, browser_session: BrowserSession, file_system: FileSystem
        ):

            # For local browsers, ensure the file exists on the local filesystem
            full_file_path = params.path
            if not os.path.exists(full_file_path):
                full_file_path = str(file_system.get_dir() / params.path)
            if not os.path.exists(full_file_path):
                msg = f'File {params.path} does not exist'
                return ActionResult(error=msg)

            # Get the selector map to find the node
            selector_map = await browser_session.get_selector_map()
            if params.index not in selector_map:
                msg = f'Element with index {params.index} does not exist.'
                return ActionResult(error=msg)

            node = selector_map[params.index]

            # Helper function to find file input near the selected element
            def find_file_input_near_element(
                    node: EnhancedDOMTreeNode, max_height: int = 3, max_descendant_depth: int = 3
            ) -> EnhancedDOMTreeNode | None:
                """Find the closest file input to the selected element."""

                def find_file_input_in_descendants(n: EnhancedDOMTreeNode, depth: int) -> EnhancedDOMTreeNode | None:
                    if depth < 0:
                        return None
                    if browser_session.is_file_input(n):
                        return n
                    for child in n.children_nodes or []:
                        result = find_file_input_in_descendants(child, depth - 1)
                        if result:
                            return result
                    return None

                current = node
                for _ in range(max_height + 1):
                    # Check the current node itself
                    if browser_session.is_file_input(current):
                        return current
                    # Check all descendants of the current node
                    result = find_file_input_in_descendants(current, max_descendant_depth)
                    if result:
                        return result
                    # Check all siblings and their descendants
                    if current.parent_node:
                        for sibling in current.parent_node.children_nodes or []:
                            if sibling is current:
                                continue
                            if browser_session.is_file_input(sibling):
                                return sibling
                            result = find_file_input_in_descendants(sibling, max_descendant_depth)
                            if result:
                                return result
                    current = current.parent_node
                    if not current:
                        break
                return None

            # Try to find a file input element near the selected element
            file_input_node = find_file_input_near_element(node)

            # If not found near the selected element, fallback to finding the closest file input to current scroll position
            if file_input_node is None:
                logger.info(
                    f'No file upload element found near index {params.index}, searching for closest file input to scroll position'
                )

                # Get current scroll position
                cdp_session = await browser_session.get_or_create_cdp_session()
                try:
                    scroll_info = await cdp_session.cdp_client.send.Runtime.evaluate(
                        params={'expression': 'window.scrollY || window.pageYOffset || 0'},
                        session_id=cdp_session.session_id
                    )
                    current_scroll_y = scroll_info.get('result', {}).get('value', 0)
                except Exception:
                    current_scroll_y = 0

                # Find all file inputs in the selector map and pick the closest one to scroll position
                closest_file_input = None
                min_distance = float('inf')

                for idx, element in selector_map.items():
                    if browser_session.is_file_input(element):
                        # Get element's Y position
                        if element.absolute_position:
                            element_y = element.absolute_position.y
                            distance = abs(element_y - current_scroll_y)
                            if distance < min_distance:
                                min_distance = distance
                                closest_file_input = element

                if closest_file_input:
                    file_input_node = closest_file_input
                    logger.info(f'Found file input closest to scroll position (distance: {min_distance}px)')
                else:
                    msg = 'No file upload element found on the page'
                    logger.error(msg)
                    raise BrowserError(msg)
                # TODO: figure out why this fails sometimes + add fallback hail mary, just look for any file input on page

            # Dispatch upload file event with the file input node
            try:
                event = browser_session.event_bus.dispatch(UploadFileEvent(node=file_input_node, file_path=full_file_path))
                await event
                await event.event_result(raise_if_any=True, raise_if_none=False)
                msg = f'Successfully uploaded file to index {params.index}'
                logger.info(f'ğŸ“ {msg}')
                return ActionResult(
                    extracted_content=msg,
                    long_term_memory=f'Uploaded file {params.path} to element {params.index}',
                )
            except Exception as e:
                logger.error(f'Failed to upload file: {e}')
                raise BrowserError(f'Failed to upload file: {e}')

        @self.registry.action(
            'Hover over an element',
            param_model=HoverAction,
        )
        async def hover_element(params: HoverAction, browser_session: AgentBrowserSession):
            """Hovers over the element specified by its index from the cached selector map or by XPath."""
            try:
                if params.xpath:
                    # Find element by XPath using CDP
                    cdp_session = await browser_session.get_or_create_cdp_session()
                    result = await cdp_session.cdp_client.send.Runtime.evaluate(
                        params={
                            'expression': f"""
        						(() => {{
        							const element = document.evaluate('{params.xpath}', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
        							if (element) {{
        								const rect = element.getBoundingClientRect();
        								return {{found: true, x: rect.x + rect.width/2, y: rect.y + rect.height/2}};
        							}}
        							return {{found: false}};
        						}})()
        					""",
                            'returnByValue': True,
                        },
                        session_id=cdp_session.session_id,
                    )
                    element_info = result.get('result', {}).get('value', {})
                    if not element_info.get('found'):
                        raise Exception(f'Failed to locate element with XPath {params.xpath}')
                    x, y = element_info['x'], element_info['y']

                elif params.selector:
                    # Find element by CSS selector using CDP
                    cdp_session = await browser_session.get_or_create_cdp_session()
                    result = await cdp_session.cdp_client.send.Runtime.evaluate(
                        params={
                            'expression': f"""
        						(() => {{
        							const element = document.querySelector('{params.selector}');
        							if (element) {{
        								const rect = element.getBoundingClientRect();
        								return {{found: true, x: rect.x + rect.width/2, y: rect.y + rect.height/2}};
        							}}
        							return {{found: false}};
        						}})()
        					""",
                            'returnByValue': True,
                        },
                        session_id=cdp_session.session_id,
                    )
                    element_info = result.get('result', {}).get('value', {})
                    if not element_info.get('found'):
                        raise Exception(f'Failed to locate element with CSS Selector {params.selector}')
                    x, y = element_info['x'], element_info['y']

                elif params.index is not None:
                    # Use index to locate the element
                    selector_map = await browser_session.get_selector_map()
                    if params.index not in selector_map:
                        raise Exception(
                            f'Element index {params.index} does not exist - retry or use alternative actions')
                    element_node = selector_map[params.index]

                    # Get element position
                    if not element_node.absolute_position:
                        raise Exception(f'Element at index {params.index} has no position information')

                    x = element_node.absolute_position.x + element_node.absolute_position.width / 2
                    y = element_node.absolute_position.y + element_node.absolute_position.height / 2

                else:
                    raise Exception('Either index, xpath, or selector must be provided')

                # Perform hover using CDP mouse events
                cdp_session = await browser_session.get_or_create_cdp_session()

                # Move mouse to the element position
                await cdp_session.cdp_client.send.Input.dispatchMouseEvent(
                    params={
                        'type': 'mouseMoved',
                        'x': x,
                        'y': y,
                    },
                    session_id=cdp_session.session_id,
                )

                # Wait a bit for hover state to trigger
                await asyncio.sleep(0.1)

                msg = (
                    f'ğŸ–±ï¸ Hovered over element at index {params.index}'
                    if params.index is not None
                    else f'ğŸ–±ï¸ Hovered over element with XPath {params.xpath}'
                    if params.xpath
                    else f'ğŸ–±ï¸ Hovered over element with selector {params.selector}'
                )
                return ActionResult(extracted_content=msg, include_in_memory=True)

            except Exception as e:
                error_msg = f'âŒ Failed to hover over element: {str(e)}'
                return ActionResult(error=error_msg)

        # =======================
        # NAVIGATION ACTIONS
        # =======================

        @self.registry.action(
            'Search the query using the specified search engine. Defaults to DuckDuckGo (recommended) to avoid reCAPTCHA. Options: duckduckgo, google, bing. Query should be concrete and not vague or super long.',
            param_model=SearchAction,
        )
        async def search(params: SearchAction, browser_session: AgentBrowserSession):
            import urllib.parse

            # Encode query for URL safety
            encoded_query = urllib.parse.quote_plus(params.query)

            # Build search URL based on search engine
            search_engines = {
                'duckduckgo': f'https://duckduckgo.com/?q={encoded_query}',
                'google': f'https://www.google.com/search?q={encoded_query}&udm=14',
                'bing': f'https://www.bing.com/search?q={encoded_query}',
            }

            if params.search_engine.lower() not in search_engines:
                return ActionResult(
                    error=f'Unsupported search engine: {params.search_engine}. Options: duckduckgo, google, bing')

            search_url = search_engines[params.search_engine.lower()]

            try:
                # Use AgentBrowserSession's direct navigation method
                await browser_session.navigate_to_url(search_url, new_tab=False)
                memory = f"Searched Google for '{params.query}'"
                msg = f'ğŸ” {memory}'
                logger.info(msg)
                return ActionResult(extracted_content=memory, include_in_memory=True, long_term_memory=memory)
            except Exception as e:
                logger.error(f'Failed to search Google: {e}')
                return ActionResult(error=f'Failed to search Google for "{params.query}": {str(e)}')

        @self.registry.action(
            'Navigate to URL, set new_tab=True to open in new tab, False to navigate in current tab',
            param_model=GoToUrlAction
        )
        async def go_to_url(params: GoToUrlAction, browser_session: AgentBrowserSession):
            try:
                # Use AgentBrowserSession's direct navigation method
                await browser_session.navigate_to_url(params.url, new_tab=params.new_tab)

                if params.new_tab:
                    memory = f'Opened new tab with URL {params.url}'
                    msg = f'ğŸ”— Opened new tab with url {params.url}'
                else:
                    memory = f'Navigated to {params.url}'
                    msg = f'ğŸ”— {memory}'

                logger.info(msg)
                return ActionResult(extracted_content=msg, include_in_memory=True, long_term_memory=memory)
            except Exception as e:
                logger.error(f'âŒ Navigation failed: {str(e)}')
                return ActionResult(error=f'Navigation failed: {str(e)}')

        @self.registry.action(
            'Go back',
        )
        async def go_back(browser_session: AgentBrowserSession):
            try:
                cdp_session = await browser_session.get_or_create_cdp_session()
                history = await cdp_session.cdp_client.send.Page.getNavigationHistory(session_id=cdp_session.session_id)
                current_index = history['currentIndex']
                entries = history['entries']

                # Check if we can go back
                if current_index <= 0:
                    memory = msg = 'âš ï¸ Cannot go back - no previous entry in history'
                    logger.info(msg)
                    return ActionResult(extracted_content=memory)

                # Navigate to the previous entry
                previous_entry_id = entries[current_index - 1]['id']
                await cdp_session.cdp_client.send.Page.navigateToHistoryEntry(
                    params={'entryId': previous_entry_id}, session_id=cdp_session.session_id
                )

                # Wait for navigation
                await asyncio.sleep(0.5)
                memory = 'Navigated back'
                msg = f'ğŸ”™ {memory}'
                logger.info(msg)
                return ActionResult(extracted_content=memory)
            except Exception as e:
                logger.error(f'Failed to go back: {str(e)}')
                return ActionResult(error=f'Failed to go back: {str(e)}')

        @self.registry.action(
            'Switch tab',
            param_model=SwitchTabAction
        )
        async def switch_tab(params: SwitchTabAction, browser_session: AgentBrowserSession):
            try:

                if params.tab_id:
                    target_id = await browser_session.get_target_id_from_tab_id(params.tab_id)
                elif params.url:
                    target_id = await browser_session.get_target_id_from_url(params.url)
                else:
                    target_id = await browser_session.get_most_recently_opened_target_id()

                # Switch to target using CDP
                await browser_session.get_or_create_cdp_session(target_id, focus=True)

                memory = f'Switched to Tab with ID {target_id[-4:]}'
                logger.info(f'ğŸ”„ {memory}')
                return ActionResult(extracted_content=memory, include_in_memory=True, long_term_memory=memory)
            except Exception as e:
                logger.error(f'Failed to switch tab: {str(e)}')
                return ActionResult(error=f'Failed to switch to tab {params.tab_id or params.url}: {str(e)}')

        @self.registry.action(
            'Take a screenshot of the current page and save it to the file system',
            param_model=NoParamsAction
        )
        async def take_screenshot(_: NoParamsAction, browser_session: AgentBrowserSession, file_system: FileSystem):
            try:
                # Take screenshot using browser session
                screenshot = await browser_session.take_screenshot()

                # Generate timestamp for filename
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

                # Get file system directory path (Path type)
                fs_dir = file_system.get_dir()

                # Create screenshots directory if it doesn't exist
                screenshots_dir = fs_dir / "screenshots"
                screenshots_dir.mkdir(exist_ok=True)

                # Save screenshot to file system
                page_title = await browser_session.get_current_page_title()
                page_title = sanitize_filename(page_title)
                filename = f"{page_title}-{timestamp}.png"
                filepath = screenshots_dir / filename

                with open(filepath, "wb") as f:
                    f.write(base64.b64decode(screenshot))

                msg = f'ğŸ“¸ Screenshot saved to path: {str(filepath.relative_to(fs_dir))}'
                logger.info(msg)
                return ActionResult(
                    extracted_content=msg,
                    include_in_memory=True,
                    long_term_memory=f'Screenshot saved to {str(filepath.relative_to(fs_dir))}',
                )
            except Exception as e:
                error_msg = f'âŒ Failed to take screenshot: {str(e)}'
                logger.error(error_msg)
                return ActionResult(error=error_msg)

        @self.registry.action(
            'Download media from URL and save to filesystem downloads folder',
            param_model=DownloadMediaAction
        )
        async def download_media(params: DownloadMediaAction, file_system: FileSystem):
            """Download media from URL with automatic file format detection"""
            try:
                # Get file system directory path (Path type)
                fs_dir = file_system.get_dir()
                
                # Create downloads directory if it doesn't exist
                downloads_dir = fs_dir / "downloads"
                downloads_dir.mkdir(exist_ok=True)
                
                # Download the file and detect format
                async with aiohttp.ClientSession() as session:
                    async with session.get(params.url) as response:
                        if response.status != 200:
                            raise Exception(f"HTTP {response.status}: Failed to download from {params.url}")
                        
                        # Get content
                        content = await response.read()
                        
                        # Detect file format and extension
                        file_extension = await self._detect_file_format(params.url, response.headers, content)
                        
                        # Generate filename
                        if params.filename:
                            # Use provided filename, add extension if missing
                            filename = params.filename
                            if not filename.endswith(file_extension):
                                filename = f"{filename}{file_extension}"
                        else:
                            # Generate filename from URL or timestamp
                            url_path = urllib.parse.urlparse(params.url).path
                            url_filename = os.path.basename(url_path)
                            
                            if url_filename and not url_filename.startswith('.'):
                                # Use URL filename, ensure correct extension
                                filename = url_filename
                                if not filename.endswith(file_extension):
                                    base_name = os.path.splitext(filename)[0]
                                    filename = f"{base_name}{file_extension}"
                            else:
                                # Generate timestamp-based filename
                                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                                filename = f"media_{timestamp}{file_extension}"
                        
                        # Sanitize filename
                        filename = sanitize_filename(filename)
                        filepath = downloads_dir / filename
                        
                        # Save file
                        with open(filepath, "wb") as f:
                            f.write(content)
                        
                        # Calculate file size for display
                        file_size = len(content)
                        size_str = self._format_file_size(file_size)
                        
                        msg = f'ğŸ“¥ Downloaded media to: {str(filepath.relative_to(fs_dir))} ({size_str})'
                        logger.info(msg)
                        return ActionResult(
                            extracted_content=msg,
                            include_in_memory=True,
                            long_term_memory=f'Downloaded media from {params.url} to {str(filepath.relative_to(fs_dir))}',
                        )
                        
            except Exception as e:
                error_msg = f'âŒ Failed to download media: {str(e)}'
                logger.error(error_msg)
                return ActionResult(error=error_msg)

    async def _detect_file_format(self, url: str, headers: dict, content: bytes) -> str:
        """Detect file format from URL, headers, and content"""

        # Try Content-Type header first
        content_type = headers.get('content-type', '').lower()
        if content_type:
            # Common image formats
            if 'image/jpeg' in content_type or 'image/jpg' in content_type:
                return '.jpg'
            elif 'image/png' in content_type:
                return '.png'
            elif 'image/gif' in content_type:
                return '.gif'
            elif 'image/webp' in content_type:
                return '.webp'
            elif 'image/svg' in content_type:
                return '.svg'
            elif 'image/bmp' in content_type:
                return '.bmp'
            elif 'image/tiff' in content_type:
                return '.tiff'
            # Video formats
            elif 'video/mp4' in content_type:
                return '.mp4'
            elif 'video/webm' in content_type:
                return '.webm'
            elif 'video/avi' in content_type:
                return '.avi'
            elif 'video/mov' in content_type or 'video/quicktime' in content_type:
                return '.mov'
            # Audio formats
            elif 'audio/mpeg' in content_type or 'audio/mp3' in content_type:
                return '.mp3'
            elif 'audio/wav' in content_type:
                return '.wav'
            elif 'audio/ogg' in content_type:
                return '.ogg'
            elif 'audio/webm' in content_type:
                return '.webm'

        # Try magic number detection
        if len(content) >= 8:
            # JPEG
            if content.startswith(b'\xff\xd8\xff'):
                return '.jpg'
            # PNG
            elif content.startswith(b'\x89PNG\r\n\x1a\n'):
                return '.png'
            # GIF
            elif content.startswith(b'GIF87a') or content.startswith(b'GIF89a'):
                return '.gif'
            # WebP
            elif content[8:12] == b'WEBP':
                return '.webp'
            # BMP
            elif content.startswith(b'BM'):
                return '.bmp'
            # TIFF
            elif content.startswith(b'II*\x00') or content.startswith(b'MM\x00*'):
                return '.tiff'
            # MP4
            elif b'ftyp' in content[4:12]:
                return '.mp4'
            # PDF
            elif content.startswith(b'%PDF'):
                return '.pdf'

        # Try URL path extension
        url_path = urllib.parse.urlparse(url).path
        if url_path:
            ext = os.path.splitext(url_path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.bmp', '.tiff',
                      '.mp4', '.webm', '.avi', '.mov', '.wmv', '.flv',
                      '.mp3', '.wav', '.ogg', '.aac', '.flac',
                      '.pdf', '.doc', '.docx', '.txt']:
                return ext

        # Default fallback
        return '.bin'

    def _format_file_size(self, size_bytes: int) -> str:
        """Format file size in human readable format"""
        if size_bytes == 0:
            return "0 B"
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        while size_bytes >= 1024.0 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        return f"{size_bytes:.1f} {size_names[i]}"



================================================
FILE: vibe_surf/tools/file_system.py
================================================
import asyncio
import pdb
import re
import os
from pathlib import Path
from browser_use.filesystem.file_system import FileSystem, FileSystemError, INVALID_FILENAME_ERROR_MESSAGE, \
    FileSystemState
from browser_use.filesystem.file_system import BaseFile, MarkdownFile, TxtFile, JsonFile, CsvFile, PdfFile


class PythonFile(BaseFile):
    """Plain text file implementation"""

    @property
    def extension(self) -> str:
        return 'py'


class HtmlFile(BaseFile):
    """Plain text file implementation"""

    @property
    def extension(self) -> str:
        return 'html'


class JSFile(BaseFile):
    """Plain text file implementation"""

    @property
    def extension(self) -> str:
        return 'js'


class CustomFileSystem(FileSystem):
    def __init__(self, base_dir: str | Path, create_default_files: bool = False):
        # Handle the Path conversion before calling super().__init__
        self.base_dir = Path(base_dir).absolute() if isinstance(base_dir, str) else base_dir
        self.base_dir.mkdir(parents=True, exist_ok=True)

        # Create and use a dedicated subfolder for all operations
        self.data_dir = self.base_dir

        self.data_dir.mkdir(exist_ok=True)

        self._file_types: dict[str, type[BaseFile]] = {
            'md': MarkdownFile,
            'txt': TxtFile,
            'json': JsonFile,
            'csv': CsvFile,
            'pdf': PdfFile,
            'py': PythonFile,
            'html': HtmlFile,
            'js': JSFile,
        }

        self.files = {}
        if create_default_files:
            self.default_files = ['todo.md']
            self._create_default_files()

        self.extracted_content_count = 0

    async def display_file(self, full_filename: str) -> str | None:
        """Display file content using file-specific display method"""
        if not self.file_exist(full_filename):
            return f"{full_filename} does not exist."

        file_content = await self.read_file(full_filename)

        return file_content

    async def read_file(self, full_filename: str, external_file: bool = False) -> str:
        """Read file content using file-specific read method and return appropriate message to LLM"""
        try:
            full_filepath = full_filename if external_file else str(self.data_dir / full_filename)
            is_file_exist = await self.file_exist(full_filepath)
            if not is_file_exist:
                return f"Error: File '{full_filepath}' not found."
            try:
                _, extension = self._parse_filename(full_filename)
            except Exception:
                return f'Error: Invalid filename format {full_filename}. Must be alphanumeric with a supported extension.'
            if extension != 'pdf' and extension in self._file_types.keys():
                with open(str(full_filepath), 'r', encoding="utf-8") as f:
                    content = f.read()
                    return f'Read from file {full_filename}.\n<content>\n{content}\n</content>'
                
            elif extension == 'pdf':
                import pypdf

                reader = pypdf.PdfReader(full_filepath)
                num_pages = len(reader.pages)
                MAX_PDF_PAGES = 10
                extra_pages = num_pages - MAX_PDF_PAGES
                extracted_text = ''
                for page in reader.pages[:MAX_PDF_PAGES]:
                    extracted_text += page.extract_text()
                extra_pages_text = f'{extra_pages} more pages...' if extra_pages > 0 else ''
                return f'Read from file {full_filename}.\n<content>\n{extracted_text}\n{extra_pages_text}</content>'
            else:
                return f'Error: Cannot read content from file {full_filename}.'
        except FileNotFoundError:
            return f"Error: File '{full_filepath}' not found."
        except PermissionError:
            return f"Error: Permission denied to read file '{full_filepath}'."
        except Exception as e:
            return f"Error: Could not read file '{full_filepath}': {str(e)}."

    async def copy_file(self, src_filename: str, dst_filename: str, external_src_file: bool = False) -> str:
        """Copy a file to the FileSystem from src (can be external) to dst filename"""
        import shutil
        from concurrent.futures import ThreadPoolExecutor

        # Check if destination file already exists
        if self.get_file(dst_filename):
            return f"Error: Destination file '{dst_filename}' already exists."

        try:
            src_path = src_filename if external_src_file else (self.data_dir / src_filename)
            dst_path = self.data_dir / dst_filename
            dst_path.parent.mkdir(parents=True, exist_ok=True)
            # Check if source file exists
            if not src_path.exists() if hasattr(src_path, 'exists') else not Path(src_path).exists():
                return f"Error: Source file '{src_filename}' not found."

            # Use shutil to copy file
            with ThreadPoolExecutor() as executor:
                await asyncio.get_event_loop().run_in_executor(executor, shutil.copy2, str(src_path), str(dst_path))

            # Read the copied file content and create file object for internal tracking
            # content = self.read_file(dst_filename)
            # dst_name, dst_extension = self._parse_filename(dst_filename)
            # file_class = self._get_file_type_class(dst_extension)
            #
            # if file_class:
            #     dst_file = file_class(name=dst_name, content=content)
            #     self.files[dst_filename] = dst_file

            source_type = "external file" if external_src_file else "file"
            return f"{source_type.capitalize()} '{src_filename}' copied to '{dst_filename}' successfully."

        except FileNotFoundError:
            return f"Error: Source file '{src_filename}' not found."
        except PermissionError:
            return f"Error: Permission denied to access files."
        except Exception as e:
            return f"Error: Could not copy file '{src_filename}' to '{dst_filename}'. {str(e)}"

    async def rename_file(self, old_filename: str, new_filename: str) -> str:
        """Rename a file within the FileSystem from old_filename to new_filename"""
        import shutil
        from concurrent.futures import ThreadPoolExecutor

        # Check if old file exists
        file_exist = await self.file_exist(old_filename)
        if not file_exist:
            return f"Error: Source File '{old_filename}' not found."

        try:
            new_file_path = os.path.join(os.path.dirname(old_filename), new_filename)
            old_path = self.data_dir / old_filename
            new_path = self.data_dir / new_file_path

            # Use shutil to move/rename file
            with ThreadPoolExecutor() as executor:
                await asyncio.get_event_loop().run_in_executor(executor, shutil.move, str(old_path), str(new_path))

            # Update internal file tracking
            # old_file = self.files[old_filename]
            # del self.files[old_filename]
            #
            # # Update file object name if needed
            # new_name, new_extension = self._parse_filename(new_file_path)
            # old_file.name = new_name
            # self.files[new_file_path] = old_file

            return f"File '{old_filename}' renamed to '{new_file_path}' successfully."

        except Exception as e:
            return f"Error: Could not rename file '{old_filename}' to '{new_file_path}'. {str(e)}"

    async def move_file(self, old_filename: str, new_filename: str) -> str:
        """Move a file within the FileSystem from old_filename to new_filename"""
        import shutil
        from concurrent.futures import ThreadPoolExecutor

        # Check if old file exists
        src_file_exist = await self.file_exist(old_filename)
        if not src_file_exist:
            return f"Error: Source File '{old_filename}' not found."

        # Check if new filename already exists
        dst_file_exist = await self.file_exist(new_filename)
        if dst_file_exist:
            return f"Error: Destination File '{new_filename}' already exists."

        try:
            old_path = self.data_dir / old_filename
            new_path = self.data_dir / new_filename
            new_path.parent.mkdir(parents=True, exist_ok=True)
            # Use shutil to move file
            with ThreadPoolExecutor() as executor:
                await asyncio.get_event_loop().run_in_executor(executor, shutil.move, str(old_path), str(new_path))

            # Update internal file tracking
            # old_file = self.files[old_filename]
            # del self.files[old_filename]
            #
            # # Update file object name if needed
            # new_name, new_extension = self._parse_filename(new_filename)
            # old_file.name = new_name
            # self.files[new_filename] = old_file

            return f"File '{old_filename}' moved to '{new_filename}' successfully."

        except Exception as e:
            return f"Error: Could not move file '{old_filename}' to '{new_filename}'. {str(e)}"

    def get_absolute_path(self, full_filename: str) -> str:
        full_path = self.data_dir.absolute() / full_filename
        return str(full_path)

    def _is_valid_filename(self, file_name: str) -> bool:
        """Check if filename matches the required pattern: name.extension"""
        # Build extensions pattern from _file_types
        file_name = os.path.splitext(file_name)[1]
        extensions = '|'.join(self._file_types.keys())
        pattern = rf'\.({extensions})$'
        return bool(re.match(pattern, file_name))

    async def append_file(self, full_filename: str, content: str) -> str:
        """Append content to file using file-specific append method"""
        if not self._is_valid_filename(full_filename):
            return INVALID_FILENAME_ERROR_MESSAGE

        full_path = self.data_dir / full_filename
        is_file_exist = await self.file_exist(full_filename)
        if not is_file_exist:
            return f"File '{full_filename}' not found."

        try:
            with open(str(full_path), encoding='utf-8', mode='a') as f:
                f.write(content)

            return f'Data appended to file {full_filename} successfully.'
        except FileSystemError as e:
            return str(e)
        except Exception as e:
            return f"Error: Could not append to file '{full_filename}'. {str(e)}"

    async def write_file(self, full_filename: str, content: str) -> str:
        """Write content to file using file-specific write method"""
        if not self._is_valid_filename(full_filename):
            return INVALID_FILENAME_ERROR_MESSAGE

        try:
            full_path = self.data_dir / full_filename
            full_path.parent.mkdir(parents=True, exist_ok=True)
            name_without_ext, extension = self._parse_filename(full_filename)
            file_class = self._get_file_type_class(extension)
            if not file_class:
                raise ValueError(f"Error: Invalid file extension '{extension}' for file '{full_filename}'.")

            # Create or get existing file using full filename as key
            if full_filename in self.files:
                file_obj = self.files[full_filename]
            else:
                file_obj = file_class(name=name_without_ext)
                self.files[full_filename] = file_obj  # Use full filename as key

            with open(str(full_path), encoding='utf-8', mode='w') as f:
                f.write(content)

            return f'Data written to file {full_filename} successfully.'
        except FileSystemError as e:
            return str(e)
        except Exception as e:
            return f"Error: Could not write to file '{full_filename}'. {str(e)}"

    async def file_exist(self, full_filename: str) -> bool:
        full_file_path = self.data_dir / full_filename
        return bool(full_file_path.exists())

    async def create_file(self, full_filename: str) -> str:
        """Create a file with empty content"""
        if not self._is_valid_filename(full_filename):
            return INVALID_FILENAME_ERROR_MESSAGE

        try:
            full_path = self.data_dir / full_filename
            full_path.parent.mkdir(parents=True, exist_ok=True)
            name_without_ext, extension = self._parse_filename(full_filename)
            file_class = self._get_file_type_class(extension)
            if not file_class:
                raise ValueError(f"Error: Invalid file extension '{extension}' for file '{full_filename}'.")

            # Create or get existing file using full filename as key
            if full_filename in self.files:
                file_obj = self.files[full_filename]
            else:
                file_obj = file_class(name=name_without_ext)
                self.files[full_filename] = file_obj  # Use full filename as key

            # Use file-specific write method
            with open(str(full_path), encoding='utf-8', mode='w') as f:
                f.write('')
            return f'Create file {full_filename} successfully.'
        except FileSystemError as e:
            return str(e)
        except Exception as e:
            return f"Error: Could not write to file '{full_filename}'. {str(e)}"

    async def save_extracted_content(self, content: str) -> str:
        """Save extracted content to a numbered file"""
        initial_filename = f'extracted_content_{self.extracted_content_count}'
        extracted_filename = f'{initial_filename}.md'
        await self.write_file(initial_filename, content)
        self.extracted_content_count += 1
        return f'Extracted content saved to file {extracted_filename} successfully.'

    async def list_directory(self, directory_path: str = "") -> str:
        """List contents of a directory within the file system (data_dir only)"""
        try:
            # Construct the full path within data_dir
            if directory_path and directory_path.strip() != ".":
                # Remove leading slash if present and ensure relative path
                directory_path = directory_path.lstrip('/')
                full_path = self.data_dir / directory_path
            else:
                full_path = self.data_dir

            # Ensure the path is within data_dir for security
            try:
                full_path = full_path.resolve()
                self.data_dir.resolve()
                if not str(full_path).startswith(str(self.data_dir.resolve())):
                    return f"Error: Access denied. Path '{directory_path}' is outside the file system."
            except Exception:
                return f"Error: Invalid directory path '{directory_path}'."

            # Check if directory exists
            if not full_path.exists():
                return f"Error: Directory '{directory_path or '.'}' does not exist."

            if not full_path.is_dir():
                return f"Error: '{directory_path or '.'}' is not a directory."

            # List directory contents
            items = []
            for item in sorted(full_path.iterdir()):
                relative_path = item.relative_to(full_path)
                if item.is_dir():
                    items.append(f"ğŸ“ {relative_path}/")
                else:
                    file_size = item.stat().st_size
                    if file_size < 1024:
                        size_str = f"{file_size}B"
                    elif file_size < 1024 * 1024:
                        size_str = f"{file_size // 1024}KB"
                    else:
                        size_str = f"{file_size // (1024 * 1024)}MB"
                    items.append(f"ğŸ“„ {relative_path} ({size_str})")

            if not items:
                return f"Directory '{directory_path or '.'}' is empty."

            directory_display = directory_path or "."
            return f"Contents of directory '{directory_display}':\n" + "\n".join(items)

        except Exception as e:
            return f"Error: Could not list directory '{directory_path or '.'}': {str(e)}"

    async def create_directory(self, directory_path: str) -> str:
        """Create a directory within the file system (data_dir only)"""
        try:
            if not directory_path or not directory_path.strip():
                return "Error: Directory path cannot be empty."

            # Remove leading slash if present and ensure relative path
            directory_path = directory_path.strip().lstrip('/')
            full_path = self.data_dir / directory_path

            # Ensure the path is within data_dir for security
            try:
                full_path = full_path.resolve()
                self.data_dir.resolve()
                if not str(full_path).startswith(str(self.data_dir.resolve())):
                    return f"Error: Access denied. Cannot create directory '{directory_path}' outside the file system."
            except Exception:
                return f"Error: Invalid directory path '{directory_path}'."

            # Check if directory already exists
            if full_path.exists():
                if full_path.is_dir():
                    return f"Directory '{directory_path}' already exists."
                else:
                    return f"Error: '{directory_path}' already exists as a file."

            # Create directory (including parent directories)
            full_path.mkdir(parents=True, exist_ok=True)

            return f"Directory '{directory_path}' created successfully."

        except Exception as e:
            return f"Error: Could not create directory '{directory_path}': {str(e)}"

    @classmethod
    def from_state(cls, state: FileSystemState) -> 'FileSystem':
        """Restore file system from serializable state at the exact same location"""
        # Create file system without default files
        fs = cls(base_dir=Path(state.base_dir), create_default_files=False)
        fs.extracted_content_count = state.extracted_content_count

        # Restore all files
        for full_filename, file_data in state.files.items():
            file_type = file_data['type']
            file_info = file_data['data']

            # Create the appropriate file object based on type
            if file_type == 'MarkdownFile':
                file_obj = MarkdownFile(**file_info)
            elif file_type == 'TxtFile':
                file_obj = TxtFile(**file_info)
            elif file_type == 'JsonFile':
                file_obj = JsonFile(**file_info)
            elif file_type == 'CsvFile':
                file_obj = CsvFile(**file_info)
            elif file_type == 'PdfFile':
                file_obj = PdfFile(**file_info)
            elif file_type == 'JSFile':
                file_obj = JSFile(**file_info)
            elif file_type == 'PythonFile':
                file_obj = PythonFile(**file_info)
            elif file_type == 'HtmlFile':
                file_obj = HtmlFile(**file_info)
            else:
                # Skip unknown file types
                continue

            # Add to files dict and sync to disk
            fs.files[full_filename] = file_obj
            file_obj.sync_to_disk_sync(fs.data_dir)

        return fs



================================================
FILE: vibe_surf/tools/finance_tools.py
================================================
"""
Comprehensive finance tools using Yahoo Finance API.
Provides access to stock market data, company financials, and trading information.
"""
import pdb
from enum import Enum
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta
import yfinance as yf
import pandas as pd
from datetime import datetime

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class FinanceMethod(Enum):
    """Available Yahoo Finance data methods"""
    # Basic Information
    GET_INFO = "get_info"  # Company basic information
    GET_FAST_INFO = "get_fast_info"  # Quick stats like current price, volume
    
    # Market Data & History
    GET_HISTORY = "get_history"  # Historical price and volume data
    GET_ACTIONS = "get_actions"  # Dividends and stock splits
    GET_DIVIDENDS = "get_dividends"  # Dividend history
    GET_SPLITS = "get_splits"  # Stock split history
    GET_CAPITAL_GAINS = "get_capital_gains"  # Capital gains distributions
    
    # Financial Statements
    GET_FINANCIALS = "get_financials"  # Income statement (annual)
    GET_QUARTERLY_FINANCIALS = "get_quarterly_financials"  # Income statement (quarterly)
    GET_BALANCE_SHEET = "get_balance_sheet"  # Balance sheet (annual)
    GET_QUARTERLY_BALANCE_SHEET = "get_quarterly_balance_sheet"  # Balance sheet (quarterly)
    GET_CASHFLOW = "get_cashflow"  # Cash flow statement (annual)
    GET_QUARTERLY_CASHFLOW = "get_quarterly_cashflow"  # Cash flow statement (quarterly)
    
    # Earnings & Analysis
    GET_EARNINGS = "get_earnings"  # Historical earnings data
    GET_QUARTERLY_EARNINGS = "get_quarterly_earnings"  # Quarterly earnings
    GET_EARNINGS_DATES = "get_earnings_dates"  # Upcoming earnings dates
    GET_CALENDAR = "get_calendar"  # Earnings calendar
    
    # Recommendations & Analysis
    GET_RECOMMENDATIONS = "get_recommendations"  # Analyst recommendations
    GET_RECOMMENDATIONS_SUMMARY = "get_recommendations_summary"  # Summary of recommendations
    GET_UPGRADES_DOWNGRADES = "get_upgrades_downgrades"  # Rating changes
    GET_ANALYSIS = "get_analysis"  # Analyst analysis
    
    # Ownership & Holdings
    GET_MAJOR_HOLDERS = "get_major_holders"  # Major shareholders
    GET_INSTITUTIONAL_HOLDERS = "get_institutional_holders"  # Institutional holdings
    GET_MUTUALFUND_HOLDERS = "get_mutualfund_holders"  # Mutual fund holdings
    GET_INSIDER_PURCHASES = "get_insider_purchases"  # Insider purchases
    GET_INSIDER_TRANSACTIONS = "get_insider_transactions"  # Insider transactions
    GET_INSIDER_ROSTER_HOLDERS = "get_insider_roster_holders"  # Insider roster
    
    # Additional Data
    GET_NEWS = "get_news"  # Latest news
    GET_SUSTAINABILITY = "get_sustainability"  # ESG scores
    GET_SEC_FILINGS = "get_sec_filings"  # SEC filings
    GET_SHARES = "get_shares"  # Share count data
    
    # Options (if applicable)
    GET_OPTIONS = "get_options"  # Option chain data


class FinanceDataRetriever:
    """Main class for retrieving and formatting Yahoo Finance data"""
    
    def __init__(self, symbol: str):
        """Initialize with stock symbol"""
        self.symbol = symbol.upper()
        self.ticker = yf.Ticker(self.symbol)
        
    def get_finance_data(self, methods: List[str], **kwargs) -> Dict[str, Any]:
        """
        Retrieve finance data using specified methods
        
        Args:
            methods: List of method names (FinanceMethod enum values)
            **kwargs: Additional parameters (e.g., period, start_date, end_date, num_news)
        
        Returns:
            Dictionary with method names as keys and data as values
        """
        results = {}
        
        for method in methods:
            try:
                if hasattr(self, f"_{method}"):
                    method_func = getattr(self, f"_{method}")
                    results[method] = method_func(**kwargs)
                else:
                    results[method] = f"Error: Method {method} not implemented"
                    logger.warning(f"Method {method} not implemented for {self.symbol}")
            except Exception as e:
                error_msg = f"Error retrieving {method}: {str(e)}"
                results[method] = error_msg
                logger.error(f"Error retrieving {method} for {self.symbol}: {e}")
        
        return results
    
    # Basic Information Methods
    def _get_info(self, **kwargs) -> Dict:
        """Get basic company information"""
        return self.ticker.info
    
    def _get_fast_info(self, **kwargs) -> Dict:
        """Get quick statistics"""
        try:
            fast_info = self.ticker.fast_info
            return dict(fast_info) if hasattr(fast_info, '__dict__') else fast_info
        except:
            return self.ticker.get_fast_info()
    
    # Market Data & History Methods
    def _get_history(self, **kwargs) -> pd.DataFrame:
        """Get historical price and volume data"""
        period = kwargs.get('period', '1y')
        start_date = kwargs.get('start_date')
        end_date = kwargs.get('end_date')
        interval = kwargs.get('interval', '1d')
        
        if start_date and end_date:
            return self.ticker.history(start=start_date, end=end_date, interval=interval)
        else:
            return self.ticker.history(period=period, interval=interval)
    
    def _get_actions(self, **kwargs) -> pd.DataFrame:
        """Get dividend and stock split history"""
        return self.ticker.actions
    
    def _get_dividends(self, **kwargs) -> pd.Series:
        """Get dividend history"""
        return self.ticker.dividends
    
    def _get_splits(self, **kwargs) -> pd.Series:
        """Get stock split history"""
        return self.ticker.splits
    
    def _get_capital_gains(self, **kwargs) -> pd.Series:
        """Get capital gains distributions"""
        return self.ticker.capital_gains
    
    # Financial Statements Methods
    def _get_financials(self, **kwargs) -> pd.DataFrame:
        """Get annual income statement"""
        return self.ticker.financials
    
    def _get_quarterly_financials(self, **kwargs) -> pd.DataFrame:
        """Get quarterly income statement"""
        return self.ticker.quarterly_financials
    
    def _get_balance_sheet(self, **kwargs) -> pd.DataFrame:
        """Get annual balance sheet"""
        return self.ticker.balance_sheet
    
    def _get_quarterly_balance_sheet(self, **kwargs) -> pd.DataFrame:
        """Get quarterly balance sheet"""
        return self.ticker.quarterly_balance_sheet
    
    def _get_cashflow(self, **kwargs) -> pd.DataFrame:
        """Get annual cash flow statement"""
        return self.ticker.cashflow
    
    def _get_quarterly_cashflow(self, **kwargs) -> pd.DataFrame:
        """Get quarterly cash flow statement"""
        return self.ticker.quarterly_cashflow
    
    # Earnings & Analysis Methods
    def _get_earnings(self, **kwargs) -> pd.DataFrame:
        """Get historical earnings data"""
        return self.ticker.earnings
    
    def _get_quarterly_earnings(self, **kwargs) -> pd.DataFrame:
        """Get quarterly earnings data"""
        return self.ticker.quarterly_earnings
    
    def _get_earnings_dates(self, **kwargs) -> pd.DataFrame:
        """Get earnings dates and estimates"""
        return self.ticker.earnings_dates
    
    def _get_calendar(self, **kwargs) -> Dict:
        """Get earnings calendar"""
        return self.ticker.calendar
    
    # Recommendations & Analysis Methods
    def _get_recommendations(self, **kwargs) -> pd.DataFrame:
        """Get analyst recommendations history"""
        return self.ticker.recommendations
    
    def _get_recommendations_summary(self, **kwargs) -> pd.DataFrame:
        """Get summary of analyst recommendations"""
        return self.ticker.recommendations_summary
    
    def _get_upgrades_downgrades(self, **kwargs) -> pd.DataFrame:
        """Get analyst upgrades and downgrades"""
        return self.ticker.upgrades_downgrades
    
    def _get_analysis(self, **kwargs) -> pd.DataFrame:
        """Get analyst analysis"""
        return getattr(self.ticker, 'analysis', pd.DataFrame())
    
    # Ownership & Holdings Methods
    def _get_major_holders(self, **kwargs) -> pd.DataFrame:
        """Get major shareholders"""
        return self.ticker.major_holders
    
    def _get_institutional_holders(self, **kwargs) -> pd.DataFrame:
        """Get institutional holdings"""
        return self.ticker.institutional_holders
    
    def _get_mutualfund_holders(self, **kwargs) -> pd.DataFrame:
        """Get mutual fund holdings"""
        return self.ticker.mutualfund_holders
    
    def _get_insider_purchases(self, **kwargs) -> pd.DataFrame:
        """Get insider purchases"""
        return getattr(self.ticker, 'insider_purchases', pd.DataFrame())
    
    def _get_insider_transactions(self, **kwargs) -> pd.DataFrame:
        """Get insider transactions"""
        return getattr(self.ticker, 'insider_transactions', pd.DataFrame())
    
    def _get_insider_roster_holders(self, **kwargs) -> pd.DataFrame:
        """Get insider roster"""
        return getattr(self.ticker, 'insider_roster_holders', pd.DataFrame())
    
    # Additional Data Methods
    def _get_news(self, **kwargs) -> List[Dict]:
        """Get latest news"""
        num_news = kwargs.get('num_news', 5)
        news = self.ticker.news
        return news[:num_news] if news else []
    
    def _get_sustainability(self, **kwargs) -> pd.DataFrame:
        """Get ESG sustainability data"""
        return getattr(self.ticker, 'sustainability', pd.DataFrame())
    
    def _get_sec_filings(self, **kwargs) -> pd.DataFrame:
        """Get SEC filings"""
        return getattr(self.ticker, 'sec_filings', pd.DataFrame())
    
    def _get_shares(self, **kwargs) -> pd.DataFrame:
        """Get share count data"""
        return getattr(self.ticker, 'shares', pd.DataFrame())
    
    def _get_options(self, **kwargs) -> Dict:
        """Get options data"""
        try:
            option_dates = self.ticker.options
            if option_dates:
                # Get the first available expiration date
                first_expiry = option_dates[0]
                opt_chain = self.ticker.option_chain(first_expiry)
                return {
                    'expiration_dates': list(option_dates),
                    'calls': opt_chain.calls,
                    'puts': opt_chain.puts,
                    'selected_expiry': first_expiry
                }
            return {'expiration_dates': [], 'calls': pd.DataFrame(), 'puts': pd.DataFrame()}
        except:
            return {'error': 'Options data not available for this ticker'}


class FinanceMarkdownFormatter:
    """Formats finance data into markdown"""
    
    @staticmethod
    def format_finance_data(symbol: str, results: Dict[str, Any], methods: List[str]) -> str:
        """Format all finance data as markdown"""
        markdown = f"# ğŸ’¹ Financial Data for {symbol.upper()}\n\n"
        
        for method in methods:
            data = results.get(method)
            
            if isinstance(data, str) and data.startswith('Error'):
                markdown += f"## âŒ {method.replace('_', ' ').title()}\n{data}\n\n"
                continue
                
            markdown += f"## ğŸ“Š {method.replace('_', ' ').title()}\n\n"
            
            # Route to appropriate formatter
            formatter_method = f"_format_{method}"
            if hasattr(FinanceMarkdownFormatter, formatter_method):
                formatter = getattr(FinanceMarkdownFormatter, formatter_method)
                markdown += formatter(data)
            else:
                # Generic formatter for unhandled methods
                markdown += FinanceMarkdownFormatter._format_generic(data)
                
            markdown += "\n\n"
        
        return markdown.strip()
    
    @staticmethod
    def _format_generic(data: Any) -> str:
        """Generic formatter for any data type"""
        if data is None or (hasattr(data, 'empty') and data.empty):
            return "No data available.\n"
        
        if isinstance(data, pd.DataFrame):
            if len(data) == 0:
                return "No data available.\n"
            return f"```\n{data.to_string()}\n```\n"
        elif isinstance(data, pd.Series):
            if len(data) == 0:
                return "No data available.\n"
            return f"```\n{data.to_string()}\n```\n"
        elif isinstance(data, (list, dict)):
            import json
            return f"```json\n{json.dumps(data, indent=2, default=str)}\n```\n"
        else:
            return f"```\n{str(data)}\n```\n"
    
    @staticmethod
    def _format_get_info(info: Dict) -> str:
        """Format company info as markdown"""
        if not info:
            return "No company information available.\n"
            
        markdown = ""
        
        # Basic company info
        if 'longName' in info:
            markdown += f"**Company Name:** {info['longName']}\n"
        if 'sector' in info:
            markdown += f"**Sector:** {info['sector']}\n"
        if 'industry' in info:
            markdown += f"**Industry:** {info['industry']}\n"
        if 'website' in info:
            markdown += f"**Website:** {info['website']}\n"
        if 'country' in info:
            markdown += f"**Country:** {info['country']}\n"
        
        markdown += "\n### ğŸ’° Financial Metrics\n"
        
        # Financial metrics
        if 'marketCap' in info and info['marketCap']:
            markdown += f"**Market Cap:** ${info['marketCap']:,.0f}\n"
        if 'enterpriseValue' in info and info['enterpriseValue']:
            markdown += f"**Enterprise Value:** ${info['enterpriseValue']:,.0f}\n"
        if 'totalRevenue' in info and info['totalRevenue']:
            markdown += f"**Total Revenue:** ${info['totalRevenue']:,.0f}\n"
        if 'grossMargins' in info and info['grossMargins']:
            markdown += f"**Gross Margin:** {info['grossMargins']:.2%}\n"
        if 'profitMargins' in info and info['profitMargins']:
            markdown += f"**Profit Margin:** {info['profitMargins']:.2%}\n"
        
        markdown += "\n### ğŸ“ˆ Stock Price Info\n"
        
        # Stock price info
        if 'currentPrice' in info and info['currentPrice']:
            markdown += f"**Current Price:** ${info['currentPrice']:.2f}\n"
        if 'previousClose' in info and info['previousClose']:
            markdown += f"**Previous Close:** ${info['previousClose']:.2f}\n"
        if 'fiftyTwoWeekHigh' in info and info['fiftyTwoWeekHigh']:
            markdown += f"**52 Week High:** ${info['fiftyTwoWeekHigh']:.2f}\n"
        if 'fiftyTwoWeekLow' in info and info['fiftyTwoWeekLow']:
            markdown += f"**52 Week Low:** ${info['fiftyTwoWeekLow']:.2f}\n"
        if 'dividendYield' in info and info['dividendYield']:
            markdown += f"**Dividend Yield:** {info['dividendYield']:.2%}\n"
        
        # Business summary
        if 'longBusinessSummary' in info:
            summary = info['longBusinessSummary'][:500]
            if len(info['longBusinessSummary']) > 500:
                summary += "..."
            markdown += f"\n### ğŸ“‹ Business Summary\n{summary}\n"
        
        return markdown
    
    @staticmethod
    def _format_get_fast_info(fast_info) -> str:
        """Format fast info as markdown"""
        if not fast_info:
            return "No fast info available.\n"
            
        markdown = ""
        
        # Convert to dict if needed
        if hasattr(fast_info, '__dict__'):
            data = fast_info.__dict__
        elif isinstance(fast_info, dict):
            data = fast_info
        else:
            return f"Fast info data: {str(fast_info)}\n"
        
        # Format key metrics
        for key, value in data.items():
            if value is not None:
                key_formatted = key.replace('_', ' ').title()
                if isinstance(value, (int, float)):
                    if 'price' in key.lower() or 'value' in key.lower():
                        markdown += f"**{key_formatted}:** ${value:,.2f}\n"
                    elif 'volume' in key.lower():
                        markdown += f"**{key_formatted}:** {value:,}\n"
                    else:
                        markdown += f"**{key_formatted}:** {value}\n"
                else:
                    markdown += f"**{key_formatted}:** {value}\n"
        
        return markdown
    
    @staticmethod
    def _format_get_history(history: pd.DataFrame) -> str:
        """Format historical data as markdown"""
        if history.empty:
            return "No historical data available.\n"
        
        markdown = f"**Period:** {history.index.min().strftime('%Y-%m-%d')} to {history.index.max().strftime('%Y-%m-%d')}\n"
        markdown += f"**Total Records:** {len(history)}\n\n"
        
        # Determine how much data to show based on total records
        total_records = len(history)
        if total_records <= 30:
            # Show all data if 30 records or less
            display_data = history
            markdown += f"### ğŸ“ˆ Historical Data (All {total_records} Records)\n\n"
        else:
            # Show recent 30 records for larger datasets
            display_data = history.tail(30)
            markdown += f"### ğŸ“ˆ Recent Data (Last 30 Records)\n\n"
        
        markdown += "| Date | Open | High | Low | Close | Volume |\n"
        markdown += "|------|------|------|-----|-------|--------|\n"
        
        for date, row in display_data.iterrows():
            markdown += f"| {date.strftime('%Y-%m-%d')} | ${row['Open']:.2f} | ${row['High']:.2f} | ${row['Low']:.2f} | ${row['Close']:.2f} | {row['Volume']:,} |\n"
        
        # Summary statistics
        markdown += "\n### ğŸ“Š Summary Statistics\n"
        markdown += f"**Highest Price:** ${history['High'].max():.2f}\n"
        markdown += f"**Lowest Price:** ${history['Low'].min():.2f}\n"
        markdown += f"**Average Volume:** {history['Volume'].mean():,.0f}\n"
        markdown += f"**Total Volume:** {history['Volume'].sum():,}\n"
        
        return markdown
    
    @staticmethod
    def _format_get_news(news: List[Dict]) -> str:
        """Format news data as markdown"""
        if not news:
            return "No news available.\n"
            
        markdown = f"**Total News Articles:** {len(news)}\n\n"
        for i, article in enumerate(news, 1):
            if isinstance(article, dict):
                # Handle new yfinance news structure with nested 'content'
                content = article.get('content', article)  # Fallback to article itself for backwards compatibility
                
                # Extract title
                title = (content.get('title') or
                        content.get('headline') or
                        content.get('summary') or
                        article.get('title') or  # Fallback to old format
                        'No title available')
                
                # Extract content type if available
                content_type = content.get('contentType', '')
                type_emoji = "ğŸ¥" if content_type == "VIDEO" else "ğŸ“°"
                
                # Extract link/URL - try new nested structure first
                link = ''
                if 'canonicalUrl' in content and isinstance(content['canonicalUrl'], dict):
                    link = content['canonicalUrl'].get('url', '')
                elif 'clickThroughUrl' in content and isinstance(content['clickThroughUrl'], dict):
                    link = content['clickThroughUrl'].get('url', '')
                else:
                    # Fallback to old format
                    link = (content.get('link') or
                           content.get('url') or
                           content.get('guid') or
                           article.get('link') or '')
                
                # Extract publisher - try new nested structure first
                publisher = 'Unknown'
                if 'provider' in content and isinstance(content['provider'], dict):
                    publisher = content['provider'].get('displayName', 'Unknown')
                else:
                    # Fallback to old format
                    publisher = (content.get('publisher') or
                               content.get('source') or
                               content.get('author') or
                               article.get('publisher') or
                               'Unknown')
                
                # Extract publication time
                publish_time = (content.get('pubDate') or
                              content.get('providerPublishTime') or
                              content.get('timestamp') or
                              content.get('published') or
                              article.get('providerPublishTime') or '')
                
                # Format the article
                markdown += f"### {type_emoji} {i}. {title}\n"
                if content_type:
                    markdown += f"**Type:** {content_type}\n"
                markdown += f"**Publisher:** {publisher}\n"
                
                if publish_time:
                    try:
                        # Handle different timestamp formats
                        if isinstance(publish_time, (int, float)):
                            dt = datetime.fromtimestamp(publish_time)
                            markdown += f"**Published:** {dt.strftime('%Y-%m-%d %H:%M')}\n"
                        elif isinstance(publish_time, str):
                            # Try to parse ISO format first (new format)
                            try:
                                if publish_time.endswith('Z'):
                                    dt = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                                    markdown += f"**Published:** {dt.strftime('%Y-%m-%d %H:%M UTC')}\n"
                                else:
                                    # Try to parse as Unix timestamp
                                    publish_time_int = int(float(publish_time))
                                    dt = datetime.fromtimestamp(publish_time_int)
                                    markdown += f"**Published:** {dt.strftime('%Y-%m-%d %H:%M')}\n"
                            except:
                                markdown += f"**Published:** {publish_time}\n"
                    except Exception as e:
                        # If timestamp parsing fails, show raw value
                        markdown += f"**Published:** {publish_time}\n"
                
                if link:
                    markdown += f"**Link:** {link}\n"
                
                # Add summary or description if available
                summary = (content.get('summary') or
                          content.get('description') or
                          content.get('snippet') or
                          article.get('summary') or '')
                if summary and summary != title:
                    # Clean HTML tags from description if present
                    import re
                    clean_summary = re.sub(r'<[^>]+>', '', summary)
                    clean_summary = re.sub(r'\s+', ' ', clean_summary).strip()
                    
                    # Limit summary length
                    if len(clean_summary) > 300:
                        clean_summary = clean_summary[:300] + "..."
                    markdown += f"**Summary:** {clean_summary}\n"
                
                # Add metadata if available
                if 'metadata' in content and isinstance(content['metadata'], dict):
                    if content['metadata'].get('editorsPick'):
                        markdown += f"**Editor's Pick:** âœ…\n"
                
                markdown += "\n"
        
        return markdown
    
    @staticmethod
    def _format_get_dividends(dividends: pd.Series) -> str:
        """Format dividend data as markdown"""
        if dividends.empty:
            return "No dividend data available.\n"

        markdown = f"**Total Dividends Recorded:** {len(dividends)}\n"
        markdown += f"**Date Range:** {dividends.index.min().strftime('%Y-%m-%d')} to {dividends.index.max().strftime('%Y-%m-%d')}\n\n"

        # Recent dividends (last 10)
        recent_dividends = dividends.tail(10)
        markdown += "### ğŸ’° Recent Dividends\n\n"
        markdown += "| Date | Dividend Amount |\n"
        markdown += "|------|----------------|\n"
        
        for date, amount in recent_dividends.items():
            markdown += f"| {date.strftime('%Y-%m-%d')} | ${amount:.4f} |\n"
        
        # Summary
        markdown += f"\n**Total Dividends Paid:** ${dividends.sum():.4f}\n"
        markdown += f"**Average Dividend:** ${dividends.mean():.4f}\n"
        if len(dividends) > 1:
            yearly_frequency = len(dividends) / ((dividends.index.max() - dividends.index.min()).days / 365.25)
            markdown += f"**Estimated Annual Frequency:** {yearly_frequency:.1f} times per year\n"
        
        return markdown
    
    @staticmethod
    def _format_get_recommendations(recommendations: pd.DataFrame) -> str:
        """Format recommendations as markdown"""
        if recommendations.empty:
            return "No recommendations available.\n"
            
        markdown = f"**Total Recommendations:** {len(recommendations)}\n\n"
        
        # Recent recommendations (last 15)
        recent_recs = recommendations.tail(15)
        markdown += "### ğŸ“Š Recent Analyst Recommendations\n\n"
        markdown += "| Date | Firm | To Grade | From Grade | Action |\n"
        markdown += "|------|------|----------|------------|--------|\n"
        
        for _, rec in recent_recs.iterrows():
            date = rec.get('Date', 'N/A')
            firm = rec.get('Firm', 'N/A')
            to_grade = rec.get('To Grade', 'N/A')
            from_grade = rec.get('From Grade', 'N/A')
            action = rec.get('Action', 'N/A')
            
            markdown += f"| {date} | {firm} | {to_grade} | {from_grade} | {action} |\n"
        
        return markdown
    
    @staticmethod
    def _format_get_earnings(earnings: pd.DataFrame) -> str:
        """Format earnings as markdown"""
        if earnings.empty:
            return "No earnings data available.\n"
            
        markdown = "### ğŸ’¼ Annual Earnings History\n\n"
        markdown += "| Year | Revenue | Earnings |\n"
        markdown += "|------|---------|----------|\n"
        
        for year, row in earnings.iterrows():
            revenue = row.get('Revenue', 'N/A')
            earnings_val = row.get('Earnings', 'N/A')
            
            # Format numbers if they're numeric
            if isinstance(revenue, (int, float)):
                revenue = f"${revenue:,.0f}"
            if isinstance(earnings_val, (int, float)):
                earnings_val = f"${earnings_val:,.0f}"
            
            markdown += f"| {year} | {revenue} | {earnings_val} |\n"
        
        return markdown


================================================
FILE: vibe_surf/tools/mcp_client.py
================================================
import asyncio
import logging
import time
from typing import Any

from browser_use.telemetry import MCPClientTelemetryEvent, ProductTelemetry
from browser_use.utils import get_browser_use_version
from browser_use.mcp.client import MCPClient
from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client

from vibe_surf.logger import get_logger

logger = get_logger(__name__)


class CustomMCPClient(MCPClient):
    async def connect(self, timeout: int = 200) -> None:
        """Connect to the MCP server and discover available tools."""
        if self._connected:
            logger.debug(f'Already connected to {self.server_name}')
            return

        start_time = time.time()
        error_msg = None

        try:
            logger.info(f"ğŸ”Œ Connecting to MCP server '{self.server_name}': {self.command} {' '.join(self.args)}")

            # Create server parameters
            server_params = StdioServerParameters(command=self.command, args=self.args, env=self.env)

            # Start stdio client in background task
            self._stdio_task = asyncio.create_task(self._run_stdio_client(server_params))

            # Wait for connection to be established
            retries = 0
            max_retries = timeout / 0.1  # 10 second timeout (increased for parallel test execution)
            while not self._connected and retries < max_retries:
                await asyncio.sleep(0.1)
                retries += 1

            if not self._connected:
                error_msg = f"Failed to connect to MCP server '{self.server_name}' after {max_retries * 0.1} seconds"
                raise RuntimeError(error_msg)

            logger.info(f"ğŸ“¦ Discovered {len(self._tools)} tools from '{self.server_name}': {list(self._tools.keys())}")

        except Exception as e:
            error_msg = str(e)
            raise
        finally:
            # Capture telemetry for connect action
            duration = time.time() - start_time
            self._telemetry.capture(
                MCPClientTelemetryEvent(
                    server_name=self.server_name,
                    command=self.command,
                    tools_discovered=len(self._tools),
                    version=get_browser_use_version(),
                    action='connect',
                    duration_seconds=duration,
                    error_message=error_msg,
                )
            )



================================================
FILE: vibe_surf/tools/report_writer_tools.py
================================================
from browser_use.tools.registry.service import Registry
from vibe_surf.tools.vibesurf_tools import VibeSurfTools
from vibe_surf.tools.file_system import CustomFileSystem
from browser_use.tools.views import NoParamsAction
from vibe_surf.tools.vibesurf_registry import VibeSurfRegistry


class ReportWriterTools(VibeSurfTools):
    def __init__(self, exclude_actions: list[str] = []):
        self.registry = VibeSurfRegistry(exclude_actions)
        self._register_file_actions()
        self._register_done_action()

    def _register_done_action(self):
        @self.registry.action(
            description="Finish writing report.",
            param_model=NoParamsAction
        )
        async def task_done(
                _: NoParamsAction,
        ):
            pass



================================================
FILE: vibe_surf/tools/vibesurf_registry.py
================================================
from browser_use.tools.registry.service import Registry, Context
import asyncio
import functools
import inspect
import logging
import re
from collections.abc import Callable
from inspect import Parameter, iscoroutinefunction, signature
from types import UnionType
from typing import Any, Generic, Optional, TypeVar, Union, get_args, get_origin

import pyotp
from pydantic import BaseModel, Field, RootModel, create_model

from browser_use.browser import BrowserSession
from browser_use.filesystem.file_system import FileSystem
from browser_use.llm.base import BaseChatModel
from browser_use.observability import observe_debug
from browser_use.telemetry.service import ProductTelemetry
from browser_use.tools.registry.views import (
    ActionModel,
    ActionRegistry,
    RegisteredAction,
    SpecialActionParameters,
)
from browser_use.utils import is_new_tab_page, match_url_with_domain_pattern, time_execution_async

from vibe_surf.logger import get_logger
from vibe_surf.browser.browser_manager import BrowserManager

logger = get_logger(__name__)


class VibeSurfRegistry(Registry):
    def _get_special_param_types(self) -> dict[str, type | UnionType | None]:
        """Get the expected types for special parameters from SpecialActionParameters"""
        # Manually define the expected types to avoid issues with Optional handling.
        # we should try to reduce this list to 0 if possible, give as few standardized objects to all the actions
        # but each driver should decide what is relevant to expose the action methods,
        # e.g. CDP client, 2fa code getters, sensitive_data wrappers, other context, etc.
        return {
            'context': None,  # Context is a TypeVar, so we can't validate type
            'browser_session': BrowserSession,
            'page_url': str,
            'cdp_client': None,  # CDPClient type from cdp_use, but we don't import it here
            'page_extraction_llm': BaseChatModel,
            'available_file_paths': list,
            'has_sensitive_data': bool,
            'file_system': FileSystem,
            'llm': BaseChatModel,
            'browser_manager': BrowserManager
        }



================================================
FILE: vibe_surf/tools/views.py
================================================
from typing import Generic, TypeVar
from pydantic import BaseModel, ConfigDict, Field


class HoverAction(BaseModel):
    """Parameters for hover action"""
    index: int | None = None
    xpath: str | None = None
    selector: str | None = None


class ExtractionAction(BaseModel):
    query: str = Field(
        default="summary this page",
        description='Extraction goal',
    )
    extract_links: bool | None = Field(
        default=False,
        description='Whether to extract links',
    )
    tab_id: str | None = Field(
        default=None,
        min_length=4,
        max_length=4,
        description='exact 4 character Tab ID of the tab for extraction',
    )  # last 4 chars of TargetID


class FileExtractionAction(BaseModel):
    """Parameters for file content extraction action"""
    file_path: str = Field(
        description='Path to the file to extract content from',
    )
    query: str = Field(
        default="Extract and summarize the content from this file",
        description='Query or instruction for content extraction',
    )


class BrowserUseAgentTask(BaseModel):
    """Parameters for a single browser_use agent task"""
    tab_id: str | None = Field(
        default=None,
        description='Tab ID to execute the task on. If None, a new blank page will be created',
    )
    task: str = Field(
        description='Task description focusing on what needs to be done, goals, and expected returns. Browser_use agent has its own planning and execution capabilities',
    )
    task_files: list[str] | None = Field(
        default=None,
        description='Optional list of file paths that may be needed for executing this task',
    )


class BrowserUseAgentExecution(BaseModel):
    """Parameters for executing browser_use agent tasks in parallel"""
    tasks: list[BrowserUseAgentTask] = Field(
        description='List of tasks to execute concurrently using browser_use agents for improved efficiency. '
                    'If only one task is provided, the agent can take over the entire browser and can also see and operate all tabs.',
        min_length=1,
    )

class BrowserUseFile(BaseModel):
    file_path: str = Field(description='Path to the file')
    file_description: str = Field(
        description='Description of the file. Briefly describe what this file is and what key information it contains.',
    )


class BrowserUseDoneAction(BaseModel):
    """Parameters for done browser_use agent tasks"""
    text: str
    files_to_return: list[BrowserUseFile] | None = Field(
        description='List of files relative to user request or task.',
    )


class ReportWriterTask(BaseModel):
    """Parameters for report writer agent task"""
    task: str = Field(
        description='Task description including report requirements, goals, insights seen, and any hints or tips for generating the report',
    )


class TodoGenerateAction(BaseModel):
    """Parameters for generating todo.md file"""
    todo_items: list[str] = Field(
        description='List of todo items to write to todo.md file',
        min_length=1,
    )


class TodoModification(BaseModel):
    """Single todo modification operation"""
    action: str = Field(
        description='Type of modification: "add", "remove", "complete", or "uncompleted"',
    )
    item: str = Field(
        description='Text of the todo item to operate on',
    )

class TodoModifyAction(BaseModel):
    """Parameters for modifying todo items"""
    modifications: list[TodoModification] = Field(
        description='List of todo modifications to apply',
        min_length=1,
    )



class VibeSurfDoneAction(BaseModel):
    """Parameters for task completion output"""
    response: str = Field(
        description='Task completion response - can be simple response for basic tasks or comprehensive markdown summary for complex tasks with key findings, results, and file links',
    )
    suggestion_follow_tasks: list[str] | None = Field(
        default=None,
        description='Optional list of 1-3 suggested follow-up tasks. Each task can only be described in one sentence, and each task must be strongly related to or extended from the original task.',
        max_length=3,
    )


class SkillSearchAction(BaseModel):
    """Parameters for skill_search action"""
    query: str = Field(
        description='Search query to generate multiple search tasks and find relevant information',
    )


class SkillCrawlAction(BaseModel):
    """Parameters for skill_crawl action"""
    query: str = Field(
        description='Query describing what structured information to extract from the webpage',
    )
    tab_id: str | None = Field(
        default=None,
        min_length=4,
        max_length=4,
        description='Optional 4 character Tab ID to extract from specific tab',
    )


class SkillSummaryAction(BaseModel):
    """Parameters for skill_summary action"""
    tab_id: str | None = Field(
        default=None,
        min_length=4,
        max_length=4,
        description='Optional 4 character Tab ID to summarize specific tab',
    )


class SkillTakeScreenshotAction(BaseModel):
    """Parameters for skill_take_screenshot action"""
    tab_id: str | None = Field(
        default=None,
        min_length=4,
        max_length=4,
        description='Optional 4 character Tab ID to take screenshot of specific tab',
    )


class SkillDeepResearchAction(BaseModel):
    """Parameters for skill_deep_research action"""
    topic: str = Field(
        description='Research topic for deep investigation',
    )


class SkillCodeAction(BaseModel):
    """Parameters for skill_code action"""
    code_requirement: str = Field(
        description='Functional requirement or code prompt describing what the JavaScript code should accomplish. Can be a description like "extract products with price over $100", requirements, or complete/incomplete JavaScript code snippets that will be processed by LLM to generate proper executable code.',
    )
    tab_id: str | None = Field(
        default=None,
        min_length=4,
        max_length=4,
        description='Optional 4 character Tab ID to execute code on specific tab',
    )


class SkillFinanceAction(BaseModel):
    """Parameters for skill_finance action"""
    symbol: str = Field(
        description='Stock symbol to retrieve financial data for (e.g., AAPL, GOOG, TSLA)',
    )
    methods: list[str] | None = Field(
        default=None,
        description='List of finance methods to retrieve. Common methods: get_info (basic company info), get_history (stock price history), get_news (latest news), get_dividends (dividend history), get_earnings (earnings data), get_fast_info (quick stats), get_recommendations (analyst recommendations), get_financials (income statement), get_balance_sheet (balance sheet), get_cashflow (cash flow). If empty, defaults to get_info. Full list available in FinanceMethod enum.',
    )
    period: str = Field(
        default='1y',
        description='Time period for historical data (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)',
    )
    start_date: str | None = Field(
        default=None,
        description='Start date for historical data (YYYY-MM-DD format). Use with end_date instead of period.',
    )
    end_date: str | None = Field(
        default=None,
        description='End date for historical data (YYYY-MM-DD format). Use with start_date instead of period.',
    )
    interval: str = Field(
        default='1d',
        description='Data interval for historical data (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo)',
    )
    num_news: int = Field(
        default=5,
        description='Number of news articles to retrieve when get_news method is selected',
        ge=1,
        le=20,
    )


class DownloadMediaAction(BaseModel):
    """Parameters for downloading media from URL"""
    url: str = Field(
        description='URL of the media to download',
    )
    filename: str | None = Field(
        default=None,
        description='Optional custom filename. If not provided, will auto-detect from URL or Content-Disposition header',
    )
 

class SkillXhsAction(BaseModel):
    """Parameters for skill_xhs action - Xiaohongshu API skill"""
    method: str = Field(
        description='''Xiaohongshu API method name. Available methods:
        - search_content_by_keyword: Search content by keyword, params required: {"keyword": "search keyword", "page": 1, "page_size": 20}
        - fetch_content_details: Get content details, params required: {"content_id": "content ID", "xsec_token": "security token"}
        - fetch_all_content_comments: Get all comments for content, params required: {"content_id": "content ID", "xsec_token": "security token", "max_comments": 100}
        - get_user_profile: Get user profile, params required: {"user_id": "user ID"}
        - fetch_all_user_content: Get all content by user, params required: {"user_id": "user ID", "max_content": 100}
        - get_home_recommendations: Get home page recommendations, params: {}'''
    )
    params: str = Field(
        description='JSON string of method parameters, provide corresponding parameters according to the method parameter. Example: {"keyword": "food"}'
    )


class SkillWeiboAction(BaseModel):
    """Parameters for skill_weibo action - Weibo API skill"""
    method: str = Field(
        description='''Weibo API method name. Available methods:
        - search_posts_by_keyword: Search posts by keyword, params required: {"keyword": "search keyword", "page": 1}
        - get_post_detail: Get post details, params required: {"mid": "post ID"}
        - get_all_post_comments: Get all comments for post, params required: {"mid": "post ID", "max_comments": 100}
        - get_user_info: Get user information, params required: {"user_id": "user ID"}
        - get_all_user_posts: Get all posts by user, params required: {"user_id": "user ID", "max_posts": 100}
        - get_hot_posts: Get hot posts(æ¨èæ¦œï¼‰, params: {}
        - get_trending_posts: Get trending posts(çƒ­æœæ¦œï¼‰, params: {}'''
    )
    params: str = Field(
        description='JSON string of method parameters, provide corresponding parameters according to the method parameter. Example: {"keyword": "AI trending"}'
    )


class SkillDouyinAction(BaseModel):
    """Parameters for skill_douyin action - Douyin API skill"""
    method: str = Field(
        description='''Douyin API method name. Available methods:
        - search_content_by_keyword: Search videos by keyword, params required: {"keyword": "search keyword", "offset": 0}
        - fetch_video_details: Get video details, params required: {"aweme_id": "video ID"}
        - fetch_all_video_comments: Get all comments for video, params required: {"aweme_id": "video ID", "max_comments": 100}
        - fetch_user_info: Get user information, params required: {"sec_user_id": "user security ID"}
        - fetch_all_user_videos: Get all videos by user, params required: {"sec_user_id": "user security ID", "max_videos": 100}'''
    )
    params: str = Field(
        description='JSON string of method parameters, provide corresponding parameters according to the method parameter. Example: {"keyword": "music"}'
    )


class SkillYoutubeAction(BaseModel):
    """Parameters for skill_youtube action - YouTube API skill"""
    method: str = Field(
        description='''YouTube API method name. Available methods:
        - search_videos: Search videos, params required: {"query": "search keyword", "max_results": 20}
        - get_video_details: Get video details, params required: {"video_id": "video ID"}
        - get_video_comments: Get video comments, params required: {"video_id": "video ID", "max_comments": 200}
        - get_channel_info: Get channel information, params required: {"channel_id": "channel ID"}
        - get_channel_videos: Get channel videos, params required: {"channel_id": "channel ID", "max_videos": 20}
        - get_trending_videos: Get trending videos, params: {}
        - get_video_transcript: Get video transcript, params required: {"video_id": "video ID", "languages": ["en", "zh-CN"] (optional, defaults to ["en"])}'''
    )
    params: str = Field(
        description='JSON string of method parameters, provide corresponding parameters according to the method parameter. Example: {"query": "tech tutorial", "max_results": 30}'
    )



================================================
FILE: vibe_surf/tools/voice_asr.py
================================================
import os
import pdb

import dashscope
from openai import OpenAI
from google import genai
from typing import Optional
from vibe_surf.logger import get_logger

logger = get_logger(__name__)

class QwenASR:
    def __init__(self, model="qwen3-asr-flash", api_key: Optional[str] = None):
        dashscope.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        self.model = model or "qwen3-asr-flash"

    def asr(self, wav_url: str):
        if not wav_url.startswith("http"):
            assert os.path.exists(wav_url), f"{wav_url} not exists!"
            wav_url = f"file://{wav_url}"

        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"audio": wav_url},
                    ]
                }
            ]
            response = dashscope.MultiModalConversation.call(
                model=self.model,
                messages=messages,
                result_format="message",
                asr_options={
                    "enable_lid": True,
                    "enable_itn": False
                }
            )
            if response.status_code != 200:
                raise Exception(f"http status_code: {response.status_code} {response}")
            output = response['output']['choices'][0]
            if output['finish_reason'] not in ('stop', 'function_call'):
                logger.warning(f'{self.model} finish with error...\n{response}')
                return ""
            recog_text = output["message"]["content"][0]["text"]
            return recog_text
        except Exception as e:
            logger.warning(str(e))
            return ""


class OpenAIASR:
    def __init__(self, model="whisper-1", api_key: Optional[str] = None, base_url: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or os.getenv("OPENAI_ENDPOINT")
        self.model = model or "whisper-1"
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def asr(self, wav_url: str):
        try:
            # Handle file path
            if wav_url.startswith("file://"):
                file_path = wav_url[7:]  # Remove file:// prefix
            elif not wav_url.startswith("http"):
                file_path = wav_url
            else:
                raise ValueError("OpenAI Whisper ASR only supports local files")
            
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Audio file not found: {file_path}")
            
            # Open and transcribe the audio file
            with open(file_path, "rb") as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model=self.model,
                    file=audio_file,
                    response_format="text"
                )
            
            return transcript if isinstance(transcript, str) else transcript.text
        except Exception as e:
            logger.warning(f"OpenAI Whisper ASR error: {str(e)}")
            return ""


class GeminiASR:
    def __init__(self, model="gemini-2.5-flash", api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        self.model = model or "gemini-2.5-flash"
        if not self.api_key:
            raise ValueError("Google API key is required for Gemini ASR")
        
        # Initialize the genai client
        self.client = genai.Client(api_key=self.api_key)

    def asr(self, wav_url: str):
        try:
            # Handle file path
            if wav_url.startswith("file://"):
                file_path = wav_url[7:]  # Remove file:// prefix
            elif not wav_url.startswith("http"):
                file_path = wav_url
            else:
                raise ValueError("Gemini ASR only supports local files")
            
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Audio file not found: {file_path}")
            
            # Upload the audio file using the client
            audio_file = self.client.files.upload(file=file_path)
            
            # Generate content with the audio file
            response = self.client.models.generate_content(
                model=self.model,
                contents=[
                    "Please transcribe the audio to text. Only return the transcribed text without any additional commentary.",
                    audio_file
                ]
            )
            
            return response.text if response.text else ""
        except Exception as e:
            logger.warning(f"Gemini ASR error: {str(e)}")
            return ""


================================================
FILE: vibe_surf/tools/website_api/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/tools/website_api/douyin/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/tools/website_api/douyin/client.py
================================================
import asyncio
import json
import copy
import pdb
import time
import urllib.parse
import os
from typing import Dict, List, Optional, Callable, Union, Any
import httpx
import random
from tenacity import retry, stop_after_attempt, wait_fixed

try:
    import execjs

    HAS_EXECJS = True
except ImportError:
    HAS_EXECJS = False

from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.logger import get_logger

from .helpers import (
    SearchChannelType, SearchSortType, PublishTimeType,
    generate_web_id, generate_trace_id, create_common_params,
    extract_cookies_from_browser, create_referer_url,
    extract_aweme_media_urls, DouyinError, NetworkError,
    DataExtractionError, AuthenticationError, RateLimitError,
    VerificationError
)

logger = get_logger(__name__)


class DouyinApiClient:
    """
    Douyin API client with integrated browser session management.
    This client handles API communication through browser session for authentication.
    """

    def __init__(self, browser_session: AgentBrowserSession, timeout: int = 60, proxy: Optional[str] = None):
        """
        Initialize the Douyin API client
        
        Args:
            browser_session: Browser session for authentication
            timeout: Request timeout in seconds
            proxy: Proxy URL if needed
        """
        self.browser_session = browser_session
        self.target_id = None
        self.proxy = proxy
        self.timeout = timeout
        self._host = "https://www.douyin.com"

        # Default headers
        self.default_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
            "Host": "www.douyin.com",
            "Origin": "https://www.douyin.com/",
            "Referer": "https://www.douyin.com/",
            "Content-Type": "application/json;charset=UTF-8",
        }
        self.cookies = {}

    async def setup(self, target_id: Optional[str] = None):
        """
        Setup Douyin client by navigating to the site and extracting cookies
        
        Args:
            target_id: Specific target ID to use, or None to create new
            
        Raises:
            AuthenticationError: If unable to access Douyin properly
        """
        try:
            if self.target_id and self.cookies:
                logger.info("Douyin client already setup. Returning!")
                return

            if target_id:
                self.target_id = target_id
            else:
                self.target_id = await self.browser_session.navigate_to_url(
                    "https://www.douyin.com/", new_tab=True
                )
                await asyncio.sleep(3)  # Wait for page to load

            cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)
            result = await asyncio.wait_for(
                cdp_session.cdp_client.send.Storage.getCookies(session_id=cdp_session.session_id),
                timeout=8.0
            )
            web_cookies = result.get('cookies', [])
            user_agent_result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "navigator.userAgent",
                    'returnByValue': True,
                    'awaitPromise': True
                },
                session_id=cdp_session.session_id,
            )
            user_agent = user_agent_result.get('result', {}).get('value')
            if user_agent:
                self.default_headers["User-Agent"] = user_agent
            cookie_str, cookie_dict = extract_cookies_from_browser(web_cookies)
            if cookie_str:
                self.default_headers["Cookie"] = cookie_str
            self.cookies = cookie_dict

            logger.info(f"Douyin client setup completed with {len(cookie_dict)} cookies")

        except Exception as e:
            logger.error(f"Failed to setup Douyin client: {e}")
            raise AuthenticationError(f"Douyin client setup failed: {e}")

    async def _get_local_storage_token(self) -> Optional[str]:
        """Get msToken from browser local storage"""
        try:
            cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)
            result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "window.localStorage.getItem('xmst')",
                    'returnByValue': True,
                    'awaitPromise': True
                },
                session_id=cdp_session.session_id,
            )
            return result.get('result', {}).get('value')
        except Exception as e:
            logger.warning(f"Failed to get local storage token: {e}")
            return None

    def _init_js_context(self):
        """Initialize JavaScript context for signature generation"""
        if not HAS_EXECJS:
            logger.warning("execjs not available, signature generation disabled")
            return None

        try:
            js_file_path = os.path.join(os.path.dirname(__file__), 'douyin.js')
            if not os.path.exists(js_file_path):
                logger.warning(f"douyin.js file not found at {js_file_path}")
                return None

            with open(js_file_path, 'r', encoding='utf-8-sig') as f:
                js_content = f.read()

            return execjs.compile(js_content)
        except Exception as e:
            logger.error(f"Failed to initialize JS context: {e}")
            return None

    async def _get_a_bogus_signature(self, uri: str, params: str, post_data: Dict = None) -> str:
        """
        Get a-bogus signature using JavaScript execution
        
        Args:
            uri: Request URI
            params: URL parameters string
            post_data: POST data if applicable
            
        Returns:
            a-bogus signature string
        """
        try:
            if not hasattr(self, '_js_context'):
                self._js_context = self._init_js_context()

            if not self._js_context:
                return ""

            user_agent = self.default_headers.get('User-Agent', '')

            # Determine the signature function name based on URI
            sign_function_name = "sign_datail"
            if "/reply" in uri:
                sign_function_name = "sign_reply"

            # Call the JavaScript function
            a_bogus = self._js_context.call(sign_function_name, params, user_agent)
            return a_bogus or ""

        except Exception as e:
            logger.warning(f"Failed to generate a-bogus signature: {e}")
            return ""

    async def _prepare_request_params(self, uri: str, params: Optional[Dict] = None,
                                      headers: Optional[Dict] = None, request_method: str = "GET",
                                      post_data: Optional[Dict] = None):
        """
        Prepare request parameters with common Douyin parameters and signatures
        
        Args:
            uri: Request URI
            params: Request parameters
            headers: Request headers
            request_method: HTTP method
            post_data: POST data if applicable
        """
        if not params:
            params = {}

        headers = headers or copy.deepcopy(self.default_headers)

        # Add common parameters
        common_params = create_common_params()

        # Add msToken from local storage
        ms_token = await self._get_local_storage_token()
        if ms_token:
            common_params["msToken"] = ms_token

        params.update(common_params)

        # Generate query string
        query_string = urllib.parse.urlencode(params)

        # Get a-bogus signature
        post_data = post_data or {}
        a_bogus = await self._get_a_bogus_signature(uri, query_string, post_data)
        if a_bogus:
            params["a_bogus"] = a_bogus

        return params, headers

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    async def _make_request(self, method: str, url: str, **kwargs) -> Union[str, Dict]:
        """
        Make HTTP request with error handling and retries
        
        Args:
            method: HTTP method
            url: Request URL
            **kwargs: Additional request parameters
            
        Returns:
            Response data
        """
        async with httpx.AsyncClient(proxy=self.proxy) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)

        # Handle common error responses
        if response.text == "" or response.text == "blocked":
            logger.error(f"Request blocked, response.text: {response.text}")
            raise VerificationError("Account may be blocked or requires verification")

        try:
            data = response.json()

            # Check for successful response
            if response.status_code == 200:
                return data
            else:
                error_msg = data.get("message", "Request failed")
                raise DataExtractionError(f"API error: {error_msg}")

        except json.JSONDecodeError:
            if response.status_code == 200:
                return response.text
            else:
                raise DataExtractionError(f"Invalid response: {response.text[:200]}")

    async def get_request(self, uri: str, params: Optional[Dict] = None, headers: Optional[Dict] = None):
        """Make GET request with Douyin-specific parameter preparation"""
        params, headers = await self._prepare_request_params(uri, params, headers, "GET")
        return await self._make_request("GET", f"{self._host}{uri}", params=params, headers=headers)

    async def post_request(self, uri: str, data: Dict, headers: Optional[Dict] = None):
        """Make POST request with Douyin-specific parameter preparation"""
        data, headers = await self._prepare_request_params(uri, data, headers, "POST", post_data=data)
        return await self._make_request("POST", f"{self._host}{uri}", data=data, headers=headers)

    async def search_content_by_keyword(
            self,
            keyword: str,
            offset: int = 0,
            search_channel: SearchChannelType = SearchChannelType.GENERAL,
            sort_type: SearchSortType = SearchSortType.GENERAL,
            publish_time: PublishTimeType = PublishTimeType.UNLIMITED,
            search_id: str = "",
    ) -> List[Dict]:
        """
        Search content by keyword using Douyin Web Search API
        
        Args:
            keyword: Search keyword
            offset: Pagination offset
            search_channel: Search channel type
            sort_type: Sort method
            publish_time: Time filter
            search_id: Search session ID
            
        Returns:
            List of simplified aweme data
        """
        query_params = {
            'search_channel': search_channel.value,
            'enable_history': '1',
            'keyword': keyword,
            'search_source': 'tab_search',
            'query_correct_type': '1',
            'is_filter_search': '0',
            'offset': offset,
            'count': '15',
            'need_filter_settings': '1',
            'list_type': 'multi',
            'search_id': search_id,
        }

        # Add filters if not default
        if sort_type.value != SearchSortType.GENERAL.value or publish_time.value != PublishTimeType.UNLIMITED.value:
            query_params["filter_selected"] = json.dumps({
                "sort_type": str(sort_type.value),
                "publish_time": str(publish_time.value)
            })
            query_params["is_filter_search"] = 1
            query_params["search_source"] = "tab_search"

        referer_url = create_referer_url(keyword=keyword)
        headers = copy.copy(self.default_headers)
        headers["Referer"] = referer_url

        search_result = await self.get_request("/aweme/v1/web/general/search/single/", query_params, headers)

        # Return simplified aweme list
        aweme_list = []
        for post_item in search_result.get("data", []):
            try:
                aweme_info: Dict = (
                            post_item.get("aweme_info") or post_item.get("aweme_mix_info", {}).get("mix_items")[0])
            except (TypeError, IndexError):
                continue

            if not aweme_info or not aweme_info.get("aweme_id"):
                continue

            user_info = aweme_info.get("author", {})
            interact_info = aweme_info.get("statistics", {})

            # Simplified aweme data
            aweme_data = {
                "aweme_id": aweme_info.get("aweme_id"),
                "aweme_type": str(aweme_info.get("aweme_type", "")),
                "title": aweme_info.get("desc", ""),
                "desc": aweme_info.get("desc", ""),
                "create_time": aweme_info.get("create_time"),
                "user_id": user_info.get("uid"),
                "sec_uid": user_info.get("sec_uid"),
                "short_user_id": user_info.get("short_id"),
                "user_unique_id": user_info.get("unique_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar_thumb", {}).get("url_list", [""])[0],
                "liked_count": str(interact_info.get("digg_count", 0)),
                "collected_count": str(interact_info.get("collect_count", 0)),
                "comment_count": str(interact_info.get("comment_count", 0)),
                "share_count": str(interact_info.get("share_count", 0)),
                "ip_location": aweme_info.get("ip_label", ""),
                "aweme_url": f"https://www.douyin.com/video/{aweme_info.get('aweme_id')}",
            }
            aweme_list.append(aweme_data)

        return aweme_list

    async def fetch_video_details(self, aweme_id: str) -> Dict:
        """
        Fetch detailed video information by aweme ID
        
        Args:
            aweme_id: Video ID
            
        Returns:
            Simplified video details data
        """
        params = {"aweme_id": aweme_id}
        headers = copy.copy(self.default_headers)
        if "Origin" in headers:
            del headers["Origin"]

        response = await self.get_request("/aweme/v1/web/aweme/detail/", params, headers)
        aweme_detail = response.get("aweme_detail", {})

        if not aweme_detail:
            return {}

        user_info = aweme_detail.get("author", {})
        interact_info = aweme_detail.get("statistics", {})

        return {
            "aweme_id": aweme_detail.get("aweme_id"),
            "aweme_type": str(aweme_detail.get("aweme_type", "")),
            "title": aweme_detail.get("desc", ""),
            "desc": aweme_detail.get("desc", ""),
            "create_time": aweme_detail.get("create_time"),
            "user_id": user_info.get("uid"),
            "sec_uid": user_info.get("sec_uid"),
            "short_user_id": user_info.get("short_id"),
            "user_unique_id": user_info.get("unique_id"),
            "nickname": user_info.get("nickname"),
            "avatar": user_info.get("avatar_thumb", {}).get("url_list", [""])[0],
            "liked_count": str(interact_info.get("digg_count", 0)),
            "collected_count": str(interact_info.get("collect_count", 0)),
            "comment_count": str(interact_info.get("comment_count", 0)),
            "share_count": str(interact_info.get("share_count", 0)),
            "ip_location": aweme_detail.get("ip_label", ""),
            "aweme_url": f"https://www.douyin.com/video/{aweme_detail.get('aweme_id')}",
        }

    async def fetch_video_comments(self, aweme_id: str, cursor: int = 0) -> List[Dict]:
        """
        Fetch video comments with pagination
        
        Args:
            aweme_id: Video ID
            cursor: Pagination cursor
            
        Returns:
            List of simplified comments data
        """
        uri = "/aweme/v1/web/comment/list/"
        params = {
            "aweme_id": aweme_id,
            "cursor": cursor,
            "count": 20,
            "item_type": 0
        }

        headers = copy.copy(self.default_headers)
        headers["Referer"] = create_referer_url(aweme_id=aweme_id)

        response = await self.get_request(uri, params, headers)

        # Return simplified comments
        comments = []
        for comment_item in response.get("comments", []):
            if not comment_item.get("cid"):
                continue

            user_info = comment_item.get("user", {})
            avatar_info = (user_info.get("avatar_medium", {}) or
                           user_info.get("avatar_300x300", {}) or
                           user_info.get("avatar_168x168", {}) or
                           user_info.get("avatar_thumb", {}) or {})

            comment_data = {
                "comment_id": comment_item.get("cid"),
                "create_time": comment_item.get("create_time"),
                "ip_location": comment_item.get("ip_label", ""),
                "aweme_id": aweme_id,
                "content": comment_item.get("text"),
                "user_id": user_info.get("uid"),
                "sec_uid": user_info.get("sec_uid"),
                "short_user_id": user_info.get("short_id"),
                "user_unique_id": user_info.get("unique_id"),
                "nickname": user_info.get("nickname"),
                "avatar": avatar_info.get("url_list", [""])[0],
                "sub_comment_count": str(comment_item.get("reply_comment_total", 0)),
                "like_count": comment_item.get("digg_count", 0),
                "parent_comment_id": comment_item.get("reply_id", "0"),
            }
            comments.append(comment_data)

        return comments

    async def fetch_comment_replies(self, aweme_id: str, comment_id: str, cursor: int = 0) -> List[Dict]:
        """
        Fetch replies to a specific comment
        
        Args:
            aweme_id: Video ID
            comment_id: Parent comment ID
            cursor: Pagination cursor
            
        Returns:
            List of simplified reply comments data
        """
        uri = "/aweme/v1/web/comment/list/reply/"
        params = {
            'comment_id': comment_id,
            "cursor": cursor,
            "count": 20,
            "item_type": 0,
            "item_id": aweme_id,
        }

        headers = copy.copy(self.default_headers)
        headers["Referer"] = create_referer_url(aweme_id=aweme_id)

        response = await self.get_request(uri, params, headers)

        # Return simplified reply comments
        replies = []
        for comment_item in response.get("comments", []):
            if not comment_item.get("cid"):
                continue

            user_info = comment_item.get("user", {})
            avatar_info = (user_info.get("avatar_medium", {}) or
                           user_info.get("avatar_300x300", {}) or
                           user_info.get("avatar_168x168", {}) or
                           user_info.get("avatar_thumb", {}) or {})

            reply_data = {
                "comment_id": comment_item.get("cid"),
                "create_time": comment_item.get("create_time"),
                "ip_location": comment_item.get("ip_label", ""),
                "aweme_id": aweme_id,
                "content": comment_item.get("text"),
                "user_id": user_info.get("uid"),
                "sec_uid": user_info.get("sec_uid"),
                "short_user_id": user_info.get("short_id"),
                "user_unique_id": user_info.get("unique_id"),
                "nickname": user_info.get("nickname"),
                "avatar": avatar_info.get("url_list", [""])[0],
                "sub_comment_count": str(comment_item.get("reply_comment_total", 0)),
                "like_count": comment_item.get("digg_count", 0),
                "parent_comment_id": comment_id,
            }
            replies.append(reply_data)

        return replies

    async def fetch_all_video_comments(
            self,
            aweme_id: str,
            fetch_interval: float = 1.0,
            include_replies: bool = False,
            progress_callback: Optional[Callable] = None,
            max_comments: int = 1000,
    ) -> List[Dict]:
        """
        Fetch all comments for a video, including replies if requested
        
        Args:
            aweme_id: Video ID
            fetch_interval: Delay between requests
            include_replies: Whether to fetch comment replies
            progress_callback: Callback for progress updates
            max_comments: Maximum comments to fetch
            
        Returns:
            List of all simplified comments
        """
        all_comments = []
        has_more = True
        cursor = 0

        while has_more and len(all_comments) < max_comments:
            uri = "/aweme/v1/web/comment/list/"
            params = {
                "aweme_id": aweme_id,
                "cursor": cursor,
                "count": 20,
                "item_type": 0
            }

            headers = copy.copy(self.default_headers)
            headers["Referer"] = create_referer_url(aweme_id=aweme_id)

            comments_data = await self.get_request(uri, params, headers)
            has_more = comments_data.get("has_more", False)
            cursor = comments_data.get("cursor", 0)

            # Get simplified comments from this batch
            batch_comments = []
            for comment_item in comments_data.get("comments", []):
                if not comment_item.get("cid"):
                    continue

                user_info = comment_item.get("user", {})
                avatar_info = (user_info.get("avatar_medium", {}) or
                               user_info.get("avatar_300x300", {}) or
                               user_info.get("avatar_168x168", {}) or
                               user_info.get("avatar_thumb", {}) or {})

                comment_data = {
                    "comment_id": comment_item.get("cid"),
                    "create_time": comment_item.get("create_time"),
                    "ip_location": comment_item.get("ip_label", ""),
                    "aweme_id": aweme_id,
                    "content": comment_item.get("text"),
                    "user_id": user_info.get("uid"),
                    "sec_uid": user_info.get("sec_uid"),
                    "short_user_id": user_info.get("short_id"),
                    "user_unique_id": user_info.get("unique_id"),
                    "nickname": user_info.get("nickname"),
                    "avatar": avatar_info.get("url_list", [""])[0],
                    "sub_comment_count": str(comment_item.get("reply_comment_total", 0)),
                    "like_count": comment_item.get("digg_count", 0),
                    "parent_comment_id": comment_item.get("reply_id", "0"),
                }
                batch_comments.append(comment_data)

            if not batch_comments:
                break

            # Limit comments to max_comments
            remaining_slots = max_comments - len(all_comments)
            if remaining_slots <= 0:
                break

            if len(batch_comments) > remaining_slots:
                batch_comments = batch_comments[:remaining_slots]

            all_comments.extend(batch_comments)

            if progress_callback:
                await progress_callback(aweme_id, batch_comments)

            await asyncio.sleep(fetch_interval)

            # Fetch replies if requested
            if include_replies:
                for comment in batch_comments:
                    reply_count = int(comment.get("sub_comment_count", 0))

                    if reply_count > 0:
                        comment_id = comment.get("comment_id")
                        replies = await self.fetch_comment_replies(aweme_id, comment_id, 0)
                        all_comments.extend(replies)

                        if progress_callback:
                            await progress_callback(aweme_id, replies)

                        await asyncio.sleep(fetch_interval)

        logger.info(f"Fetched {len(all_comments)} comments for video {aweme_id}")
        return all_comments

    async def fetch_user_info(self, sec_user_id: str) -> Dict:
        """
        Fetch user profile information
        
        Args:
            sec_user_id: User's security ID
            
        Returns:
            Simplified user information data
        """
        uri = "/aweme/v1/web/user/profile/other/"
        params = {
            "sec_user_id": sec_user_id,
            "publish_video_strategy_type": 2,
            "personal_center_strategy": 1,
        }
        response = await self.get_request(uri, params)

        user_data = response.get("user", {})
        if not user_data:
            return {}

        gender_map = {0: "æœªçŸ¥", 1: "ç”·", 2: "å¥³"}
        avatar_uri = user_data.get("avatar_300x300", {}).get("uri", "")

        return {
            "user_id": user_data.get("uid"),
            "nickname": user_data.get("nickname"),
            "gender": gender_map.get(user_data.get("gender"), "æœªçŸ¥"),
            "avatar": f"https://p3-pc.douyinpic.com/img/{avatar_uri}~c5_300x300.jpeg?from=2956013662" if avatar_uri else "",
            "desc": user_data.get("signature"),
            "ip_location": user_data.get("ip_location"),
            "follows": user_data.get("following_count", 0),
            "fans": user_data.get("max_follower_count", 0),
            "interaction": user_data.get("total_favorited", 0),
            "videos_count": user_data.get("aweme_count", 0),
        }

    async def fetch_user_videos(self, sec_user_id: str, max_cursor: str = "") -> List[Dict]:
        """
        Fetch user's videos with pagination
        
        Args:
            sec_user_id: User's security ID
            max_cursor: Pagination cursor
            
        Returns:
            List of simplified user videos data
        """
        uri = "/aweme/v1/web/aweme/post/"
        params = {
            "sec_user_id": sec_user_id,
            "count": 18,
            "max_cursor": max_cursor,
            "locate_query": "false",
            "publish_video_strategy_type": 2,
        }
        response = await self.get_request(uri, params)

        # Return simplified aweme list
        aweme_list = []
        for aweme_info in response.get("aweme_list", []):
            if not aweme_info.get("aweme_id"):
                continue

            user_info = aweme_info.get("author", {})
            interact_info = aweme_info.get("statistics", {})

            aweme_data = {
                "aweme_id": aweme_info.get("aweme_id"),
                "aweme_type": str(aweme_info.get("aweme_type", "")),
                "title": aweme_info.get("desc", ""),
                "desc": aweme_info.get("desc", ""),
                "create_time": aweme_info.get("create_time"),
                "user_id": user_info.get("uid"),
                "sec_uid": user_info.get("sec_uid"),
                "short_user_id": user_info.get("short_id"),
                "user_unique_id": user_info.get("unique_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar_thumb", {}).get("url_list", [""])[0],
                "liked_count": str(interact_info.get("digg_count", 0)),
                "collected_count": str(interact_info.get("collect_count", 0)),
                "comment_count": str(interact_info.get("comment_count", 0)),
                "share_count": str(interact_info.get("share_count", 0)),
                "ip_location": aweme_info.get("ip_label", ""),
                "aweme_url": f"https://www.douyin.com/video/{aweme_info.get('aweme_id')}",
            }
            aweme_list.append(aweme_data)

        return aweme_list

    async def fetch_all_user_videos(
            self,
            sec_user_id: str,
            progress_callback: Optional[Callable] = None,
            max_videos: int = 1000
    ) -> List[Dict]:
        """
        Fetch all videos from a user
        
        Args:
            sec_user_id: User's security ID
            progress_callback: Callback for progress updates
            max_videos: Maximum videos to fetch
            
        Returns:
            List of all simplified user videos
        """
        all_videos = []
        has_more = True
        max_cursor = ""

        while has_more and len(all_videos) < max_videos:
            uri = "/aweme/v1/web/aweme/post/"
            params = {
                "sec_user_id": sec_user_id,
                "count": 18,
                "max_cursor": max_cursor,
                "locate_query": "false",
                "publish_video_strategy_type": 2,
            }
            videos_data = await self.get_request(uri, params)
            has_more = videos_data.get("has_more", False)
            max_cursor = videos_data.get("max_cursor", "")

            # Get simplified videos from this batch
            batch_videos = []
            for aweme_info in videos_data.get("aweme_list", []):
                if not aweme_info.get("aweme_id"):
                    continue

                user_info = aweme_info.get("author", {})
                interact_info = aweme_info.get("statistics", {})

                aweme_data = {
                    "aweme_id": aweme_info.get("aweme_id"),
                    "aweme_type": str(aweme_info.get("aweme_type", "")),
                    "title": aweme_info.get("desc", ""),
                    "desc": aweme_info.get("desc", ""),
                    "create_time": aweme_info.get("create_time"),
                    "user_id": user_info.get("uid"),
                    "sec_uid": user_info.get("sec_uid"),
                    "short_user_id": user_info.get("short_id"),
                    "user_unique_id": user_info.get("unique_id"),
                    "nickname": user_info.get("nickname"),
                    "avatar": user_info.get("avatar_thumb", {}).get("url_list", [""])[0],
                    "liked_count": str(interact_info.get("digg_count", 0)),
                    "collected_count": str(interact_info.get("collect_count", 0)),
                    "comment_count": str(interact_info.get("comment_count", 0)),
                    "share_count": str(interact_info.get("share_count", 0)),
                    "ip_location": aweme_info.get("ip_label", ""),
                    "aweme_url": f"https://www.douyin.com/video/{aweme_info.get('aweme_id')}",
                }
                batch_videos.append(aweme_data)

            if not batch_videos:
                break

            remaining_slots = max_videos - len(all_videos)
            if remaining_slots <= 0:
                break

            if len(batch_videos) > remaining_slots:
                batch_videos = batch_videos[:remaining_slots]

            all_videos.extend(batch_videos)
            logger.info(f"Fetched {len(batch_videos)} videos for user {sec_user_id}, total: {len(all_videos)}")

            if progress_callback:
                await progress_callback(batch_videos)

            await asyncio.sleep(1.0)  # Rate limiting

        return all_videos

    async def check_login_status(self) -> bool:
        """
        Check if user is logged in to Douyin
        
        Returns:
            True if logged in, False otherwise
        """
        try:
            if not self.target_id:
                return False

            cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)

            # Check localStorage for login status
            result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "window.localStorage.getItem('HasUserLogin')",
                    'returnByValue': True,
                },
                session_id=cdp_session.session_id,
            )

            has_user_login = result.get('result', {}).get('value')
            if has_user_login == "1":
                return True

            # Also check cookies for LOGIN_STATUS
            return self.cookies.get("LOGIN_STATUS") == "1"

        except Exception as e:
            logger.error(f"Failed to check login status: {e}")
            return False

    async def close(self):
        if self.browser_session and self.target_id:
            try:
                logger.info(f"Close target id: {self.target_id}")
                await self.browser_session.cdp_client.send.Target.closeTarget(params={'targetId': self.target_id})
            except Exception as e:
                logger.warning(f"Error closing target {self.target_id}: {e}")




================================================
FILE: vibe_surf/tools/website_api/douyin/douyin.js
================================================
// All the content in this article is only for learning and communication use, not for any other purpose, strictly prohibited for commercial use and illegal use, otherwise all the consequences are irrelevant to the author!
// copy from https://github.com/ShilongLee/Crawler/tree/main/lib/js thanks for ShilongLee
function rc4_encrypt(plaintext, key) {
    var s = [];
    for (var i = 0; i < 256; i++) {
        s[i] = i;
    }
    var j = 0;
    for (var i = 0; i < 256; i++) {
        j = (j + s[i] + key.charCodeAt(i % key.length)) % 256;
        var temp = s[i];
        s[i] = s[j];
        s[j] = temp;
    }

    var i = 0;
    var j = 0;
    var cipher = [];
    for (var k = 0; k < plaintext.length; k++) {
        i = (i + 1) % 256;
        j = (j + s[i]) % 256;
        var temp = s[i];
        s[i] = s[j];
        s[j] = temp;
        var t = (s[i] + s[j]) % 256;
        cipher.push(String.fromCharCode(s[t] ^ plaintext.charCodeAt(k)));
    }
    return cipher.join('');
}

function le(e, r) {
    return (e << (r %= 32) | e >>> 32 - r) >>> 0
}

function de(e) {
    return 0 <= e && e < 16 ? 2043430169 : 16 <= e && e < 64 ? 2055708042 : void console['error']("invalid j for constant Tj")
}

function pe(e, r, t, n) {
    return 0 <= e && e < 16 ? (r ^ t ^ n) >>> 0 : 16 <= e && e < 64 ? (r & t | r & n | t & n) >>> 0 : (console['error']('invalid j for bool function FF'),
        0)
}

function he(e, r, t, n) {
    return 0 <= e && e < 16 ? (r ^ t ^ n) >>> 0 : 16 <= e && e < 64 ? (r & t | ~r & n) >>> 0 : (console['error']('invalid j for bool function GG'),
        0)
}

function reset() {
    this.reg[0] = 1937774191,
        this.reg[1] = 1226093241,
        this.reg[2] = 388252375,
        this.reg[3] = 3666478592,
        this.reg[4] = 2842636476,
        this.reg[5] = 372324522,
        this.reg[6] = 3817729613,
        this.reg[7] = 2969243214,
        this["chunk"] = [],
        this["size"] = 0
}

function write(e) {
    var a = "string" == typeof e ? function (e) {
        n = encodeURIComponent(e)['replace'](/%([0-9A-F]{2})/g, (function (e, r) {
                return String['fromCharCode']("0x" + r)
            }
        ))
            , a = new Array(n['length']);
        return Array['prototype']['forEach']['call'](n, (function (e, r) {
                a[r] = e.charCodeAt(0)
            }
        )),
            a
    }(e) : e;
    this.size += a.length;
    var f = 64 - this['chunk']['length'];
    if (a['length'] < f)
        this['chunk'] = this['chunk'].concat(a);
    else
        for (this['chunk'] = this['chunk'].concat(a.slice(0, f)); this['chunk'].length >= 64;)
            this['_compress'](this['chunk']),
                f < a['length'] ? this['chunk'] = a['slice'](f, Math['min'](f + 64, a['length'])) : this['chunk'] = [],
                f += 64
}

function sum(e, t) {
    e && (this['reset'](),
        this['write'](e)),
        this['_fill']();
    for (var f = 0; f < this.chunk['length']; f += 64)
        this._compress(this['chunk']['slice'](f, f + 64));
    var i = null;
    if (t == 'hex') {
        i = "";
        for (f = 0; f < 8; f++)
            i += se(this['reg'][f]['toString'](16), 8, "0")
    } else
        for (i = new Array(32),
                 f = 0; f < 8; f++) {
            var c = this.reg[f];
            i[4 * f + 3] = (255 & c) >>> 0,
                c >>>= 8,
                i[4 * f + 2] = (255 & c) >>> 0,
                c >>>= 8,
                i[4 * f + 1] = (255 & c) >>> 0,
                c >>>= 8,
                i[4 * f] = (255 & c) >>> 0
        }
    return this['reset'](),
        i
}

function _compress(t) {
    if (t < 64)
        console.error("compress error: not enough data");
    else {
        for (var f = function (e) {
            for (var r = new Array(132), t = 0; t < 16; t++)
                r[t] = e[4 * t] << 24,
                    r[t] |= e[4 * t + 1] << 16,
                    r[t] |= e[4 * t + 2] << 8,
                    r[t] |= e[4 * t + 3],
                    r[t] >>>= 0;
            for (var n = 16; n < 68; n++) {
                var a = r[n - 16] ^ r[n - 9] ^ le(r[n - 3], 15);
                a = a ^ le(a, 15) ^ le(a, 23),
                    r[n] = (a ^ le(r[n - 13], 7) ^ r[n - 6]) >>> 0
            }
            for (n = 0; n < 64; n++)
                r[n + 68] = (r[n] ^ r[n + 4]) >>> 0;
            return r
        }(t), i = this['reg'].slice(0), c = 0; c < 64; c++) {
            var o = le(i[0], 12) + i[4] + le(de(c), c)
                , s = ((o = le(o = (4294967295 & o) >>> 0, 7)) ^ le(i[0], 12)) >>> 0
                , u = pe(c, i[0], i[1], i[2]);
            u = (4294967295 & (u = u + i[3] + s + f[c + 68])) >>> 0;
            var b = he(c, i[4], i[5], i[6]);
            b = (4294967295 & (b = b + i[7] + o + f[c])) >>> 0,
                i[3] = i[2],
                i[2] = le(i[1], 9),
                i[1] = i[0],
                i[0] = u,
                i[7] = i[6],
                i[6] = le(i[5], 19),
                i[5] = i[4],
                i[4] = (b ^ le(b, 9) ^ le(b, 17)) >>> 0
        }
        for (var l = 0; l < 8; l++)
            this['reg'][l] = (this['reg'][l] ^ i[l]) >>> 0
    }
}

function _fill() {
    var a = 8 * this['size']
        , f = this['chunk']['push'](128) % 64;
    for (64 - f < 8 && (f -= 64); f < 56; f++)
        this.chunk['push'](0);
    for (var i = 0; i < 4; i++) {
        var c = Math['floor'](a / 4294967296);
        this['chunk'].push(c >>> 8 * (3 - i) & 255)
    }
    for (i = 0; i < 4; i++)
        this['chunk']['push'](a >>> 8 * (3 - i) & 255)

}

function SM3() {
    this.reg = [];
    this.chunk = [];
    this.size = 0;
    this.reset()
}
SM3.prototype.reset = reset;
SM3.prototype.write = write;
SM3.prototype.sum = sum;
SM3.prototype._compress = _compress;
SM3.prototype._fill = _fill;

function result_encrypt(long_str, num = null) {
    let s_obj = {
        "s0": "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=",
        "s1": "Dkdpgh4ZKsQB80/Mfvw36XI1R25+WUAlEi7NLboqYTOPuzmFjJnryx9HVGcaStCe=",
        "s2": "Dkdpgh4ZKsQB80/Mfvw36XI1R25-WUAlEi7NLboqYTOPuzmFjJnryx9HVGcaStCe=",
        "s3": "ckdp1h4ZKsUB80/Mfvw36XIgR25+WQAlEi7NLboqYTOPuzmFjJnryx9HVGDaStCe",
        "s4": "Dkdpgh2ZmsQB80/MfvV36XI1R45-WUAlEixNLwoqYTOPuzKFjJnry79HbGcaStCe"
    }
    let constant = {
        "0": 16515072,
        "1": 258048,
        "2": 4032,
        "str": s_obj[num],
    }

    let result = "";
    let lound = 0;
    let long_int = get_long_int(lound, long_str);
    for (let i = 0; i < long_str.length / 3 * 4; i++) {
        if (Math.floor(i / 4) !== lound) {
            lound += 1;
            long_int = get_long_int(lound, long_str);
        }
        let key = i % 4;
        switch (key) {
            case 0:
                temp_int = (long_int & constant["0"]) >> 18;
                result += constant["str"].charAt(temp_int);
                break;
            case 1:
                temp_int = (long_int & constant["1"]) >> 12;
                result += constant["str"].charAt(temp_int);
                break;
            case 2:
                temp_int = (long_int & constant["2"]) >> 6;
                result += constant["str"].charAt(temp_int);
                break;
            case 3:
                temp_int = long_int & 63;
                result += constant["str"].charAt(temp_int);
                break;
            default:
                break;
        }
    }
    return result;
}

function get_long_int(round, long_str) {
    round = round * 3;
    return (long_str.charCodeAt(round) << 16) | (long_str.charCodeAt(round + 1) << 8) | (long_str.charCodeAt(round + 2));
}

function gener_random(random, option) {
    return [
        (random & 255 & 170) | option[0] & 85, // 163
        (random & 255 & 85) | option[0] & 170, //87
        (random >> 8 & 255 & 170) | option[1] & 85, //37
        (random >> 8 & 255 & 85) | option[1] & 170, //41
    ]
}

//////////////////////////////////////////////
function generate_rc4_bb_str(url_search_params, user_agent, window_env_str, suffix = "cus", Arguments = [0, 1, 14]) {
    let sm3 = new SM3()
    let start_time = Date.now()
    /**
     * è¿›è¡Œ3æ¬¡åŠ å¯†å¤„ç†
     * 1: url_search_paramsä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
     * 2: å¯¹åç¼€ä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
     * 3: å¯¹uaå¤„ç†ä¹‹åçš„ç»“æœ
     */
        // url_search_paramsä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
    let url_search_params_list = sm3.sum(sm3.sum(url_search_params + suffix))
    // å¯¹åç¼€ä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
    let cus = sm3.sum(sm3.sum(suffix))
    // å¯¹uaå¤„ç†ä¹‹åçš„ç»“æœ
    let ua = sm3.sum(result_encrypt(rc4_encrypt(user_agent, String.fromCharCode.apply(null, [0.00390625, 1, Arguments[2]])), "s3"))
    //
    let end_time = Date.now()
    // b
    let b = {
        8: 3, // å›ºå®š
        10: end_time, //3æ¬¡åŠ å¯†ç»“æŸæ—¶é—´
        15: {
            "aid": 6383,
            "pageId": 6241,
            "boe": false,
            "ddrt": 7,
            "paths": {
                "include": [
                    {},
                    {},
                    {},
                    {},
                    {},
                    {},
                    {}
                ],
                "exclude": []
            },
            "track": {
                "mode": 0,
                "delay": 300,
                "paths": []
            },
            "dump": true,
            "rpU": ""
        },
        16: start_time, //3æ¬¡åŠ å¯†å¼€å§‹æ—¶é—´
        18: 44, //å›ºå®š
        19: [1, 0, 1, 5],
    }

    //3æ¬¡åŠ å¯†å¼€å§‹æ—¶é—´
    b[20] = (b[16] >> 24) & 255
    b[21] = (b[16] >> 16) & 255
    b[22] = (b[16] >> 8) & 255
    b[23] = b[16] & 255
    b[24] = (b[16] / 256 / 256 / 256 / 256) >> 0
    b[25] = (b[16] / 256 / 256 / 256 / 256 / 256) >> 0

    // å‚æ•°Arguments [0, 1, 14, ...]
    // let Arguments = [0, 1, 14]
    b[26] = (Arguments[0] >> 24) & 255
    b[27] = (Arguments[0] >> 16) & 255
    b[28] = (Arguments[0] >> 8) & 255
    b[29] = Arguments[0] & 255

    b[30] = (Arguments[1] / 256) & 255
    b[31] = (Arguments[1] % 256) & 255
    b[32] = (Arguments[1] >> 24) & 255
    b[33] = (Arguments[1] >> 16) & 255

    b[34] = (Arguments[2] >> 24) & 255
    b[35] = (Arguments[2] >> 16) & 255
    b[36] = (Arguments[2] >> 8) & 255
    b[37] = Arguments[2] & 255

    // (url_search_params + "cus") ä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
    /**let url_search_params_list = [
     91, 186,  35,  86, 143, 253,   6,  76,
     34,  21, 167, 148,   7,  42, 192, 219,
     188,  20, 182,  85, 213,  74, 213, 147,
     37, 155,  93, 139,  85, 118, 228, 213
     ]*/
    b[38] = url_search_params_list[21]
    b[39] = url_search_params_list[22]

    // ("cus") å¯¹åç¼€ä¸¤æ¬¡sm3ä¹‹çš„ç»“æœ
    /**
     * let cus = [
     136, 101, 114, 147,  58,  77, 207, 201,
     215, 162, 154,  93, 248,  13, 142, 160,
     105,  73, 215, 241,  83,  58,  51,  43,
     255,  38, 168, 141, 216, 194,  35, 236
     ]*/
    b[40] = cus[21]
    b[41] = cus[22]

    // å¯¹uaå¤„ç†ä¹‹åçš„ç»“æœ
    /**
     * let ua = [
     129, 190,  70, 186,  86, 196, 199,  53,
     99,  38,  29, 209, 243,  17, 157,  69,
     147, 104,  53,  23, 114, 126,  66, 228,
     135,  30, 168, 185, 109, 156, 251,  88
     ]*/
    b[42] = ua[23]
    b[43] = ua[24]

    //3æ¬¡åŠ å¯†ç»“æŸæ—¶é—´
    b[44] = (b[10] >> 24) & 255
    b[45] = (b[10] >> 16) & 255
    b[46] = (b[10] >> 8) & 255
    b[47] = b[10] & 255
    b[48] = b[8]
    b[49] = (b[10] / 256 / 256 / 256 / 256) >> 0
    b[50] = (b[10] / 256 / 256 / 256 / 256 / 256) >> 0


    // objecté…ç½®é¡¹
    b[51] = b[15]['pageId']
    b[52] = (b[15]['pageId'] >> 24) & 255
    b[53] = (b[15]['pageId'] >> 16) & 255
    b[54] = (b[15]['pageId'] >> 8) & 255
    b[55] = b[15]['pageId'] & 255

    b[56] = b[15]['aid']
    b[57] = b[15]['aid'] & 255
    b[58] = (b[15]['aid'] >> 8) & 255
    b[59] = (b[15]['aid'] >> 16) & 255
    b[60] = (b[15]['aid'] >> 24) & 255

    // ä¸­é—´è¿›è¡Œäº†ç¯å¢ƒæ£€æµ‹
    // ä»£ç ç´¢å¼•:  2496 ç´¢å¼•å€¼:  17 ï¼ˆç´¢å¼•64å…³é”®æ¡ä»¶ï¼‰
    // '1536|747|1536|834|0|30|0|0|1536|834|1536|864|1525|747|24|24|Win32'.charCodeAt()å¾—åˆ°65ä½æ•°ç»„
    /**
     * let window_env_list = [49, 53, 51, 54, 124, 55, 52, 55, 124, 49, 53, 51, 54, 124, 56, 51, 52, 124, 48, 124, 51,
     * 48, 124, 48, 124, 48, 124, 49, 53, 51, 54, 124, 56, 51, 52, 124, 49, 53, 51, 54, 124, 56,
     * 54, 52, 124, 49, 53, 50, 53, 124, 55, 52, 55, 124, 50, 52, 124, 50, 52, 124, 87, 105, 110,
     * 51, 50]
     */
    let window_env_list = [];
    for (let index = 0; index < window_env_str.length; index++) {
        window_env_list.push(window_env_str.charCodeAt(index))
    }
    b[64] = window_env_list.length
    b[65] = b[64] & 255
    b[66] = (b[64] >> 8) & 255

    b[69] = [].length
    b[70] = b[69] & 255
    b[71] = (b[69] >> 8) & 255

    b[72] = b[18] ^ b[20] ^ b[26] ^ b[30] ^ b[38] ^ b[40] ^ b[42] ^ b[21] ^ b[27] ^ b[31] ^ b[35] ^ b[39] ^ b[41] ^ b[43] ^ b[22] ^
        b[28] ^ b[32] ^ b[36] ^ b[23] ^ b[29] ^ b[33] ^ b[37] ^ b[44] ^ b[45] ^ b[46] ^ b[47] ^ b[48] ^ b[49] ^ b[50] ^ b[24] ^
        b[25] ^ b[52] ^ b[53] ^ b[54] ^ b[55] ^ b[57] ^ b[58] ^ b[59] ^ b[60] ^ b[65] ^ b[66] ^ b[70] ^ b[71]
    let bb = [
        b[18], b[20], b[52], b[26], b[30], b[34], b[58], b[38], b[40], b[53], b[42], b[21], b[27], b[54], b[55], b[31],
        b[35], b[57], b[39], b[41], b[43], b[22], b[28], b[32], b[60], b[36], b[23], b[29], b[33], b[37], b[44], b[45],
        b[59], b[46], b[47], b[48], b[49], b[50], b[24], b[25], b[65], b[66], b[70], b[71]
    ]
    bb = bb.concat(window_env_list).concat(b[72])
    return rc4_encrypt(String.fromCharCode.apply(null, bb), String.fromCharCode.apply(null, [121]));
}

function generate_random_str() {
    let random_str_list = []
    random_str_list = random_str_list.concat(gener_random(Math.random() * 10000, [3, 45]))
    random_str_list = random_str_list.concat(gener_random(Math.random() * 10000, [1, 0]))
    random_str_list = random_str_list.concat(gener_random(Math.random() * 10000, [1, 5]))
    return String.fromCharCode.apply(null, random_str_list)
}

function sign(url_search_params, user_agent, arguments) {
    /**
     * url_search_paramsï¼š"device_platform=webapp&aid=6383&channel=channel_pc_web&update_version_code=170400&pc_client_type=1&version_code=170400&version_name=17.4.0&cookie_enabled=true&screen_width=1536&screen_height=864&browser_language=zh-CN&browser_platform=Win32&browser_name=Chrome&browser_version=123.0.0.0&browser_online=true&engine_name=Blink&engine_version=123.0.0.0&os_name=Windows&os_version=10&cpu_core_num=16&device_memory=8&platform=PC&downlink=10&effective_type=4g&round_trip_time=50&webid=7362810250930783783&msToken=VkDUvz1y24CppXSl80iFPr6ez-3FiizcwD7fI1OqBt6IICq9RWG7nCvxKb8IVi55mFd-wnqoNkXGnxHrikQb4PuKob5Q-YhDp5Um215JzlBszkUyiEvR"
     * user_agentï¼š"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
     */
    let result_str = generate_random_str() + generate_rc4_bb_str(
        url_search_params,
        user_agent,
        "1536|747|1536|834|0|30|0|0|1536|834|1536|864|1525|747|24|24|Win32",
        "cus",
        arguments
    );
    return result_encrypt(result_str, "s4") + "=";
}

function sign_datail(params, userAgent) {
    return sign(params, userAgent, [0, 1, 14])
}

function sign_reply(params, userAgent) {
    return sign(params, userAgent, [0, 1, 8])
}


================================================
FILE: vibe_surf/tools/website_api/douyin/helpers.py
================================================
import pdb
import random
import time
import json
import urllib.parse
from typing import Dict, List, Tuple, Optional
from enum import Enum


class SearchChannelType(Enum):
    """Search channel type constants"""
    GENERAL = "aweme_general"  # General content
    VIDEO = "aweme_video_web"  # Video content
    USER = "aweme_user_web"  # User content
    LIVE = "aweme_live"  # Live content


class SearchSortType(Enum):
    """Search sort type constants"""
    GENERAL = 0  # General sorting
    MOST_LIKED = 1  # Most liked
    LATEST = 2  # Latest published


class PublishTimeType(Enum):
    """Publish time type constants"""
    UNLIMITED = 0  # Unlimited
    ONE_DAY = 1  # Within one day
    ONE_WEEK = 7  # Within one week
    SIX_MONTHS = 180  # Within six months


def generate_web_id() -> str:
    """
    Generate random webid for Douyin requests
    
    Returns:
        Random webid string
    """
    def generate_part(t):
        if t is not None:
            return str(t ^ (int(16 * random.random()) >> (t // 4)))
        else:
            return ''.join([
                str(int(1e7)), '-', str(int(1e3)), '-', 
                str(int(4e3)), '-', str(int(8e3)), '-', str(int(1e11))
            ])

    web_id = ''.join(
        generate_part(int(x)) if x in '018' else x for x in generate_part(None)
    )
    return web_id.replace('-', '')[:19]


def generate_trace_id() -> str:
    """Generate a random trace ID for requests"""
    chars = "abcdef0123456789"
    return ''.join(random.choices(chars, k=16))


def create_session_id() -> str:
    """Create a unique session identifier"""
    timestamp = int(time.time() * 1000) << 64
    rand_num = random.randint(0, 2147483646)
    return encode_base36(timestamp + rand_num)


def encode_base36(number: int, alphabet: str = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ') -> str:
    """Convert integer to base36 string"""
    if not isinstance(number, int):
        raise TypeError('Input must be an integer')
    
    if number == 0:
        return alphabet[0]
    
    result = ''
    sign = ''
    
    if number < 0:
        sign = '-'
        number = -number
    
    while number:
        number, remainder = divmod(number, len(alphabet))
        result = alphabet[remainder] + result
    
    return sign + result


def create_common_params() -> Dict[str, any]:
    """
    Create common parameters for Douyin API requests
    
    Returns:
        Dictionary of common parameters
    """
    return {
        "device_platform": "webapp",
        "aid": "6383",
        "channel": "channel_pc_web",
        "version_code": "190600",
        "version_name": "19.6.0",
        "update_version_code": "170400",
        "pc_client_type": "1",
        "cookie_enabled": "true",
        "browser_language": "en-US",
        "browser_platform": "MacIntel",
        "browser_name": "Chrome",
        "browser_version": "125.0.0.0",
        "browser_online": "true",
        "engine_name": "Blink",
        "os_name": "Mac OS",
        "os_version": "10.15.7",
        "cpu_core_num": "8",
        "device_memory": "8",
        "engine_version": "109.0",
        "platform": "PC",
        "screen_width": "1920",
        "screen_height": "1080",
        "effective_type": "4g",
        "round_trip_time": "50",
        "webid": generate_web_id(),
    }


def extract_cookies_from_browser(web_cookies: List[Dict]) -> Tuple[str, Dict[str, str]]:
    """Extract and format cookies from browser, filtering only Douyin related cookies"""
    cookie_dict = {}
    cookie_parts = []
    
    # Douyin domain patterns to filter
    douyin_domains = [
        '.douyin.com',
        # 'www.douyin.com',
        # 'sso.douyin.com'
    ]
    
    for cookie in web_cookies:
        if 'name' in cookie and 'value' in cookie and 'domain' in cookie:
            domain = cookie['domain']
            
            # Filter only Douyin related cookies
            if any(douyin_domain in domain for douyin_domain in douyin_domains):
                name = cookie['name']
                value = cookie['value']
                cookie_dict[name] = value
                cookie_parts.append(f"{name}={value}")
    cookie_string = "; ".join(cookie_parts)
    return cookie_string, cookie_dict


def create_referer_url(keyword: str = "", aweme_id: str = "") -> str:
    """
    Create appropriate referer URL for Douyin requests
    
    Args:
        keyword: Search keyword if applicable
        aweme_id: Aweme ID if applicable
        
    Returns:
        Referer URL string
    """
    if keyword:
        return f"https://www.douyin.com/search/{urllib.parse.quote(keyword)}"
    elif aweme_id:
        return f"https://www.douyin.com/video/{aweme_id}"
    else:
        return "https://www.douyin.com/"


def extract_aweme_media_urls(aweme_data: Dict) -> Dict[str, List[str]]:
    """
    Extract media URLs from aweme data
    
    Args:
        aweme_data: Aweme item data
        
    Returns:
        Dictionary containing image and video URLs
    """
    result = {
        "images": [],
        "videos": [],
        "cover": ""
    }
    
    try:
        # Extract images if available
        if "images" in aweme_data:
            for img in aweme_data["images"]:
                if "url_list" in img and img["url_list"]:
                    result["images"].append(img["url_list"][0])
        
        # Extract video URL
        if "video" in aweme_data and "play_addr" in aweme_data["video"]:
            play_addr = aweme_data["video"]["play_addr"]
            if "url_list" in play_addr and play_addr["url_list"]:
                result["videos"].append(play_addr["url_list"][0])
        
        # Extract cover image
        if "video" in aweme_data and "cover" in aweme_data["video"]:
            cover_data = aweme_data["video"]["cover"]
            if "url_list" in cover_data and cover_data["url_list"]:
                result["cover"] = cover_data["url_list"][0]
                
    except (KeyError, TypeError, IndexError) as e:
        pass  # Ignore extraction errors
    
    return result


class DouyinError(Exception):
    """Base exception for Douyin API errors"""
    pass


class NetworkError(DouyinError):
    """Network connection error"""
    pass


class DataExtractionError(DouyinError):
    """Data extraction error"""
    pass


class AuthenticationError(DouyinError):
    """Authentication error"""
    pass


class RateLimitError(DouyinError):
    """Rate limit exceeded error"""
    pass


class VerificationError(DouyinError):
    """Account verification required error"""
    pass


================================================
FILE: vibe_surf/tools/website_api/weibo/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/tools/website_api/weibo/client.py
================================================
import asyncio
import json
import pdb
import re
import copy
import time
import urllib.parse
from typing import Dict, List, Optional, Callable, Union, Any
import httpx
from tenacity import retry, stop_after_attempt, wait_fixed
from urllib.parse import parse_qs, unquote, urlencode

from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.logger import get_logger

from .helpers import (
    SearchType, TrendingType, TrendingConstants,
    create_container_id, extract_cookies_from_browser,
    filter_search_result_card, extract_container_params,
    build_image_proxy_url, extract_render_data, process_weibo_text,
    validate_weibo_data, sanitize_filename,
    extract_redirect_url_from_html, decode_chinese_html,
    WeiboError, NetworkError, DataExtractionError,
    AuthenticationError, RateLimitError, ContentNotFoundError,
    get_mobile_user_agent
)

logger = get_logger(__name__)


class WeiboApiClient:
    """
    Weibo API client with integrated browser session management.
    This client handles API communication through browser session for authentication.
    """

    def __init__(self, browser_session: AgentBrowserSession, timeout: int = 60, proxy: Optional[str] = None):
        """
        Initialize the Weibo API client
        
        Args:
            browser_session: Browser session for authentication
            timeout: Request timeout in seconds
            proxy: Proxy URL if needed
        """
        self.browser_session = browser_session
        self.target_id = None
        self.proxy = proxy
        self.timeout = timeout
        self._api_base = "https://m.weibo.cn"
        self._web_base = "https://www.weibo.com"
        self._image_proxy_host = "https://i1.wp.com/"

        # Default headers for mobile Weibo
        self.default_headers = {
            "User-Agent": get_mobile_user_agent(),
            "Origin": "https://m.weibo.cn",
            "Referer": "https://m.weibo.cn",
            "Content-Type": "application/json;charset=UTF-8",
        }
        self.cookies = {}

    async def setup(self, target_id: Optional[str] = None):
        """
        Setup Weibo client by navigating to the site and extracting cookies
        
        Args:
            target_id: Specific browser target ID to use
            
        Raises:
            AuthenticationError: If setup fails or user is not logged in
        """
        try:
            if self.target_id and self.cookies:
                logger.info("Already setup. Return!")
                return
            if target_id:
                self.target_id = target_id
            else:
                # Navigate to mobile version for better API compatibility
                self.target_id = await self.browser_session.navigate_to_url(
                    "https://weibo.com/", new_tab=True
                )
                await asyncio.sleep(3)  # Wait for page load

            # Extract cookies from browser
            cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)
            result = await asyncio.wait_for(
                cdp_session.cdp_client.send.Storage.getCookies(session_id=cdp_session.session_id),
                timeout=8.0
            )
            web_cookies = result.get('cookies', [])

            cookie_str, cookie_dict = extract_cookies_from_browser(web_cookies)
            self.default_headers["Cookie"] = cookie_str
            self.cookies = cookie_dict

            user_agent_result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "navigator.userAgent",
                    'returnByValue': True,
                    'awaitPromise': True
                },
                session_id=cdp_session.session_id,
            )
            user_agent = user_agent_result.get('result', {}).get('value')
            if user_agent:
                self.default_headers["User-Agent"] = user_agent

            # Check if user is logged in
            # is_logged_in = await self.pong()
            #
            # if not is_logged_in:
            #     logger.warning("User is not logged in to Weibo, redirecting to login page")
            #
            #     # Navigate to Weibo SSO login page
            #     weibo_sso_login_url = "https://passport.weibo.com/sso/signin?entry=miniblog&source=miniblog"
            #     await self.browser_session.navigate_to_url(weibo_sso_login_url, new_tab=True)
            #
            #     # Raise authentication error to inform user they need to login
            #     raise AuthenticationError(
            #         "User is not logged in to Weibo. Please complete login process and try again.")

            logger.info("Weibo client setup completed successfully")

        except Exception as e:
            logger.error(f"Failed to setup Weibo client: {e}")
            raise AuthenticationError(f"Setup failed: {e}")

    async def pong(self) -> bool:
        """Check if login state is valid using multiple methods"""
        try:
            logger.info("Testing Weibo login status...")

            # Method 1: Check essential login cookies
            login_cookies = ['SUB', 'SUBP', 'ALF', 'SSOLoginState']
            has_essential_cookies = any(
                cookie_name in self.cookies and self.cookies[cookie_name]
                for cookie_name in login_cookies
            )
            if has_essential_cookies:
                logger.info("Weibo login status: Valid (found essential cookies)")
                return True

            # Method 2: Try to access user info API
            try:
                uri = "/api/config"
                response_data = await self._make_request("GET", f"{self._api_base}{uri}")

                if isinstance(response_data, dict) and response_data.get("login"):
                    logger.info("Weibo login status: Valid (API check passed)")
                    return True
            except Exception as api_error:
                logger.debug(f"API config check failed: {api_error}")

            # Method 3: Check browser localStorage for login indicators
            try:
                cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)
                js_check = """
                (function() {
                    try {
                        // Check various login indicators
                        var hasLoginCookie = document.cookie.includes('SUB=') || document.cookie.includes('SUBP=');
                        var hasLoginStorage = localStorage.getItem('login_status') === '1' ||
                                            localStorage.getItem('isLogin') === 'true' ||
                                            localStorage.getItem('weiboLoginStatus') === '1';
                        
                        // Check if there's user info in the page
                        var hasUserInfo = window.__INITIAL_STATE__ &&
                                         window.__INITIAL_STATE__.user &&
                                         window.__INITIAL_STATE__.user.id;
                        
                        return hasLoginCookie || hasLoginStorage || hasUserInfo;
                    } catch(e) {
                        return false;
                    }
                })()
                """

                result = await cdp_session.cdp_client.send.Runtime.evaluate(
                    params={
                        'expression': js_check,
                        'returnByValue': True,
                    },
                    session_id=cdp_session.session_id,
                )

                browser_login_check = result.get('result', {}).get('value', False)
                if browser_login_check:
                    logger.info("Weibo login status: Valid (browser check passed)")
                    return True

            except Exception as browser_error:
                logger.debug(f"Browser login check failed: {browser_error}")

            logger.warning("Weibo login status: No valid login indicators found")
            return False

        except Exception as e:
            logger.error(f"Failed to check Weibo login status: {e}")
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
    async def _make_request(self, method: str, url: str, **kwargs):
        """
        Make HTTP request with error handling and retry logic
        
        Args:
            method: HTTP method
            url: Request URL
            **kwargs: Additional request parameters
            
        Returns:
            Response data
        """
        raw_response = kwargs.pop("raw_response", False)

        async with httpx.AsyncClient(proxy=self.proxy, timeout=self.timeout) as client:
            response = await client.request(method, url, **kwargs)
        # Handle common error status codes
        if response.status_code == 403:
            raise AuthenticationError("Access forbidden - may need login or verification")
        elif response.status_code == 429:
            raise RateLimitError("Rate limit exceeded")
        elif response.status_code == 404:
            raise ContentNotFoundError("Content not found")
        elif response.status_code >= 500:
            raise NetworkError(f"Server error: {response.status_code}")

        if raw_response:
            return response

        try:
            data = response.json()

            # Check Weibo API response format
            if isinstance(data, dict):
                ok_code = data.get("ok")
                if ok_code == 0:  # Weibo error response
                    error_msg = data.get("msg", "Response error")
                    logger.error(f"Weibo API error: {error_msg}")
                    raise DataExtractionError(error_msg)
                elif ok_code == 1:  # Success response
                    return data.get("data", {})
                elif ok_code is None:  # Some endpoints don't return 'ok' field
                    return data
                else:  # Unknown error
                    error_msg = data.get("msg", "Unknown error")
                    logger.error(f"Weibo API unknown error: {error_msg}")
                    raise DataExtractionError(error_msg)

            return data

        except json.JSONDecodeError:
            raise DataExtractionError(f"Invalid JSON response: {response.text[:200]}")

    async def _get_request(self, endpoint: str, params: Optional[Dict] = None, headers: Optional[Dict] = None,
                           **kwargs) -> Dict:
        """Make GET request with proper headers and parameters"""
        final_endpoint = endpoint
        if params:
            final_endpoint = f"{endpoint}?{urllib.parse.urlencode(params)}"

        request_headers = headers or self.default_headers

        return await self._make_request(
            "GET", f"{self._api_base}{final_endpoint}",
            headers=request_headers,
            **kwargs
        )

    async def _post_request(self, endpoint: str, data: Dict, headers: Optional[Dict] = None) -> Dict:
        """Make POST request with proper headers and data"""
        request_headers = headers or self.default_headers
        json_payload = json.dumps(data, separators=(",", ":"), ensure_ascii=False)

        return await self._make_request(
            "POST", f"{self._api_base}{endpoint}",
            data=json_payload, headers=request_headers
        )

    async def search_posts_by_keyword(
            self,
            keyword: str,
            page: int = 1,
            search_type: SearchType = SearchType.DEFAULT,
    ) -> List[Dict]:
        """
        Search Weibo posts by keyword
        
        Args:
            keyword: Search keyword
            page: Page number (starting from 1)
            search_type: Search type filter
            
        Returns:
            List of simplified post information
        """
        endpoint = "/api/container/getIndex"
        container_id = create_container_id(search_type, keyword)

        cards = []
        posts = []
        for page_num in range(page):
            params = {
                "containerid": container_id,
                "page_type": "searchall",
                "page": page_num,
            }

            raw_response = await self._get_request(endpoint, params)
            cards.extend(raw_response.get("cards", []))

        for card in cards:
            mblog = card.get("mblog", {})
            if not mblog.get("id"):
                continue

            user_info = mblog.get("user", {})
            clean_text = re.sub(r"<.*?>", "", mblog.get("text", ""))

            post = {
                "note_id": mblog.get("id"),
                "content": clean_text,
                "created_at": mblog.get("created_at"),
                "liked_count": str(mblog.get("attitudes_count", 0)),
                "comments_count": str(mblog.get("comments_count", 0)),
                "shared_count": str(mblog.get("reposts_count", 0)),
                "ip_location": mblog.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                "note_url": f"https://m.weibo.cn/detail/{mblog.get('id')}",
                "user_id": str(user_info.get("id", "")),
                "nickname": user_info.get("screen_name", ""),
                "gender": user_info.get("gender", ""),
                "profile_url": user_info.get("profile_url", ""),
                "avatar": user_info.get("profile_image_url", ""),
            }
            posts.append(post)

        return posts

    async def get_post_detail(self, mid: str) -> Optional[Dict]:
        """
        Get detailed post information by mid ID
        
        Args:
            mid: Weibo post ID
            
        Returns:
            Simplified post detail information
        """
        url = f"{self._api_base}/detail/{mid}"

        response = await self._make_request(
            "GET", url, headers=self.default_headers, raw_response=True,
        )
        # Extract render data from HTML
        render_data = extract_render_data(response.text)
        if render_data:
            note_detail = render_data.get("status")
            if note_detail:
                user_info = note_detail.get("user", {})
                clean_text = re.sub(r"<.*?>", "", note_detail.get("text", ""))

                return {
                    "note_id": note_detail.get("id"),
                    "content": clean_text,
                    "created_at": note_detail.get("created_at"),
                    "liked_count": str(note_detail.get("attitudes_count", 0)),
                    "comments_count": str(note_detail.get("comments_count", 0)),
                    "shared_count": str(note_detail.get("reposts_count", 0)),
                    "ip_location": note_detail.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                    "note_url": f"https://m.weibo.cn/detail/{note_detail.get('id')}",
                    "user_id": str(user_info.get("id", "")),
                    "nickname": user_info.get("screen_name", ""),
                    "gender": user_info.get("gender", ""),
                    "profile_url": user_info.get("profile_url", ""),
                    "avatar": user_info.get("profile_image_url", ""),
                }

        logger.warning(f"Could not extract render data for post {mid}")
        return None

    async def get_post_comments(
            self,
            mid: str,
            max_id: int = 0,
            max_id_type: int = 0
    ) -> List[Dict]:
        """
        Get comments for a Weibo post
        
        Args:
            mid: Weibo post ID
            max_id: Pagination parameter
            max_id_type: Pagination type parameter
            
        Returns:
            List of simplified comment information
        """
        endpoint = "/comments/hotflow"

        params = {
            "id": mid,
            "mid": mid,
            "max_id_type": str(max_id_type),
        }

        if max_id > 0:
            params["max_id"] = str(max_id)

        # Set referer for comment requests
        headers = copy.deepcopy(self.default_headers)
        headers["Referer"] = f"https://m.weibo.cn/detail/{mid}"

        raw_response = await self._get_request(endpoint, params, headers)

        # Return simplified comments
        comments = []
        for comment in raw_response.get("data", []):
            if not comment.get("id"):
                continue

            user_info = comment.get("user", {})
            clean_text = re.sub(r"<.*?>", "", comment.get("text", ""))

            comment_data = {
                "comment_id": str(comment.get("id")),
                "content": clean_text,
                "created_at": comment.get("created_at"),
                "comment_like_count": str(comment.get("like_count", 0)),
                "sub_comment_count": str(comment.get("total_number", 0)),
                "ip_location": comment.get("source", "").replace("æ¥è‡ª", ""),
                "parent_comment_id": comment.get("rootid", ""),
                "user_id": str(user_info.get("id", "")),
                "nickname": user_info.get("screen_name", ""),
                "gender": user_info.get("gender", ""),
                "profile_url": user_info.get("profile_url", ""),
                "avatar": user_info.get("profile_image_url", ""),
            }
            comments.append(comment_data)

        return comments

    async def get_all_post_comments(
            self,
            mid: str,
            fetch_interval: float = 1.0,
            include_sub_comments: bool = False,
            progress_callback: Optional[Callable] = None,
            max_comments: int = 1000,
    ) -> List[Dict]:
        """
        Fetch all comments for a post including sub-comments
        
        Args:
            mid: Weibo post ID
            fetch_interval: Interval between requests in seconds
            include_sub_comments: Whether to include sub-comments
            progress_callback: Callback function for progress updates
            max_comments: Maximum comments to fetch
            
        Returns:
            List of all simplified comments
        """
        all_comments = []
        is_end = False
        max_id = -1
        max_id_type = 0

        while not is_end and len(all_comments) < max_comments:
            # Get raw response to access pagination info
            endpoint = "/comments/hotflow"

            params = {
                "id": mid,
                "mid": mid,
                "max_id_type": str(max_id_type),
            }

            if max_id > 0:
                params["max_id"] = str(max_id)

            # Set referer for comment requests
            headers = copy.deepcopy(self.default_headers)
            headers["Referer"] = f"https://m.weibo.cn/detail/{mid}"

            raw_response = await self._get_request(endpoint, params, headers)

            # Extract pagination info from raw response
            max_id = raw_response.get("max_id", 0)
            max_id_type = raw_response.get("max_id_type", 0)
            is_end = max_id == 0

            # Transform to simplified comments
            batch_comments = []
            for comment in raw_response.get("data", []):
                if not comment.get("id"):
                    continue

                user_info = comment.get("user", {})
                clean_text = re.sub(r"<.*?>", "", comment.get("text", ""))

                comment_data = {
                    "comment_id": str(comment.get("id")),
                    "content": clean_text,
                    "created_at": comment.get("created_at"),
                    "comment_like_count": str(comment.get("like_count", 0)),
                    "sub_comment_count": str(comment.get("total_number", 0)),
                    "ip_location": comment.get("source", "").replace("æ¥è‡ª", ""),
                    "parent_comment_id": comment.get("rootid", ""),
                    "user_id": str(user_info.get("id", "")),
                    "nickname": user_info.get("screen_name", ""),
                    "gender": user_info.get("gender", ""),
                    "profile_url": user_info.get("profile_url", ""),
                    "avatar": user_info.get("profile_image_url", ""),
                }
                batch_comments.append(comment_data)

            # Limit comments if approaching max
            remaining_slots = max_comments - len(all_comments)
            if len(batch_comments) > remaining_slots:
                batch_comments = batch_comments[:remaining_slots]

            if progress_callback:
                await progress_callback(mid, batch_comments)

            await asyncio.sleep(fetch_interval)
            all_comments.extend(batch_comments)

        logger.info(f"Fetched {len(all_comments)} comments for post {mid}")
        return all_comments

    async def get_user_info(self, user_id: str) -> Optional[Dict]:
        """
        Get user profile information
        
        Args:
            user_id: User ID
            
        Returns:
            Simplified user profile information
        """
        endpoint = "/api/container/getIndex"

        # Set proper headers for user info request
        headers = copy.deepcopy(self.default_headers)
        headers["Referer"] = f"{self._api_base}/u/{user_id}"

        # Use standard user profile container ID
        params = {
            "type": "uid",
            "value": user_id,
            "containerid": f"100505{user_id}",  # Standard user profile container
        }

        try:
            user_data = await self._get_request(endpoint, params, headers)
            # Extract user info from cards if available
            user_info = user_data.get('userInfo', {})
            user_info["user_id"] = user_info.get("id", user_id)
            return user_info

        except Exception as e:
            logger.error(f"Failed to get user info for {user_id}: {e}")
            return None

    async def get_user_posts(
            self,
            user_id: str,
            since_id: str = "0",
    ) -> Optional[Dict]:
        """
        Get posts by user
        
        Args:
            user_id: User ID
            since_id: Pagination parameter (last post ID from previous page)
            
        Returns:
            Simplified user posts data
        """
        endpoint = "/api/container/getIndex"

        # response = await self._get_request(f"/u/{user_id}", raw_response=True)
        # m_weibocn_params = response.cookies.get("M_WEIBOCN_PARAMS")
        # m_weibocn_params_dict = parse_qs(unquote(m_weibocn_params))
        # containerid = m_weibocn_params_dict['fid'][0]

        params = {
            "jumpfrom": "weibocom",
            "type": "uid",
            "value": user_id,
            "containerid": f"100505{user_id}",
            "since_id": since_id,
        }

        response = await self._get_request(endpoint, params)
        containerid = f"100505{user_id}"
        if response.get("tabsInfo"):
            tabs: List[Dict] = response.get("tabsInfo", {}).get("tabs", [])
            for tab in tabs:
                if tab.get("tabKey") == "weibo":
                    containerid = tab.get("containerid")
                    break
        params = {
            "jumpfrom": "weibocom",
            "type": "uid",
            "value": user_id,
            "containerid": containerid,
            "since_id": since_id,
        }

        response = await self._get_request(endpoint, params)

        # Transform to simplified posts
        posts = []
        cards = response.get("cards", [])
        for card in cards:
            if card.get("card_type") == 9:  # Weibo post card type
                mblog = card.get("mblog", {})
                if not mblog.get("id"):
                    continue

                user_info = mblog.get("user", {})
                clean_text = re.sub(r"<.*?>", "", mblog.get("text", ""))

                post = {
                    "note_id": mblog.get("id"),
                    "content": clean_text,
                    "created_at": mblog.get("created_at"),
                    "liked_count": str(mblog.get("attitudes_count", 0)),
                    "comments_count": str(mblog.get("comments_count", 0)),
                    "shared_count": str(mblog.get("reposts_count", 0)),
                    "ip_location": mblog.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                    "note_url": f"https://m.weibo.cn/detail/{mblog.get('id')}",
                    "user_id": str(user_info.get("id", "")),
                    "nickname": user_info.get("screen_name", ""),
                    "gender": user_info.get("gender", ""),
                    "profile_url": user_info.get("profile_url", ""),
                    "avatar": user_info.get("profile_image_url", ""),
                }
                posts.append(post)

        return {
            "posts": posts,
            "pagination": {
                "since_id": response.get("cardlistInfo", {}).get("since_id", ""),
                "total": response.get("cardlistInfo", {}).get("total", 0)
            }
        }

    async def get_all_user_posts(
            self,
            user_id: str,
            fetch_interval: float = 1.0,
            progress_callback: Optional[Callable] = None,
            max_posts: int = 1000,
    ) -> List[Dict]:
        """
        Fetch all posts by a user
        
        Args:
            user_id: User ID
            fetch_interval: Interval between requests in seconds
            progress_callback: Callback function for progress updates
            max_posts: Maximum posts to fetch
            
        Returns:
            List of all simplified user posts
        """
        all_posts = []
        has_more = True
        since_id = ""
        crawler_total_count = 0

        while has_more and len(all_posts) < max_posts:
            # Get raw response to access pagination info and then transform
            endpoint = "/api/container/getIndex"

            params = {
                "jumpfrom": "weibocom",
                "type": "uid",
                "value": user_id,
                "containerid": f"100505{user_id}",
                "since_id": since_id,
            }

            raw_posts_data = await self._get_request(endpoint, params)

            if not raw_posts_data:
                logger.error(f"User {user_id} may be restricted or data unavailable")
                break

            # Extract pagination info from raw response
            since_id = raw_posts_data.get("cardlistInfo", {}).get("since_id", "0")
            if "cards" not in raw_posts_data:
                logger.info(f"No posts found in response for user {user_id}")
                break

            # Transform to simplified posts
            posts = []
            cards = raw_posts_data.get("cards", [])
            for card in cards:
                if card.get("card_type") == 9:  # Weibo post card type
                    mblog = card.get("mblog", {})
                    if not mblog.get("id"):
                        continue

                    user_info = mblog.get("user", {})
                    clean_text = re.sub(r"<.*?>", "", mblog.get("text", ""))

                    post = {
                        "note_id": mblog.get("id"),
                        "content": clean_text,
                        "created_at": mblog.get("created_at"),
                        "liked_count": str(mblog.get("attitudes_count", 0)),
                        "comments_count": str(mblog.get("comments_count", 0)),
                        "shared_count": str(mblog.get("reposts_count", 0)),
                        "ip_location": mblog.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                        "note_url": f"https://m.weibo.cn/detail/{mblog.get('id')}",
                        "user_id": str(user_info.get("id", "")),
                        "nickname": user_info.get("screen_name", ""),
                        "gender": user_info.get("gender", ""),
                        "profile_url": user_info.get("profile_url", ""),
                        "avatar": user_info.get("profile_image_url", ""),
                    }
                    posts.append(post)

            logger.info(f"Fetched {len(posts)} posts for user {user_id}")

            remaining_slots = max_posts - len(all_posts)
            if remaining_slots <= 0:
                break

            posts_to_add = posts[:remaining_slots]

            if progress_callback:
                await progress_callback(posts_to_add)

            all_posts.extend(posts_to_add)
            await asyncio.sleep(fetch_interval)

            crawler_total_count += 10
            total_available = raw_posts_data.get("cardlistInfo", {}).get("total", 0)
            has_more = total_available > crawler_total_count and since_id != "0"

        logger.info(f"Fetched total {len(all_posts)} posts for user {user_id}")
        return all_posts

    async def get_trending_posts(self) -> List[Dict]:
        """
        Get Weibo trending posts (çƒ­æœæ¦œ)
        
        Returns:
            List of simplified trending post information
        """
        endpoint = "/api/feed/trendtop"
        params = {
            "containerid": TrendingConstants.TRENDING_CONTAINER_ID
        }

        raw_response = await self._get_request(endpoint, params)

        # Transform to simplified posts
        posts = []
        cards = raw_response.get("statuses", [])
        for mblog in cards:
            if not mblog.get("id"):
                continue

            user_info = mblog.get("user", {})
            clean_text = re.sub(r"<.*?>", "", mblog.get("text", ""))

            post = {
                "note_id": mblog.get("id"),
                "content": clean_text,
                "created_at": mblog.get("created_at"),
                "liked_count": str(mblog.get("attitudes_count", 0)),
                "comments_count": str(mblog.get("comments_count", 0)),
                "shared_count": str(mblog.get("reposts_count", 0)),
                "ip_location": mblog.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                "note_url": f"https://m.weibo.cn/detail/{mblog.get('id')}",
                "user_id": str(user_info.get("id", "")),
                "nickname": user_info.get("screen_name", ""),
                "gender": user_info.get("gender", ""),
                "profile_url": user_info.get("profile_url", ""),
                "avatar": user_info.get("profile_image_url", ""),
            }
            posts.append(post)

        return posts

    async def get_hot_posts(self) -> List[Dict]:
        """
        Get Weibo hot posts (çƒ­é—¨æ¨è)
        
        Returns:
            List of simplified hot post information
        """
        endpoint = "/api/container/getIndex"
        params = {
            "containerid": TrendingConstants.HOT_POSTS_CONTAINER_ID,
            "openApp": TrendingConstants.OPEN_APP
        }

        raw_response = await self._get_request(endpoint, params)

        # Transform to simplified posts (same structure as search results)
        posts = []
        cards = raw_response.get("cards", [])
        for card in cards:
            if card.get("card_type") == 9:  # Weibo post card type
                mblog = card.get("mblog", {})
                if not mblog.get("id"):
                    continue

                user_info = mblog.get("user", {})
                clean_text = re.sub(r"<.*?>", "", mblog.get("text", ""))

                post = {
                    "note_id": mblog.get("id"),
                    "content": clean_text,
                    "created_at": mblog.get("created_at"),
                    "liked_count": str(mblog.get("attitudes_count", 0)),
                    "comments_count": str(mblog.get("comments_count", 0)),
                    "shared_count": str(mblog.get("reposts_count", 0)),
                    "ip_location": mblog.get("region_name", "").replace("å‘å¸ƒäº ", ""),
                    "note_url": f"https://m.weibo.cn/detail/{mblog.get('id')}",
                    "user_id": str(user_info.get("id", "")),
                    "nickname": user_info.get("screen_name", ""),
                    "gender": user_info.get("gender", ""),
                    "profile_url": user_info.get("profile_url", ""),
                    "avatar": user_info.get("profile_image_url", ""),
                }
                posts.append(post)

        return posts

    async def close(self):
        if self.browser_session and self.target_id:
            try:
                logger.info(f"Close target id: {self.target_id}")
                await self.browser_session.cdp_client.send.Target.closeTarget(params={'targetId': self.target_id})
            except Exception as e:
                logger.warning(f"Error closing target {self.target_id}: {e}")




================================================
FILE: vibe_surf/tools/website_api/weibo/helpers.py
================================================
import pdb
import random
import time
import re
import json
import html
from typing import Dict, List, Tuple, Optional
from enum import Enum
from urllib.parse import parse_qs, unquote


class SearchType(Enum):
    """Search type enumeration for Weibo"""
    DEFAULT = "1"
    REAL_TIME = "61"
    POPULAR = "60"
    VIDEO = "64"


class TrendingType(Enum):
    """Trending type enumeration for Weibo mobile APIs"""
    TRENDING_LIST = "trending_list"
    HOT_POSTS = "hot_posts"


class TrendingConstants:
    """Constants for Weibo mobile trending APIs"""
    # Trending list API
    TRENDING_CONTAINER_ID = "102803_ctg1_8999_-_ctg1_8999_home"
    
    # Hot posts API
    HOT_POSTS_CONTAINER_ID = "102803"
    
    # Common parameters
    OPEN_APP = "0"


def generate_device_id() -> str:
    """Generate a random device ID for Weibo requests"""
    chars = "0123456789abcdef"
    return ''.join(random.choices(chars, k=32))


def create_container_id(search_type: SearchType, keyword: str) -> str:
    """Create container ID for search requests"""
    return f"100103type={search_type.value}&q={keyword}"


def extract_cookies_from_browser(web_cookies: List[Dict]) -> Tuple[str, Dict[str, str]]:
    """Extract and format cookies from browser, filtering only Weibo related cookies"""
    cookie_dict = {}
    cookie_parts = []
    
    # Weibo domain patterns to filter
    weibo_domains = [
        # '.weibo.com',
        '.weibo.cn',
        # 'm.weibo.cn',
        # 'www.weibo.com'
    ]
    for cookie in web_cookies:
        if 'name' in cookie and 'value' in cookie and 'domain' in cookie:
            domain = cookie['domain']
            
            # Filter only Weibo related cookies
            if any(wb_domain in domain for wb_domain in weibo_domains):
                name = cookie['name']
                value = cookie['value']
                cookie_dict[name] = value
                cookie_parts.append(f"{name}={value}")
    
    cookie_string = "; ".join(cookie_parts)
    return cookie_string, cookie_dict


def extract_mid_from_url(weibo_url: str) -> Optional[str]:
    """Extract mid from Weibo URL"""
    patterns = [
        r'/detail/(\w+)',
        r'mid=(\w+)',
        r'/(\w+)$',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, weibo_url)
        if match:
            return match.group(1)
    
    return None


def extract_user_id_from_url(user_url: str) -> Optional[str]:
    """Extract user ID from Weibo user URL"""
    patterns = [
        r'/u/(\d+)',
        r'uid=(\d+)',
        r'/profile/(\d+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, user_url)
        if match:
            return match.group(1)
    
    return None


def parse_weibo_time(time_str: str) -> Optional[int]:
    """Parse Weibo time string to timestamp"""
    if not time_str:
        return None
        
    try:
        # Handle relative time like "3åˆ†é’Ÿå‰", "1å°æ—¶å‰", etc.
        if "åˆ†é’Ÿå‰" in time_str:
            minutes = int(re.search(r'(\d+)åˆ†é’Ÿå‰', time_str).group(1))
            return int(time.time()) - minutes * 60
        elif "å°æ—¶å‰" in time_str:
            hours = int(re.search(r'(\d+)å°æ—¶å‰', time_str).group(1))
            return int(time.time()) - hours * 3600
        elif "å¤©å‰" in time_str:
            days = int(re.search(r'(\d+)å¤©å‰', time_str).group(1))
            return int(time.time()) - days * 86400
        elif "ä»Šå¤©" in time_str:
            return int(time.time())
        elif "æ˜¨å¤©" in time_str:
            return int(time.time()) - 86400
        else:
            # Try to parse as timestamp
            return int(time_str)
    except (ValueError, AttributeError):
        return None


def extract_image_urls(pics: List[Dict]) -> List[str]:
    """Extract image URLs from Weibo pics data"""
    image_urls = []
    
    for pic in pics:
        if isinstance(pic, dict):
            # Try different URL fields
            url = pic.get('url') or pic.get('large', {}).get('url') or pic.get('pic_big')
            if url:
                image_urls.append(url)
    
    return image_urls


def process_weibo_text(text: str) -> str:
    """Process Weibo text content, remove HTML tags and clean up"""
    if not text:
        return ""
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def validate_weibo_data(weibo_data: Dict) -> bool:
    """Validate if weibo data contains required fields"""
    required_fields = ["id", "text", "user"]
    
    for field in required_fields:
        if field not in weibo_data:
            return False
    
    return True


def filter_search_result_card(card_list: List[Dict]) -> List[Dict]:
    """
    Filter Weibo search results, only keep card_type=9 data
    """
    note_list: List[Dict] = []
    
    for card_item in card_list:
        if card_item.get("card_type") == 9:
            note_list.append(card_item)
            
        # Check card_group for nested items
        card_group = card_item.get("card_group", [])
        for card_group_item in card_group:
            if card_group_item.get("card_type") == 9:
                note_list.append(card_group_item)

    return note_list


def extract_container_params(m_weibocn_params: str) -> Dict[str, str]:
    """Extract container parameters from M_WEIBOCN_PARAMS cookie"""
    try:
        params_dict = parse_qs(unquote(m_weibocn_params))
        return {
            "fid_container_id": params_dict.get("fid", [""])[0],
            "lfid_container_id": params_dict.get("lfid", [""])[0]
        }
    except Exception:
        return {"fid_container_id": "", "lfid_container_id": ""}


def build_image_proxy_url(image_url: str, proxy_host: str = "https://i1.wp.com/") -> str:
    """Build proxied image URL to bypass anti-hotlinking"""
    if not image_url.startswith("http"):
        return image_url
    
    # Remove https:// prefix
    clean_url = image_url[8:] if image_url.startswith("https://") else image_url[7:]
    
    # Split URL parts
    url_parts = clean_url.split("/")
    
    # Reconstruct URL with 'large' for high quality images
    processed_url = ""
    for i, part in enumerate(url_parts):
        if i == 1:  # Insert 'large' after domain
            processed_url += "large/"
        elif i == len(url_parts) - 1:  # Last part (filename)
            processed_url += part
        else:
            processed_url += part + "/"
    
    return f"{proxy_host}{processed_url}"


def sanitize_filename(filename: str) -> str:
    """Sanitize filename for file system"""
    # Remove invalid characters
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    # Remove extra spaces
    filename = re.sub(r'\s+', ' ', filename).strip()
    # Limit length
    if len(filename) > 100:
        filename = filename[:100]
    
    return filename or "untitled"


def extract_render_data(html_content: str) -> Optional[Dict]:
    """Extract render data from Weibo detail page HTML"""
    try:
        match = re.search(r'var \$render_data = (\[.*?\])\[0\]', html_content, re.DOTALL)
        if match:
            render_data_json = match.group(1)
            render_data_dict = json.loads(render_data_json)
            return render_data_dict[0] if render_data_dict else None
    except (json.JSONDecodeError, IndexError):
        pass
    
    return None


class WeiboError(Exception):
    """Base exception for Weibo API errors"""
    pass


class NetworkError(WeiboError):
    """Network connection error"""
    pass


class DataExtractionError(WeiboError):
    """Data extraction error"""
    pass


class AuthenticationError(WeiboError):
    """Authentication error"""
    pass


class RateLimitError(WeiboError):
    """Rate limit exceeded error"""
    pass


class ContentNotFoundError(WeiboError):
    """Content not found error"""
    pass


class ValidationError(WeiboError):
    """Data validation error"""
    pass

def extract_redirect_url_from_html(html_content: str) -> Optional[str]:
    """Extract redirect URL from HTML meta refresh or JavaScript redirect"""
    try:
        # Try meta refresh tag
        meta_match = re.search(r'<meta[^>]*http-equiv=["\']refresh["\'][^>]*content=["\'][^"\']*url=([^"\']+)["\']', html_content, re.IGNORECASE)
        if meta_match:
            return html.unescape(meta_match.group(1))
        
        # Try JavaScript location.replace
        js_match = re.search(r'location\.replace\(["\']([^"\']+)["\']\)', html_content, re.IGNORECASE)
        if js_match:
            return html.unescape(js_match.group(1))
        
        # Try window.location.href
        js_match2 = re.search(r'window\.location\.href\s*=\s*["\']([^"\']+)["\']', html_content, re.IGNORECASE)
        if js_match2:
            return html.unescape(js_match2.group(1))
            
    except Exception:
        pass
    
    return None


def decode_chinese_html(html_content: bytes) -> str:
    """Decode HTML content that might be in GBK or other Chinese encodings"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5']
    
    for encoding in encodings:
        try:
            return html_content.decode(encoding)
        except UnicodeDecodeError:
            continue
    
    # If all else fails, try with error handling
    return html_content.decode('utf-8', errors='ignore')


def get_mobile_user_agent() -> str:
    ua_list = [
        "Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (iPad; CPU OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/114.0.5735.99 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (iPad; CPU OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/114.0.5735.124 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36",
        "Mozilla/5.0 (Linux; Android 13; SAMSUNG SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/21.0 Chrome/110.0.5481.154 Mobile Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 OPR/99.0.0.0",
        "Mozilla/5.0 (Linux; Android 10; JNY-LX1; HMSCore 6.11.0.302) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.88 HuaweiBrowser/13.0.5.303 Mobile Safari/537.36"
    ]
    return random.choice(ua_list)


def transform_weibo_post_data(card_data: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo card data into structured post information
    
    Args:
        card_data: Raw card data from Weibo API
        
    Returns:
        Structured post information or None if invalid data
    """
    if not isinstance(card_data, dict) or card_data.get("card_type") != 9:
        return None
    
    mblog = card_data.get("mblog", {})
    if not mblog:
        return None
    
    user = mblog.get("user", {})
    if not user:
        return None
    
    try:
        post_info = {
            "mid": mblog.get("id"),
            "text": process_weibo_text(mblog.get("text", "")),
            "created_at": mblog.get("created_at"),
            "source": mblog.get("source"),
            "reposts_count": mblog.get("reposts_count", 0),
            "comments_count": mblog.get("comments_count", 0),
            "attitudes_count": mblog.get("attitudes_count", 0),
            "user": {
                "id": user.get("id"),
                "screen_name": user.get("screen_name"),
                "profile_image_url": user.get("profile_image_url"),
                "followers_count": user.get("followers_count", 0),
                "friends_count": user.get("friends_count", 0),
                "statuses_count": user.get("statuses_count", 0),
            },
            "pics": mblog.get("pics", []),
            "page_info": mblog.get("page_info", {}),  # Video info if present
        }
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user.get("followers_count", 0)
        if isinstance(followers_count, str):
            # Handle cases like "11.2ä¸‡"
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    post_info["user"]["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    post_info["user"]["followers_count"] = 0
            else:
                try:
                    post_info["user"]["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    post_info["user"]["followers_count"] = 0
        
        # Validate essential fields
        if not post_info["mid"] or not post_info["user"]["id"]:
            return None
            
        return post_info
        
    except Exception as e:
        # Log error but don't fail completely
        return None


def transform_weibo_search_results(api_response: Dict) -> List[Dict]:
    """
    Transform raw Weibo search API response into list of structured posts
    
    Args:
        api_response: Raw API response from search_posts_by_keyword
        
    Returns:
        List of structured post information
    """
    if not isinstance(api_response, dict):
        return []
    
    cards = api_response.get("cards", [])
    if not isinstance(cards, list):
        return []
    
    # Filter and transform cards
    filtered_cards = filter_search_result_card(cards)
    structured_posts = []
    
    for card in filtered_cards:
        post_info = transform_weibo_post_data(card)
        if post_info:
            structured_posts.append(post_info)
    
    return structured_posts


def transform_weibo_post_detail(detail_response: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo post detail response into structured post information
    
    Args:
        detail_response: Raw response from get_post_detail
        
    Returns:
        Structured post detail information or None if invalid data
    """
    if not isinstance(detail_response, dict):
        return None
    
    mblog = detail_response.get("mblog", {})
    if not mblog:
        return None
    
    user = mblog.get("user", {})
    if not user:
        return None
    
    try:
        post_detail = {
            "mid": mblog.get("id"),
            "text": process_weibo_text(mblog.get("text", "")),
            "created_at": mblog.get("created_at"),
            "source": mblog.get("source"),
            "reposts_count": mblog.get("reposts_count", 0),
            "comments_count": mblog.get("comments_count", 0),
            "attitudes_count": mblog.get("attitudes_count", 0),
            "user": {
                "id": user.get("id"),
                "screen_name": user.get("screen_name"),
                "profile_image_url": user.get("profile_image_url"),
                "followers_count": user.get("followers_count", 0),
                "friends_count": user.get("follow_count", 0),  # Note: different field name
                "statuses_count": user.get("statuses_count", 0),
                "verified": user.get("verified", False),
                "verified_type": user.get("verified_type", 0),
                "verified_reason": user.get("verified_reason", ""),
                "description": user.get("description", ""),
            },
            "pics": mblog.get("pic_ids", []),
            "pic_num": mblog.get("pic_num", 0),
            "page_info": mblog.get("page_info", {}),  # Video info if present
            "is_long_text": mblog.get("isLongText", False),
            "favorited": mblog.get("favorited", False),
            "can_edit": mblog.get("can_edit", False),
            "visible": mblog.get("visible", {}),
            "bid": mblog.get("bid", ""),
            "status_title": mblog.get("status_title", ""),
        }
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user.get("followers_count", 0)
        if isinstance(followers_count, str):
            # Handle cases like "3800.8ä¸‡"
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    post_detail["user"]["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    post_detail["user"]["followers_count"] = 0
            else:
                try:
                    post_detail["user"]["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    post_detail["user"]["followers_count"] = 0
        
        # Process video information if present
        page_info = mblog.get("page_info", {})
        if page_info and page_info.get("type") == "video":
            post_detail["video_info"] = {
                "title": page_info.get("title", ""),
                "page_title": page_info.get("page_title", ""),
                "object_id": page_info.get("object_id", ""),
                "page_url": page_info.get("page_url", ""),
                "duration": page_info.get("media_info", {}).get("duration", 0),
                "video_orientation": page_info.get("video_orientation", ""),
                "urls": page_info.get("urls", {}),
                "cover_image": {
                    "url": page_info.get("page_pic", {}).get("url", ""),
                    "width": page_info.get("page_pic", {}).get("width", ""),
                    "height": page_info.get("page_pic", {}).get("height", ""),
                }
            }
        
        # Validate essential fields
        if not post_detail["mid"] or not post_detail["user"]["id"]:
            return None
            
        return post_detail
        
    except Exception as e:
        # Log error but don't fail completely
        return None


def transform_weibo_comment_data(comment_data: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo comment data into structured comment information
    
    Args:
        comment_data: Raw comment data from Weibo API
        
    Returns:
        Structured comment information or None if invalid data
    """
    if not isinstance(comment_data, dict):
        return None
    
    user = comment_data.get("user", {})
    if not user:
        return None
    
    try:
        comment_info = {
            "id": comment_data.get("id"),
            "text": process_weibo_text(comment_data.get("text", "")),
            "created_at": comment_data.get("created_at"),
            "source": comment_data.get("source"),
            "floor_number": comment_data.get("floor_number", 0),
            "like_count": comment_data.get("like_count", 0),
            "liked": comment_data.get("liked", False),
            "user": {
                "id": user.get("id"),
                "screen_name": user.get("screen_name"),
                "profile_image_url": user.get("profile_image_url"),
                "followers_count": user.get("followers_count", 0),
                "follow_count": user.get("follow_count", 0),
                "statuses_count": user.get("statuses_count", 0),
                "verified": user.get("verified", False),
                "verified_type": user.get("verified_type", -1),
                "verified_reason": user.get("verified_reason", ""),
                "description": user.get("description", ""),
                "gender": user.get("gender", ""),
            },
            "rootid": comment_data.get("rootid"),
            "disable_reply": comment_data.get("disable_reply", 0),
            "isLikedByMblogAuthor": comment_data.get("isLikedByMblogAuthor", False),
            "bid": comment_data.get("bid", ""),
            # Sub-comments information
            "has_sub_comments": comment_data.get("comments", False),
            "sub_comments_count": comment_data.get("total_number", 0),
        }
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user.get("followers_count", 0)
        if isinstance(followers_count, str):
            # Handle cases like "115", "11ä¸‡", etc.
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    comment_info["user"]["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    comment_info["user"]["followers_count"] = 0
            else:
                try:
                    comment_info["user"]["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    comment_info["user"]["followers_count"] = 0
        
        # Validate essential fields
        if not comment_info["id"] or not comment_info["user"]["id"]:
            return None
            
        return comment_info
        
    except Exception as e:
        # Log error but don't fail completely
        return None


def transform_weibo_comments_response(comments_response: Dict) -> List[Dict]:
    """
    Transform raw Weibo comments API response into list of structured comments
    
    Args:
        comments_response: Raw API response from get_post_comments
        
    Returns:
        List of structured comment information
    """
    if not isinstance(comments_response, dict):
        return []
    
    comments_data = comments_response.get("data", [])
    if not isinstance(comments_data, list):
        return []
    
    structured_comments = []
    
    for comment in comments_data:
        comment_info = transform_weibo_comment_data(comment)
        if comment_info:
            structured_comments.append(comment_info)
    
    return structured_comments


def transform_weibo_user_info(user_response: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo user info response into structured user information
    
    Args:
        user_response: Raw response from get_user_info
        
    Returns:
        Structured user information or None if invalid data
    """
    if not isinstance(user_response, dict):
        return None
    
    user = user_response.get("user", {})
    if not user or not user.get("id"):
        return None
    
    try:
        user_info = {
            "id": user.get("id"),
            "screen_name": user.get("screen_name", ""),
            "profile_image_url": user.get("profile_image_url", ""),
            "followers_count": user.get("followers_count", 0),
            "friends_count": user.get("friends_count", 0),
            "statuses_count": user.get("statuses_count", 0),
            "verified": user.get("verified", False),
            "verified_type": user.get("verified_type", -1),
            "verified_reason": user.get("verified_reason", ""),
            "description": user.get("description", ""),
            "gender": user.get("gender", ""),
            "location": user.get("location", ""),
            "created_at": user.get("created_at", ""),
            "profile_url": user.get("profile_url", ""),
            "cover_image_phone": user.get("cover_image_phone", ""),
            "avatar_hd": user.get("avatar_hd", ""),
            # Container and navigation info
            "containerid": user_response.get("containerid", ""),
            "tabs_info": {
                "selected_tab": user_response.get("tabsInfo", {}).get("selectedTab", 1),
                "tabs": []
            }
        }
        
        # Process tabs information
        tabs = user_response.get("tabsInfo", {}).get("tabs", [])
        for tab in tabs:
            if isinstance(tab, dict):
                tab_info = {
                    "id": tab.get("id"),
                    "tab_key": tab.get("tabKey", ""),
                    "title": tab.get("title", ""),
                    "tab_type": tab.get("tab_type", ""),
                    "containerid": tab.get("containerid", ""),
                    "must_show": tab.get("must_show", 0),
                    "hidden": tab.get("hidden", 0),
                }
                
                # Add optional fields if present
                if "apipath" in tab:
                    tab_info["apipath"] = tab["apipath"]
                if "headSubTitleText" in tab:
                    tab_info["head_subtitle_text"] = tab["headSubTitleText"]
                if "tab_icon" in tab:
                    tab_info["tab_icon"] = tab["tab_icon"]
                if "tab_icon_dark" in tab:
                    tab_info["tab_icon_dark"] = tab["tab_icon_dark"]
                if "url" in tab:
                    tab_info["url"] = tab["url"]
                
                user_info["tabs_info"]["tabs"].append(tab_info)
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user.get("followers_count", 0)
        if isinstance(followers_count, str):
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    user_info["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    user_info["followers_count"] = 0
            else:
                try:
                    user_info["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    user_info["followers_count"] = 0
        
        return user_info
        
    except Exception as e:
        # Log error but don't fail completely
        return None


def transform_weibo_user_posts_response(user_posts_response: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo user posts response into structured information
    
    Args:
        user_posts_response: Raw response from get_user_posts
        
    Returns:
        Structured user posts information or None if invalid data
    """
    if not isinstance(user_posts_response, dict):
        return None
    
    user_info = user_posts_response.get("userInfo", {})
    if not user_info:
        return None
    
    try:
        user_posts_info = {
            "user": {
                "id": user_info.get("id"),
                "screen_name": user_info.get("screen_name", ""),
                "profile_image_url": user_info.get("profile_image_url", ""),
                "followers_count": user_info.get("followers_count", 0),
                "follow_count": user_info.get("follow_count", 0),
                "statuses_count": user_info.get("statuses_count", 0),
                "verified": user_info.get("verified", False),
                "verified_type": user_info.get("verified_type", -1),
                "verified_reason": user_info.get("verified_reason", ""),
                "description": user_info.get("description", ""),
                "gender": user_info.get("gender", ""),
                "profile_url": user_info.get("profile_url", ""),
                "cover_image_phone": user_info.get("cover_image_phone", ""),
                "avatar_hd": user_info.get("avatar_hd", ""),
                "mbtype": user_info.get("mbtype", 0),
                "svip": user_info.get("svip", 0),
                "urank": user_info.get("urank", 0),
                "mbrank": user_info.get("mbrank", 0),
            },
            "style_config": {
                "is_video_cover_style": user_posts_response.get("isVideoCoverStyle", 0),
                "is_star_style": user_posts_response.get("isStarStyle", 0),
            },
            "navigation": {
                "fans_scheme": user_posts_response.get("fans_scheme", ""),
                "follow_scheme": user_posts_response.get("follow_scheme", ""),
                "profile_scheme": user_posts_response.get("scheme", ""),
            },
            "tabs_info": {
                "selected_tab": user_posts_response.get("tabsInfo", {}).get("selectedTab", 1),
                "tabs": []
            },
            "toolbar_menus": [],
            "profile_ext": user_posts_response.get("profile_ext", ""),
            "show_app_tips": user_posts_response.get("showAppTips", 0),
            # Posts data if present
            "posts": [],
            "pagination": {
                "since_id": user_posts_response.get("cardlistInfo", {}).get("since_id", ""),
                "total": user_posts_response.get("cardlistInfo", {}).get("total", 0),
            }
        }
        
        # Process tabs information
        tabs = user_posts_response.get("tabsInfo", {}).get("tabs", [])
        for tab in tabs:
            if isinstance(tab, dict):
                tab_info = {
                    "id": tab.get("id"),
                    "tab_key": tab.get("tabKey", ""),
                    "title": tab.get("title", ""),
                    "tab_type": tab.get("tab_type", ""),
                    "containerid": tab.get("containerid", ""),
                    "must_show": tab.get("must_show", 0),
                    "hidden": tab.get("hidden", 0),
                }
                
                # Add optional fields if present
                if "apipath" in tab:
                    tab_info["apipath"] = tab["apipath"]
                if "headSubTitleText" in tab:
                    tab_info["head_subtitle_text"] = tab["headSubTitleText"]
                if "tab_icon" in tab:
                    tab_info["tab_icon"] = tab["tab_icon"]
                if "tab_icon_dark" in tab:
                    tab_info["tab_icon_dark"] = tab["tab_icon_dark"]
                if "url" in tab:
                    tab_info["url"] = tab["url"]
                
                user_posts_info["tabs_info"]["tabs"].append(tab_info)
        
        # Process toolbar menus
        toolbar_menus = user_info.get("toolbar_menus", [])
        for menu in toolbar_menus:
            if isinstance(menu, dict):
                menu_info = {
                    "type": menu.get("type", ""),
                    "name": menu.get("name", ""),
                    "params": menu.get("params", {}),
                    "scheme": menu.get("scheme", ""),
                }
                user_posts_info["toolbar_menus"].append(menu_info)
        
        # Process posts if present in cards
        cards = user_posts_response.get("cards", [])
        if isinstance(cards, list):
            for card in cards:
                if card.get("card_type") == 9:  # Regular post card
                    post_info = transform_weibo_post_data(card)
                    if post_info:
                        user_posts_info["posts"].append(post_info)
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user_info.get("followers_count", 0)
        if isinstance(followers_count, str):
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    user_posts_info["user"]["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    user_posts_info["user"]["followers_count"] = 0
            else:
                try:
                    user_posts_info["user"]["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    user_posts_info["user"]["followers_count"] = 0
        
        # Validate essential fields
        if not user_posts_info["user"]["id"]:
            return None
            
        return user_posts_info
        
    except Exception as e:
        # Log error but don't fail completely
        return None


def transform_weibo_trending_response(trending_response: Dict) -> List[Dict]:
    """
    Transform raw Weibo trending API response into list of structured posts
    
    Args:
        trending_response: Raw API response from get_trending_list
        
    Returns:
        List of structured post information
    """
    if not isinstance(trending_response, dict):
        return []
    
    statuses = trending_response.get("statuses", [])
    if not isinstance(statuses, list):
        return []
    
    structured_posts = []
    
    for status in statuses:
        post_info = transform_weibo_status_data(status)
        if post_info:
            structured_posts.append(post_info)
    
    return structured_posts


def transform_weibo_status_data(status_data: Dict) -> Optional[Dict]:
    """
    Transform raw Weibo status data into structured post information
    (for trending list and similar direct status responses)
    
    Args:
        status_data: Raw status data from Weibo API
        
    Returns:
        Structured post information or None if invalid data
    """
    if not isinstance(status_data, dict):
        return None
    
    user = status_data.get("user", {})
    if not user:
        return None
    
    try:
        post_info = {
            "mid": status_data.get("id"),
            "text": process_weibo_text(status_data.get("text", "")),
            "created_at": status_data.get("created_at"),
            "source": status_data.get("source"),
            "reposts_count": status_data.get("reposts_count", 0),
            "comments_count": status_data.get("comments_count", 0),
            "attitudes_count": status_data.get("attitudes_count", 0),
            "user": {
                "id": user.get("id"),
                "screen_name": user.get("screen_name"),
                "profile_image_url": user.get("profile_image_url"),
                "followers_count": user.get("followers_count", 0),
                "friends_count": user.get("follow_count", 0),  # Note: different field name
                "statuses_count": user.get("statuses_count", 0),
                "verified": user.get("verified", False),
                "verified_type": user.get("verified_type", 0),
                "verified_reason": user.get("verified_reason", ""),
                "description": user.get("description", ""),
                "gender": user.get("gender", ""),
                "mbtype": user.get("mbtype", 0),
                "svip": user.get("svip", 0),
                "urank": user.get("urank", 0),
                "mbrank": user.get("mbrank", 0),
            },
            "pics": status_data.get("pic_ids", []),
            "pic_num": status_data.get("pic_num", 0),
            "page_info": status_data.get("page_info", {}),  # Video info if present
            "is_long_text": status_data.get("isLongText", False),
            "favorited": status_data.get("favorited", False),
            "can_edit": status_data.get("can_edit", False),
            "visible": status_data.get("visible", {}),
            "bid": status_data.get("bid", ""),
            "mixed_count": status_data.get("mixed_count", 0),
            "pending_approval_count": status_data.get("pending_approval_count", 0),
            "floor_number": status_data.get("floor_number", 0),
        }
        
        # Clean up followers_count if it's a string with suffix
        followers_count = user.get("followers_count", 0)
        if isinstance(followers_count, str):
            # Handle cases like "83.2ä¸‡"
            if "ä¸‡" in followers_count:
                try:
                    num_str = followers_count.replace("ä¸‡", "")
                    post_info["user"]["followers_count"] = int(float(num_str) * 10000)
                except (ValueError, TypeError):
                    post_info["user"]["followers_count"] = 0
            else:
                try:
                    post_info["user"]["followers_count"] = int(followers_count)
                except (ValueError, TypeError):
                    post_info["user"]["followers_count"] = 0
        
        # Process video information if present
        page_info = status_data.get("page_info", {})
        if page_info and page_info.get("type") == "video":
            post_info["video_info"] = {
                "title": page_info.get("title", ""),
                "page_title": page_info.get("page_title", ""),
                "object_id": page_info.get("object_id", ""),
                "page_url": page_info.get("page_url", ""),
                "duration": page_info.get("media_info", {}).get("duration", 0),
                "video_orientation": page_info.get("video_orientation", ""),
                "urls": page_info.get("urls", {}),
                "cover_image": {
                    "url": page_info.get("page_pic", {}).get("url", ""),
                    "width": page_info.get("page_pic", {}).get("width", ""),
                    "height": page_info.get("page_pic", {}).get("height", ""),
                }
            }
        
        # Validate essential fields
        if not post_info["mid"] or not post_info["user"]["id"]:
            return None
            
        return post_info
        
    except Exception as e:
        # Log error but don't fail completely
        return None


================================================
FILE: vibe_surf/tools/website_api/xhs/__init__.py
================================================
[Empty file]


================================================
FILE: vibe_surf/tools/website_api/xhs/client.py
================================================
import asyncio
import json
import pdb
import time
from typing import Dict, List, Optional, Callable, Union, Any
import httpx
from urllib.parse import urlencode
import random
import copy
from tenacity import retry, stop_after_attempt, wait_fixed

from vibe_surf.browser.agent_browser_session import AgentBrowserSession
from vibe_surf.logger import get_logger

from .helpers import (
    generate_trace_id, create_session_id, create_signature_headers,
    extract_cookies_from_browser, XHSError, NetworkError,
    DataExtractionError, AuthenticationError, extract_user_info_from_html
)

logger = get_logger(__name__)


class SearchType:
    """Search type constants"""
    GENERAL = "general"
    LATEST = "time"
    POPULAR = "popularity_descending"


class ContentType:
    """Content type constants"""
    ALL = 0
    VIDEO = 1
    IMAGE = 2


class XiaoHongShuApiClient:
    """
    XiaoHongShu API client with integrated browser session management.
    This client handles API communication through browser session for authentication.
    """

    def __init__(self, browser_session: AgentBrowserSession, timeout: int = 60, proxy: Optional[str] = None):
        """
        Initialize the RedBook API client
        
        Args:
            timeout: Request timeout in secondsÂ¬
            proxy: Proxy URL if needed
        """
        self.browser_session = browser_session
        self.target_id = None
        self.proxy = proxy
        self.timeout = timeout
        self._api_base = "https://edith.xiaohongshu.com"
        self._web_base = "https://www.xiaohongshu.com"

        # Error constants
        self.NETWORK_ERROR_MSG = "Network connection error, please check network settings or restart"
        self.NETWORK_ERROR_CODE = 300012
        self.CONTENT_ERROR_MSG = "Content status abnormal, please check later"
        self.CONTENT_ERROR_CODE = -510001

        # Default headers
        self.default_headers = {
            'content-type': 'application/json;charset=UTF-8',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
        }
        self.cookies = {}

    async def _prepare_request_headers(self, endpoint, payload: Optional[Dict] = None):
        headers = copy.deepcopy(self.default_headers)

        js_expression = f"window._webmsxyw && window._webmsxyw('{endpoint}', {json.dumps(payload) if payload else 'null'})"
        cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=self.target_id)
        result = await cdp_session.cdp_client.send.Runtime.evaluate(
            params={
                'expression': js_expression,
                'returnByValue': True,
                'awaitPromise': True
            },
            session_id=cdp_session.session_id,
        )

        encrypt_result = result.get('result', {}).get('value') if result else None
        if encrypt_result:
            # Get browser storage value
            b1_result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "window.localStorage.getItem('b1')",
                    'returnByValue': True,
                    'awaitPromise': True
                },
                session_id=cdp_session.session_id,
            )

            b1_storage = b1_result.get('result', {}).get('value') if b1_result else None
            # Create signature headers
            signature_headers = create_signature_headers(
                a1=self.cookies.get('a1', ''),
                b1=b1_storage or '',
                x_s=encrypt_result.get('X-s', ''),
                x_t=str(encrypt_result.get('X-t', ''))
            )
            headers.update(signature_headers)

        return headers

    async def get_me(self) -> Dict:
        """
        Get current user information to check login status

        Returns:
            User information dictionary
        """
        uri = '/api/sns/web/v2/user/me'
        return await self._make_request(
            "GET", f"{self._api_base}{uri}", headers=self.default_headers
        )

    async def setup(self, target_id: Optional[str] = None):
        """
        Get XiaoHongShu cookies and verify login status

        Args:
            browser_session: Main browser session to use for navigation

        Returns:
            Dict containing status and message

        Raises:
            AuthenticationError: If user is not logged in
        """
        try:
            if self.target_id and self.cookies:
                logger.info("Already setup. Return!")
                return

            if target_id:
                self.target_id = target_id
            else:
                self.target_id = await self.browser_session.navigate_to_url("https://www.xiaohongshu.com/",
                                                                            new_tab=True)
                await asyncio.sleep(2)

            cdp_session = await self.browser_session.get_or_create_cdp_session(target_id=target_id)
            result = await asyncio.wait_for(
                cdp_session.cdp_client.send.Storage.getCookies(session_id=cdp_session.session_id), timeout=8.0
            )
            web_cookies = result.get('cookies', [])

            cookie_str, cookie_dict = extract_cookies_from_browser(web_cookies)
            self.default_headers["Cookie"] = cookie_str
            self.cookies = cookie_dict

            if not self.cookies:
                raise AuthenticationError("No valid cookies found! Please Login first!")

            user_agent_result = await cdp_session.cdp_client.send.Runtime.evaluate(
                params={
                    'expression': "navigator.userAgent",
                    'returnByValue': True,
                    'awaitPromise': True
                },
                session_id=cdp_session.session_id,
            )
            user_agent = user_agent_result.get('result', {}).get('value')
            if user_agent:
                self.default_headers["User-Agent"] = user_agent

            user_info = await self.get_me()
            if not user_info or 'user_id' not in user_info:
                self.cookies = {}
                del self.default_headers["Cookie"]
                raise AuthenticationError("No user login in xiaohongshu!")

        except Exception as e:
            logger.error(f"Failed to get XiaoHongShu cookies: {e}")
            raise e

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
    async def _make_request(self, method: str, url: str, **kwargs) -> Union[str, Dict]:
        """
        Make HTTP request with error handling

        Args:
            method: HTTP method
            url: Request URL
            **kwargs: Additional request parameters

        Returns:
            Response data
        """
        raw_response = kwargs.pop("raw_response", False)

        async with httpx.AsyncClient(proxy=self.proxy) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)

        # Handle verification challenges
        if response.status_code in [471, 461]:
            verify_type = response.headers.get("Verifytype", "")
            verify_uuid = response.headers.get("Verifyuuid", "")
            error_msg = f"Verification challenge detected, Verifytype: {verify_type}, Verifyuuid: {verify_uuid}"
            logger.error(error_msg)
            raise AuthenticationError(error_msg)

        if raw_response:
            return response.text

        try:
            data = response.json()
            if data.get("success"):
                return data.get("data", data.get("success", {}))
            elif data.get("code") == self.NETWORK_ERROR_CODE:
                raise NetworkError(self.NETWORK_ERROR_MSG)
            else:
                raise DataExtractionError(data.get("msg", "Request failed"))
        except json.JSONDecodeError:
            raise DataExtractionError(f"Invalid JSON response: {response.text}")

    async def _get_request(self, endpoint: str, params: Optional[Dict] = None) -> Dict:
        """
        Make GET request with signature

        Args:
            endpoint: API endpoint
            params: URL parameters

        Returns:
            Response data
        """
        final_endpoint = endpoint
        if params:
            final_endpoint = f"{endpoint}?{urlencode(params)}"

        headers = await self._prepare_request_headers(final_endpoint)
        return await self._make_request(
            "GET", f"{self._api_base}{final_endpoint}", headers=headers
        )

    async def _post_request(self, endpoint: str, data: Dict, **kwargs) -> Dict:
        """
        Make POST request with signature

        Args:
            endpoint: API endpoint
            data: Request body data
            **kwargs: Additional parameters

        Returns:
            Response data
        """
        headers = await self._prepare_request_headers(endpoint, data)
        json_payload = json.dumps(data, separators=(",", ":"), ensure_ascii=False)
        return await self._make_request(
            "POST", f"{self._api_base}{endpoint}",
            data=json_payload, headers=headers, **kwargs
        )

    async def search_content_by_keyword(
            self,
            keyword: str,
            session_id: Optional[str] = None,
            page: int = 1,
            page_size: int = 20,
            sort_type: str = SearchType.GENERAL,
            content_type: int = ContentType.ALL,
    ) -> List[Dict]:
        """
        Search content by keyword

        Args:
            keyword: Search keyword
            session_id: Search session ID (auto-generated if not provided)
            page: Page number
            page_size: Items per page
            sort_type: Sort method
            content_type: Content type filter

        Returns:
            List of simplified search results
        """
        if session_id is None:
            session_id = create_session_id()

        endpoint = "/api/sns/web/v1/search/notes"
        payload = {
            "keyword": keyword,
            "page": page,
            "page_size": page_size,
            "search_id": session_id,
            "sort": sort_type,
            "note_type": content_type,
        }
        result = await self._post_request(endpoint, payload)
        # Return simplified note list
        note_list = []
        for item in result.get('items', []):
            if not item.get('id'):
                continue

            note_card = item.get("note_card", {})
            user_info = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})
            image_list = note_card.get('image_list', [])
            tag_list = note_card.get('tag_list', [])

            note_data = {
                "note_id": note_card.get("note_id"),
                "type": note_card.get("type"),
                "title": note_card.get("display_title", "")[:255],
                "desc": note_card.get("desc", ""),
                "time": note_card.get("time"),
                "last_update_time": note_card.get("last_update_time", 0),
                "user_id": user_info.get("user_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar"),
                "liked_count": interact_info.get("liked_count", 0),
                "collected_count": interact_info.get("collected_count", 0),
                "comment_count": interact_info.get("comment_count", 0),
                "share_count": interact_info.get("share_count", 0),
                "ip_location": note_card.get("ip_location", ""),
                "image_list": ','.join([img.get('url', '') for img in image_list]),
                "tag_list": ','.join([tag.get('name', '') for tag in tag_list if tag.get('type') == 'topic']),
                "note_url": f"https://www.xiaohongshu.com/explore/{item.get('id')}",
                "xsec_token": item.get("xsec_token", ""),
            }
            note_list.append(note_data)

        return note_list

    async def fetch_content_details(
            self,
            content_id: str,
            xsec_token: str,
            source_channel: str = "pc_search",
    ) -> Dict:
        """
        Fetch detailed content information

        Args:
            content_id: Content ID
            source_channel: Source channel identifier
            security_token: Security token

        Returns:
            Simplified content details
        """
        payload = {
            "source_note_id": content_id,
            "image_formats": ["jpg", "webp", "avif"],
            "extra": {"need_body_topic": 1},
            "xsec_source": source_channel,
            "xsec_token": xsec_token,
        }
        endpoint = "/api/sns/web/v1/feed"
        result = await self._post_request(endpoint, payload)

        if result and result.get("items"):
            note_item = result.get("items")[0]
            note_card = note_item.get("note_card", {})
            user_info = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})
            image_list = note_card.get('image_list', [])
            tag_list = note_card.get('tag_list', [])

            return {
                "note_id": note_card.get("note_id"),
                "type": note_card.get("type"),
                "title": note_card.get("title", ""),
                "desc": note_card.get("desc", ""),
                "time": note_card.get("time"),
                "last_update_time": note_card.get("last_update_time", 0),
                "user_id": user_info.get("user_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar"),
                "liked_count": interact_info.get("liked_count", 0),
                "collected_count": interact_info.get("collected_count", 0),
                "comment_count": interact_info.get("comment_count", 0),
                "share_count": interact_info.get("share_count", 0),
                "ip_location": note_card.get("ip_location", ""),
                "image_list": ','.join([img.get('url', '') for img in image_list]),
                "tag_list": ','.join([tag.get('name', '') for tag in tag_list if tag.get('type') == 'topic']),
                "note_url": f"https://www.xiaohongshu.com/explore/{note_card.get('note_id')}",
                "xsec_token": xsec_token,
            }

        logger.error(f"Failed to fetch content {content_id}, response: {result}")
        return {}

    async def fetch_content_comments(
            self,
            content_id: str,
            xsec_token: str,
            cursor: str = "",
    ) -> List[Dict]:
        """
        Fetch content comments (first level)

        Args:
            content_id: Content ID
            security_token: Security token
            cursor: Pagination cursor

        Returns:
            List of simplified comments data
        """
        endpoint = "/api/sns/web/v2/comment/page"
        params = {
            "note_id": content_id,
            "cursor": cursor,
            "top_comment_id": "",
            "image_formats": "jpg,webp,avif",
            "xsec_token": xsec_token,
        }
        response = await self._get_request(endpoint, params)

        # Return simplified comments
        comments = []
        for comment_item in response.get("comments", []):
            if not comment_item.get("id"):
                continue

            user_info = comment_item.get("user_info", {})
            comment_pictures = [item.get("url_default", "") for item in comment_item.get("pictures", [])]
            target_comment = comment_item.get("target_comment", {})

            comment_data = {
                "comment_id": comment_item.get("id"),
                "create_time": comment_item.get("create_time"),
                "ip_location": comment_item.get("ip_location"),
                "note_id": content_id,
                "content": comment_item.get("content"),
                "user_id": user_info.get("user_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("image"),
                "sub_comment_count": comment_item.get("sub_comment_count", 0),
                "pictures": ",".join(comment_pictures),
                "parent_comment_id": target_comment.get("id", 0),
                "like_count": comment_item.get("like_count", 0),
            }
            comments.append(comment_data)

        return comments

    async def fetch_all_content_comments(
            self,
            content_id: str,
            xsec_token: str,
            fetch_interval: float = 1.0,
            progress_callback: Optional[Callable] = None,
            max_comments: int = 1000,
    ) -> List[Dict]:
        """
        Fetch all comments for content (including pagination)

        Args:
            content_id: Content ID
            security_token: Security token
            fetch_interval: Interval between requests in seconds
            progress_callback: Callback function for progress updates
            max_comments: Maximum comments to fetch

        Returns:
            List of all simplified comments
        """
        all_comments = []
        has_more = True
        cursor = ""

        while has_more and len(all_comments) < max_comments:
            endpoint = "/api/sns/web/v2/comment/page"
            params = {
                "note_id": content_id,
                "cursor": cursor,
                "top_comment_id": "",
                "image_formats": "jpg,webp,avif",
                "xsec_token": xsec_token,
            }
            comments_data = await self._get_request(endpoint, params)
            has_more = comments_data.get("has_more", False)
            cursor = comments_data.get("cursor", "")

            if "comments" not in comments_data:
                logger.info(f"No more comments found: {comments_data}")
                break

            # Get simplified comments from this batch
            batch_comments = []
            for comment_item in comments_data["comments"]:
                if not comment_item.get("id"):
                    continue

                user_info = comment_item.get("user_info", {})
                comment_pictures = [item.get("url_default", "") for item in comment_item.get("pictures", [])]
                target_comment = comment_item.get("target_comment", {})

                comment_data = {
                    "comment_id": comment_item.get("id"),
                    "create_time": comment_item.get("create_time"),
                    "ip_location": comment_item.get("ip_location"),
                    "note_id": content_id,
                    "content": comment_item.get("content"),
                    "user_id": user_info.get("user_id"),
                    "nickname": user_info.get("nickname"),
                    "avatar": user_info.get("image"),
                    "sub_comment_count": comment_item.get("sub_comment_count", 0),
                    "pictures": ",".join(comment_pictures),
                    "parent_comment_id": target_comment.get("id", 0),
                    "like_count": comment_item.get("like_count", 0),
                }
                batch_comments.append(comment_data)

            remaining_slots = max_comments - len(all_comments)
            if remaining_slots <= 0:
                break

            if len(batch_comments) > remaining_slots:
                batch_comments = batch_comments[:remaining_slots]

            if progress_callback:
                await progress_callback(content_id, batch_comments)

            await asyncio.sleep(fetch_interval)
            all_comments.extend(batch_comments)

        logger.info(f"Fetched {len(all_comments)} comments for content {content_id}")
        return all_comments

    async def get_user_profile(self, user_id: str) -> Dict:
        """
        Get user profile information

        Args:
            user_id: User ID

        Returns:
            Simplified user profile data
        """
        endpoint = f"/user/profile/{user_id}"
        try:
            html_response = await self._make_request(
                "GET", self._web_base + endpoint,
                raw_response=True, headers=self.default_headers
            )

            # Extract user info from HTML response
            if "window.__INITIAL_STATE__" in html_response:
                # For now, return basic info since full extraction would need HTML parsing
                user_info = extract_user_info_from_html(html_response)
                return user_info
            else:
                return {}

        except Exception as e:
            logger.error(f"Failed to get user profile for {user_id}: {e}")
            return {}

    async def fetch_user_content(
            self,
            user_id: str,
            cursor: str = "",
            page_size: int = 30,
    ) -> List[Dict]:
        """
        Fetch content by user

        Args:
            user_id: User ID
            cursor: Last content ID for pagination
            page_size: Number of items per page

        Returns:
            List of simplified user content data
        """
        endpoint = "/api/sns/web/v1/user_posted"
        params = {
            "user_id": user_id,
            "cursor": cursor,
            "num": page_size,
            "image_formats": "jpg,webp,avif",
        }
        response = await self._get_request(endpoint, params)

        # Return simplified note list
        note_list = []
        for note_item in response.get("notes", []):
            if not note_item.get('id'):
                continue

            user_info = note_item.get('user', {})
            interact_info = note_item.get('interact_info', {})
            image_list = note_item.get('image_list', [])
            tag_list = note_item.get('tag_list', [])

            note_data = {
                "note_id": note_item.get("id"),
                "type": note_item.get("type"),
                "title": note_item.get("display_title", "")[:255],
                "desc": note_item.get("desc", ""),
                "time": note_item.get("time"),
                "last_update_time": note_item.get("last_update_time", 0),
                "user_id": user_info.get("user_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar"),
                "liked_count": interact_info.get("liked_count", 0),
                "collected_count": interact_info.get("collected_count", 0),
                "comment_count": interact_info.get("comment_count", 0),
                "share_count": interact_info.get("share_count", 0),
                "ip_location": note_item.get("ip_location", ""),
                "image_list": ','.join([img.get('url', '') for img in image_list]),
                "tag_list": ','.join([tag.get('name', '') for tag in tag_list if tag.get('type') == 'topic']),
                "note_url": f"https://www.xiaohongshu.com/explore/{note_item.get('id')}",
                "xsec_token": note_item.get("xsec_token", ""),
            }
            note_list.append(note_data)

        return note_list

    async def fetch_all_user_content(
            self,
            user_id: str,
            fetch_interval: float = 1.0,
            progress_callback: Optional[Callable] = None,
            max_content: int = 1000,
    ) -> List[Dict]:
        """
        Fetch all content by user

        Args:
            user_id: User ID
            fetch_interval: Interval between requests in seconds
            progress_callback: Callback function for progress updates
            max_content: Maximum content items to fetch

        Returns:
            List of all simplified user content
        """
        all_content = []
        has_more = True
        cursor = ""

        while has_more and len(all_content) < max_content:
            endpoint = "/api/sns/web/v1/user_posted"
            params = {
                "user_id": user_id,
                "cursor": cursor,
                "num": 30,
                "image_formats": "jpg,webp,avif",
            }
            content_data = await self._get_request(endpoint, params)
            if not content_data:
                logger.error(f"User {user_id} may be restricted or data unavailable")
                break

            has_more = content_data.get("has_more", False)
            cursor = content_data.get("cursor", "")

            if "notes" not in content_data:
                logger.info(f"No content found: {content_data}")
                break

            # Get simplified content from this batch
            batch_content = []
            for note_item in content_data["notes"]:
                if not note_item.get('note_id'):
                    continue

                user_info = note_item.get('user', {})
                interact_info = note_item.get('interact_info', {})
                image_list = note_item.get('image_list', [])
                tag_list = note_item.get('tag_list', [])

                note_data = {
                    "note_id": note_item.get("note_id"),
                    "type": note_item.get("type"),
                    "title": note_item.get("display_title", ""),
                    "desc": note_item.get("desc", ""),
                    "time": note_item.get("time"),
                    "last_update_time": note_item.get("last_update_time", 0),
                    "user_id": user_info.get("user_id"),
                    "nickname": user_info.get("nickname"),
                    "avatar": user_info.get("avatar"),
                    "liked_count": interact_info.get("liked_count", 0),
                    "collected_count": interact_info.get("collected_count", 0),
                    "comment_count": interact_info.get("comment_count", 0),
                    "share_count": interact_info.get("share_count", 0),
                    "ip_location": note_item.get("ip_location", ""),
                    "image_list": ','.join([img.get('url', '') for img in image_list]),
                    "tag_list": ','.join([tag.get('name', '') for tag in tag_list if tag.get('type') == 'topic']),
                    "note_url": f"https://www.xiaohongshu.com/explore/{note_item.get('note_id')}",
                    "xsec_token": note_item.get("xsec_token", ""),
                }
                batch_content.append(note_data)

            logger.info(f"Fetched {len(batch_content)} content items for user {user_id}")

            remaining_slots = max_content - len(all_content)
            if remaining_slots <= 0:
                break

            content_to_add = batch_content[:remaining_slots]
            if progress_callback:
                await progress_callback(content_to_add)

            all_content.extend(content_to_add)
            await asyncio.sleep(fetch_interval)

        logger.info(f"Fetched {len(all_content)} content items for user {user_id}")
        return all_content

    async def get_home_recommendations(self) -> List[Dict]:
        """
        Get home feed recommendations with proper header signature

        Returns:
            List of simplified home feed data
        """
        payload = {
            "category": "homefeed_recommend",
            "cursor_score": "",
            "image_formats": json.dumps(["jpg", "webp", "avif"], separators=(",", ":")),
            "need_filter_image": False,
            "need_num": 8,
            "num": 18,
            "note_index": 33,
            "refresh_type": 1,
            "search_key": "",
            "unread_begin_note_id": "",
            "unread_end_note_id": "",
            "unread_note_count": 0
        }
        endpoint = "/api/sns/web/v1/homefeed"

        # Prepare headers with signature specifically for home feed
        headers = await self._prepare_request_headers(endpoint, payload)

        # Make the request with proper headers
        json_payload = json.dumps(payload, separators=(",", ":"), ensure_ascii=False)
        result = await self._make_request(
            "POST", f"{self._api_base}{endpoint}",
            data=json_payload, headers=headers
        )

        # Return simplified note list
        note_list = []
        for item in result.get("items", []):
            if not item.get('id'):
                continue
            note_card = item.get('note_card', {})
            user_info = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})
            image_list = note_card.get('image_list', [])
            tag_list = note_card.get('tag_list', [])

            note_data = {
                "note_id": item.get("id"),
                "type": note_card.get("type"),
                "title": note_card.get("display_title", ""),
                "desc": note_card.get("desc", ""),
                "time": note_card.get("time"),
                "last_update_time": note_card.get("last_update_time", 0),
                "user_id": user_info.get("user_id"),
                "nickname": user_info.get("nickname"),
                "avatar": user_info.get("avatar"),
                "liked_count": interact_info.get("liked_count", 0),
                "collected_count": interact_info.get("collected_count", 0),
                "comment_count": interact_info.get("comment_count", 0),
                "share_count": interact_info.get("share_count", 0),
                "ip_location": note_card.get("ip_location", ""),
                "image_list": ','.join([img.get('url', '') for img in image_list]),
                "tag_list": ','.join([tag.get('name', '') for tag in tag_list if tag.get('type') == 'topic']),
                "note_url": f"https://www.xiaohongshu.com/explore/{item.get('id')}",
                "xsec_token": item.get("xsec_token", ""),
            }
            note_list.append(note_data)

        return note_list

    async def submit_comment(self, content_id: str, comment_text: str) -> Dict:
        """
        Submit comment to content

        Args:
            content_id: Content ID
            comment_text: Comment text

        Returns:
            Submit result
        """
        endpoint = '/api/sns/web/v1/comment/post'
        payload = {
            "note_id": content_id,
            "content": comment_text,
            "at_users": []
        }
        return await self._post_request(endpoint, payload)

    async def close(self):
        if self.browser_session and self.target_id:
            try:
                logger.info(f"Close target id: {self.target_id}")
                await self.browser_session.cdp_client.send.Target.closeTarget(params={'targetId': self.target_id})
            except Exception as e:
                logger.warning(f"Error closing target {self.target_id}: {e}")




================================================
FILE: vibe_surf/tools/website_api/xhs/helpers.py
================================================
import hashlib
import random
import time
import json
import re
import urllib.parse
from typing import Dict, List, Tuple, Optional


def generate_trace_id() -> str:
    """Generate a random trace ID for requests"""
    chars = "abcdef0123456789"
    return ''.join(random.choices(chars, k=16))


def create_session_id() -> str:
    """Create a unique session identifier"""
    timestamp = int(time.time() * 1000) << 64
    rand_num = random.randint(0, 2147483646)
    return encode_base36(timestamp + rand_num)


def encode_base36(number: int, alphabet: str = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ') -> str:
    """Convert integer to base36 string"""
    if not isinstance(number, int):
        raise TypeError('Input must be an integer')

    if number == 0:
        return alphabet[0]

    result = ''
    sign = ''

    if number < 0:
        sign = '-'
        number = -number

    while number:
        number, remainder = divmod(number, len(alphabet))
        result = alphabet[remainder] + result

    return sign + result


def decode_base36(encoded: str) -> int:
    """Decode base36 string to integer"""
    return int(encoded, 36)


def compute_hash(data: str) -> int:
    """Compute hash for given data string"""
    hash_table = [
        0, 1996959894, 3993919788, 2567524794, 124634137, 1886057615, 3915621685,
        2657392035, 249268274, 2044508324, 3772115230, 2547177864, 162941995,
        2125561021, 3887607047, 2428444049, 498536548, 1789927666, 4089016648,
        2227061214, 450548861, 1843258603, 4107580753, 2211677639, 325883990,
        1684777152, 4251122042, 2321926636, 335633487, 1661365465, 4195302755,
        2366115317, 997073096, 1281953886, 3579855332, 2724688242, 1006888145,
        1258607687, 3524101629, 2768942443, 901097722, 1119000684, 3686517206,
        2898065728, 853044451, 1172266101, 3705015759, 2882616665, 651767980,
        1373503546, 3369554304, 3218104598, 565507253, 1454621731, 3485111705,
        3099436303, 671266974, 1594198024, 3322730930, 2970347812, 795835527,
        1483230225, 3244367275, 3060149565, 1994146192, 31158534, 2563907772,
        4023717930, 1907459465, 112637215, 2680153253, 3904427059, 2013776290,
        251722036, 2517215374, 3775830040, 2137656763, 141376813, 2439277719,
        3865271297, 1802195444, 476864866, 2238001368, 4066508878, 1812370925,
        453092731, 2181625025, 4111451223, 1706088902, 314042704, 2344532202,
        4240017532, 1658658271, 366619977, 2362670323, 4224994405, 1303535960,
        984961486, 2747007092, 3569037538, 1256170817, 1037604311, 2765210733,
        3554079995, 1131014506, 879679996, 2909243462, 3663771856, 1141124467,
        855842277, 2852801631, 3708648649, 1342533948, 654459306, 3188396048,
        3373015174, 1466479909, 544179635, 3110523913, 3462522015, 1591671054,
        702138776, 2966460450, 3352799412, 1504918807, 783551873, 3082640443,
        3233442989, 3988292384, 2596254646, 62317068, 1957810842, 3939845945,
        2647816111, 81470997, 1943803523, 3814918930, 2489596804, 225274430,
        2053790376, 3826175755, 2466906013, 167816743, 2097651377, 4027552580,
        2265490386, 503444072, 1762050814, 4150417245, 2154129355, 426522225,
        1852507879, 4275313526, 2312317920, 282753626, 1742555852, 4189708143,
        2394877945, 397917763, 1622183637, 3604390888, 2714866558, 953729732,
        1340076626, 3518719985, 2797360999, 1068828381, 1219638859, 3624741850,
        2936675148, 906185462, 1090812512, 3747672003, 2825379669, 829329135,
        1181335161, 3412177804, 3160834842, 628085408, 1382605366, 3423369109,
        3138078467, 570562233, 1426400815, 3317316542, 2998733608, 733239954,
        1555261956, 3268935591, 3050360625, 752459403, 1541320221, 2607071920,
        3965973030, 1969922972, 40735498, 2617837225, 3943577151, 1913087877,
        83908371, 2512341634, 3803740692, 2075208622, 213261112, 2463272603,
        3855990285, 2094854071, 198958881, 2262029012, 4057260610, 1759359992,
        534414190, 2176718541, 4139329115, 1873836001, 414664567, 2282248934,
        4279200368, 1711684554, 285281116, 2405801727, 4167216745, 1634467795,
        376229701, 2685067896, 3608007406, 1308918612, 956543938, 2808555105,
        3495958263, 1231636301, 1047427035, 2932959818, 3654703836, 1088359270,
        936918000, 2847714899, 3736837829, 1202900863, 817233897, 3183342108,
        3401237130, 1404277552, 615818150, 3134207493, 3453421203, 1423857449,
        601450431, 3009837614, 3294710456, 1567103746, 711928724, 3020668471,
        3272380065, 1510334235, 755167117,
    ]

    hash_val = -1
    for i in range(min(57, len(data))):
        hash_val = hash_table[(hash_val & 255) ^ ord(data[i])] ^ (hash_val >> 8)

    return hash_val ^ -1 ^ 3988292384


# Custom base64 implementation
ENCODING_CHARS = [
    "Z", "m", "s", "e", "r", "b", "B", "o", "H", "Q", "t", "N", "P", "+", "w", "O",
    "c", "z", "a", "/", "L", "p", "n", "g", "G", "8", "y", "J", "q", "4", "2", "K",
    "W", "Y", "j", "0", "D", "S", "f", "d", "i", "k", "x", "3", "V", "T", "1", "6",
    "I", "l", "U", "A", "F", "M", "9", "7", "h", "E", "C", "v", "u", "R", "X", "5",
]


def encode_triplet(triplet: int) -> str:
    """Encode 3-byte triplet to 4-character string"""
    return (
            ENCODING_CHARS[63 & (triplet >> 18)] +
            ENCODING_CHARS[63 & (triplet >> 12)] +
            ENCODING_CHARS[(triplet >> 6) & 63] +
            ENCODING_CHARS[triplet & 63]
    )


def encode_chunk(data: List[int], start: int, end: int) -> str:
    """Encode chunk of bytes"""
    result = []
    for i in range(start, end, 3):
        triplet = (data[i] << 16) | (data[i + 1] << 8) | data[i + 2]
        result.append(encode_triplet(triplet))
    return ''.join(result)


def custom_base64_encode(data: List[int]) -> str:
    """Custom base64 encoding"""
    length = len(data)
    remainder = length % 3
    chunks = []
    chunk_size = 16383

    main_length = length - remainder
    offset = 0

    while offset < main_length:
        end = min(offset + chunk_size, main_length)
        chunks.append(encode_chunk(data, offset, end))
        offset += chunk_size

    if remainder == 1:
        last_byte = data[length - 1]
        chunks.append(
            ENCODING_CHARS[last_byte >> 2] +
            ENCODING_CHARS[(last_byte << 4) & 63] +
            "=="
        )
    elif remainder == 2:
        last_two = (data[length - 2] << 8) | data[length - 1]
        chunks.append(
            ENCODING_CHARS[last_two >> 10] +
            ENCODING_CHARS[(last_two >> 4) & 63] +
            ENCODING_CHARS[(last_two << 2) & 63] +
            "="
        )

    return "".join(chunks)


def utf8_encode(text: str) -> List[int]:
    """Encode text to UTF-8 byte array"""
    encoded_text = urllib.parse.quote(text, safe='~()*!.\'')
    bytes_array = []
    i = 0

    while i < len(encoded_text):
        char = encoded_text[i]
        if char == "%":
            hex_code = encoded_text[i + 1:i + 3]
            byte_val = int(hex_code, 16)
            bytes_array.append(byte_val)
            i += 3
        else:
            bytes_array.append(ord(char))
            i += 1

    return bytes_array


def create_signature_headers(a1: str = "", b1: str = "", x_s: str = "", x_t: str = "") -> Dict[str, str]:
    """Create signature headers for API requests"""
    common_data = {
        "s0": 3,
        "s1": "",
        "x0": "1",
        "x1": "3.7.8-2",
        "x2": "Mac OS",
        "x3": "xhs-pc-web",
        "x4": "4.27.2",
        "x5": a1,
        "x6": x_t,
        "x7": x_s,
        "x8": b1,
        "x9": compute_hash(x_t + x_s + b1),
        "x10": 154,
    }

    json_data = json.dumps(common_data, separators=(',', ':'))
    encoded_bytes = utf8_encode(json_data)
    x_s_common = custom_base64_encode(encoded_bytes)
    trace_id = generate_trace_id()

    return {
        "x-s": x_s,
        "x-t": x_t,
        "x-s-common": x_s_common,
        "x-b3-traceid": trace_id
    }


def extract_cookies_from_browser(web_cookies: List[Dict]) -> Tuple[str, Dict[str, str]]:
    """Extract and format cookies from browser, filtering only XiaoHongShu related cookies"""
    cookie_dict = {}
    cookie_parts = []

    # XiaoHongShu domain patterns to filter
    xhs_domains = [
        '.xiaohongshu.com',
        'www.xiaohongshu.com',
        'edith.xiaohongshu.com'
    ]

    for cookie in web_cookies:
        if 'name' in cookie and 'value' in cookie and 'domain' in cookie:
            domain = cookie['domain']

            # Filter only XiaoHongShu related cookies
            if any(xhs_domain in domain for xhs_domain in xhs_domains):
                name = cookie['name']
                value = cookie['value']
                cookie_dict[name] = value
                cookie_parts.append(f"{name}={value}")

    cookie_string = "; ".join(cookie_parts)
    return cookie_string, cookie_dict


# Image CDN configurations
IMAGE_CDNS = [
    "https://sns-img-qc.xhscdn.com",
    "https://sns-img-hw.xhscdn.com",
    "https://sns-img-bd.xhscdn.com",
    "https://sns-img-qn.xhscdn.com",
]


def get_image_url(trace_id: str, image_format: str = "png") -> str:
    """Get image URL from trace ID"""
    cdn = random.choice(IMAGE_CDNS)
    return f"{cdn}/{trace_id}?imageView2/format/{image_format}"


def get_all_image_urls(trace_id: str, image_format: str = "png") -> List[str]:
    """Get all image URLs from different CDNs"""
    return [f"{cdn}/{trace_id}?imageView2/format/{image_format}" for cdn in IMAGE_CDNS]


def extract_trace_id_from_url(image_url: str) -> str:
    """Extract trace ID from image URL"""
    if "spectrum" in image_url:
        return f"spectrum/{image_url.split('/')[-1]}"
    return image_url.split("/")[-1]


def extract_user_info_from_html(html: str) -> Optional[Dict]:
    match = re.search(
        r"<script>window.__INITIAL_STATE__=(.+)<\/script>", html, re.M
    )
    if match is None:
        return None
    info = json.loads(match.group(1).replace(":undefined", ":null"), strict=False)
    if info is None:
        return None
    return info.get("user").get("userPageData")


class XHSError(Exception):
    """Base exception for XHS API errors"""
    pass


class NetworkError(XHSError):
    """Network connection error"""
    pass


class DataExtractionError(XHSError):
    """Data extraction error"""
    pass


class AuthenticationError(XHSError):
    """Authentication error"""
    pass



================================================
FILE: vibe_surf/tools/website_api/youtube/__init__.py
================================================
"""
YouTube API client module for VibeSurf

This module provides a browser-session based YouTube API client that can:
- Search for videos, channels, and playlists
- Get detailed video information
- Fetch video comments
- Get channel information and videos
- Access trending videos

The client uses browser session authentication to avoid needing API keys.
"""

from .client import YouTubeApiClient
from .helpers import (
    SearchType, SortType, Duration, UploadDate,
    extract_video_id_from_url, extract_channel_id_from_url,
    extract_playlist_id_from_url, parse_youtube_duration,
    format_view_count, process_youtube_text,
    YouTubeError, NetworkError, DataExtractionError,
    AuthenticationError, RateLimitError, ContentNotFoundError
)

__all__ = [
    'YouTubeApiClient',
    'SearchType', 'SortType', 'Duration', 'UploadDate',
    'extract_video_id_from_url', 'extract_channel_id_from_url',
    'extract_playlist_id_from_url', 'parse_youtube_duration',
    'format_view_count', 'process_youtube_text',
    'YouTubeError', 'NetworkError', 'DataExtractionError',
    'AuthenticationError', 'RateLimitError', 'ContentNotFoundError'
]


================================================
FILE: vibe_surf/tools/website_api/youtube/helpers.py
================================================
import pdb
import re
import json
import html
import random
import time
from typing import Dict, List, Tuple, Optional
from enum import Enum
from urllib.parse import parse_qs, unquote, urlparse


class SearchType(Enum):
    """Search type enumeration for YouTube"""
    VIDEO = "video"
    CHANNEL = "channel"
    PLAYLIST = "playlist"
    ALL = "all"


class SortType(Enum):
    """Sort type enumeration for YouTube search"""
    RELEVANCE = "relevance"
    DATE = "date"
    VIEW_COUNT = "viewCount"
    RATING = "rating"


class Duration(Enum):
    """Duration filter for YouTube search"""
    ANY = "any"
    SHORT = "short"  # < 4 minutes
    MEDIUM = "medium"  # 4-20 minutes
    LONG = "long"  # > 20 minutes


class UploadDate(Enum):
    """Upload date filter for YouTube search"""
    ANY = "any"
    HOUR = "hour"
    TODAY = "today"
    WEEK = "week"
    MONTH = "month"
    YEAR = "year"


def generate_visitor_data() -> str:
    """Generate a random visitor data string for YouTube requests"""
    chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_"
    return ''.join(random.choices(chars, k=24))


def extract_cookies_from_browser(web_cookies: List[Dict]) -> Tuple[str, Dict[str, str]]:
    """Extract and format cookies from browser, filtering only YouTube related cookies"""
    cookie_dict = {}
    cookie_parts = []
    
    # YouTube domain patterns to filter
    youtube_domains = [
        '.youtube.com',
        # 'www.youtube.com',
        # 'm.youtube.com',
        # '.google.com'
    ]
    
    for cookie in web_cookies:
        if 'name' in cookie and 'value' in cookie and 'domain' in cookie:
            domain = cookie['domain']
            
            # Filter only YouTube related cookies
            if any(yt_domain in domain for yt_domain in youtube_domains):
                name = cookie['name']
                value = cookie['value']
                cookie_dict[name] = value
                cookie_parts.append(f"{name}={value}")
    
    cookie_string = "; ".join(cookie_parts)
    return cookie_string, cookie_dict


def extract_video_id_from_url(youtube_url: str) -> Optional[str]:
    """Extract video ID from YouTube URL"""
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
        r'(?:embed\/)([0-9A-Za-z_-]{11})',
        r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})',
        r'(?:watch\?v=)([0-9A-Za-z_-]{11})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            return match.group(1)
    
    return None


def extract_channel_id_from_url(channel_url: str) -> Optional[str]:
    """Extract channel ID from YouTube channel URL"""
    patterns = [
        r'(?:channel\/)([UC][0-9A-Za-z_-]{22})',
        r'(?:c\/)([^\/\?]+)',
        r'(?:user\/)([^\/\?]+)',
        r'(?:@)([^\/\?]+)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, channel_url)
        if match:
            return match.group(1)
    
    return None


def extract_playlist_id_from_url(playlist_url: str) -> Optional[str]:
    """Extract playlist ID from YouTube playlist URL"""
    match = re.search(r'(?:list=)([0-9A-Za-z_-]+)', playlist_url)
    if match:
        return match.group(1)
    return None


def parse_youtube_duration(duration_str: str) -> int:
    """Parse YouTube duration string (e.g., "PT4M13S") to seconds"""
    if not duration_str:
        return 0
    
    # Remove PT prefix
    duration_str = duration_str.replace('PT', '')
    
    # Extract hours, minutes, seconds
    hours = 0
    minutes = 0
    seconds = 0
    
    # Hours
    hour_match = re.search(r'(\d+)H', duration_str)
    if hour_match:
        hours = int(hour_match.group(1))
    
    # Minutes
    minute_match = re.search(r'(\d+)M', duration_str)
    if minute_match:
        minutes = int(minute_match.group(1))
    
    # Seconds
    second_match = re.search(r'(\d+)S', duration_str)
    if second_match:
        seconds = int(second_match.group(1))
    
    return hours * 3600 + minutes * 60 + seconds


def format_view_count(view_count: str) -> int:
    """Parse YouTube view count string to integer"""
    if not view_count:
        return 0
    
    try:
        # Remove non-numeric characters except for multipliers
        view_count = view_count.replace(',', '').replace(' ', '').lower()
        
        multipliers = {
            'k': 1000,
            'm': 1000000,
            'b': 1000000000,
            't': 1000000000000
        }
        
        for suffix, multiplier in multipliers.items():
            if view_count.endswith(suffix):
                number = float(view_count[:-1])
                return int(number * multiplier)
        
        # Try to parse as regular integer
        return int(''.join(filter(str.isdigit, view_count)))
    
    except (ValueError, TypeError):
        return 0


def parse_youtube_time(time_str: str) -> Optional[int]:
    """Parse YouTube time string to timestamp"""
    if not time_str:
        return None
    
    try:
        # Handle relative time like "2 hours ago", "1 day ago", etc.
        if "ago" in time_str.lower():
            time_str = time_str.lower().replace('ago', '').strip()
            
            if 'second' in time_str:
                seconds = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - seconds
            elif 'minute' in time_str:
                minutes = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - minutes * 60
            elif 'hour' in time_str:
                hours = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - hours * 3600
            elif 'day' in time_str:
                days = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - days * 86400
            elif 'week' in time_str:
                weeks = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - weeks * 604800
            elif 'month' in time_str:
                months = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - months * 2592000  # Approximate
            elif 'year' in time_str:
                years = int(re.search(r'(\d+)', time_str).group(1))
                return int(time.time()) - years * 31536000  # Approximate
        
        # Try to parse as timestamp
        return int(time_str)
    
    except (ValueError, AttributeError):
        return None


def process_youtube_text(text: str) -> str:
    """Process YouTube text content, remove HTML tags and clean up"""
    if not text:
        return ""
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Decode HTML entities
    text = html.unescape(text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def validate_youtube_data(video_data: Dict) -> bool:
    """Validate if YouTube video data contains required fields"""
    required_fields = ["videoId", "title"]
    
    for field in required_fields:
        if field not in video_data:
            return False
    
    return True


def sanitize_filename(filename: str) -> str:
    """Sanitize filename for file system"""
    # Remove invalid characters
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    # Remove extra spaces
    filename = re.sub(r'\s+', ' ', filename).strip()
    # Limit length
    if len(filename) > 100:
        filename = filename[:100]
    
    return filename or "untitled"


def extract_ytcfg_data(html_content: str) -> Optional[Dict]:
    """Extract ytcfg data from YouTube page HTML"""
    try:
        # Try to find ytcfg.set pattern
        match = re.search(r'ytcfg\.set\s*\(\s*({.+?})\s*\)', html_content, re.DOTALL)
        if match:
            config_json = match.group(1)
            return json.loads(config_json)
    except (json.JSONDecodeError, IndexError):
        pass
    
    return None


def extract_initial_data(html_content: str) -> Optional[Dict]:
    """Extract initial data from YouTube page HTML"""
    try:
        # Try to find var ytInitialData pattern
        match = re.search(r'var ytInitialData = ({.+?});', html_content, re.DOTALL)
        if not match:
            # Try window.ytInitialData pattern
            match = re.search(r'window\["ytInitialData"\] = ({.+?});', html_content, re.DOTALL)
        
        if match:
            initial_data_json = match.group(1)
            return json.loads(initial_data_json)
    except (json.JSONDecodeError, IndexError):
        pass
    
    return None


def get_desktop_user_agent() -> str:
    """Get a random desktop user agent for YouTube requests"""
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    ]
    return random.choice(ua_list)


def build_search_url(query: str, search_type: SearchType = SearchType.ALL, 
                    sort_by: SortType = SortType.RELEVANCE,
                    upload_date: UploadDate = UploadDate.ANY,
                    duration: Duration = Duration.ANY) -> str:
    """Build YouTube search URL with filters"""
    base_url = "https://www.youtube.com/results"
    params = {"search_query": query}
    
    # Add search type filter
    if search_type != SearchType.ALL:
        params["sp"] = _get_search_params(search_type, sort_by, upload_date, duration)
    
    param_string = "&".join([f"{k}={v}" for k, v in params.items()])
    return f"{base_url}?{param_string}"


def _get_search_params(search_type: SearchType, sort_by: SortType, 
                      upload_date: UploadDate, duration: Duration) -> str:
    """Generate search parameters string for YouTube search filters"""
    # This is a simplified version - YouTube's actual search parameters are more complex
    # and may need to be reverse-engineered for full functionality
    filters = []
    
    if search_type == SearchType.VIDEO:
        filters.append("EgIQAQ%253D%253D")
    elif search_type == SearchType.CHANNEL:
        filters.append("EgIQAg%253D%253D")
    elif search_type == SearchType.PLAYLIST:
        filters.append("EgIQAw%253D%253D")
    
    return "".join(filters)


# Exception classes
class YouTubeError(Exception):
    """Base exception for YouTube API errors"""
    pass


class NetworkError(YouTubeError):
    """Network connection error"""
    pass


class DataExtractionError(YouTubeError):
    """Data extraction error"""
    pass


class AuthenticationError(YouTubeError):
    """Authentication error"""
    pass


class RateLimitError(YouTubeError):
    """Rate limit exceeded error"""
    pass


class ContentNotFoundError(YouTubeError):
    """Content not found error"""
    pass


class ValidationError(YouTubeError):
    """Data validation error"""
    pass


def extract_continuation_token(data: Dict) -> Optional[str]:
    """Extract continuation token for pagination"""
    try:
        # Look for continuation token in various possible locations
        if isinstance(data, dict):
            # Check common continuation locations
            continuations = data.get("continuations", [])
            if continuations and isinstance(continuations, list):
                for continuation in continuations:
                    if isinstance(continuation, dict):
                        token = continuation.get("nextContinuationData", {}).get("continuation")
                        if token:
                            return token
            
            # Check other possible locations
            reload_continuation = data.get("reloadContinuationData", {}).get("continuation")
            if reload_continuation:
                return reload_continuation
    except Exception:
        pass
    
    return None


def decode_html_entities(text: str) -> str:
    """Decode HTML entities in text"""
    if not text:
        return ""
    
    # Decode HTML entities
    text = html.unescape(text)
    
    return text


def extract_thumbnail_url(thumbnails: List[Dict]) -> str:
    """Extract the best quality thumbnail URL from thumbnails list"""
    if not thumbnails:
        return ""
    
    # Sort by resolution and pick the highest quality
    sorted_thumbnails = sorted(thumbnails, key=lambda x: x.get('width', 0) * x.get('height', 0), reverse=True)
    
    if sorted_thumbnails:
        return sorted_thumbnails[0].get('url', '')
    
    return ""


================================================
FILE: .github/workflows/publish.yml
================================================
name: Build and Publish

on:
  release:
    types: [published, edited]
  workflow_dispatch:  # Allow manual trigger

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: 'main'  # Always checkout main branch for latest code
        fetch-depth: 0  # Full history for setuptools-scm
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build pytest
        pip install -e .
    
    - name: Test CLI import
      run: |
        python -c "import vibe_surf; print(f'VibeSurf version: {vibe_surf.__version__}')"
        python -c "from vibe_surf.cli import main; print('CLI import successful')"

  build-wheels:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: 'main'  # Always checkout main branch for latest code
        fetch-depth: 0  # Full history for setuptools-scm
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build setuptools-scm[toml]
    
    - name: Build package
      run: python -m build
    
    - name: Check built package
      run: |
        pip install twine
        twine check dist/*
        
        # Show package contents
        echo "=== Built packages ==="
        ls -la dist/
        
        echo "=== Package info ==="
        python -m pip install dist/*.whl
        python -c "import vibe_surf; print(f'Installed version: {vibe_surf.__version__}')"
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist
        path: dist/

  build-executables:
    needs: test
    strategy:
      matrix:
        include:
          # Windows x64
          - os: windows-latest
            asset_name: vibesurf-windows-x64.exe
            activate_cmd: .venv\Scripts\activate.bat

    runs-on: ${{ matrix.os }}
    
    steps:
    - uses: actions/checkout@v4
      with:
        ref: 'main'  # Always checkout main branch for latest code
        fetch-depth: 0
        
    # Add diagnostic logging
    - name: Debug checkout and dependencies
      shell: bash
      run: |
        echo "=== Git info ==="
        git branch
        git log --oneline -5
        echo "=== pyproject.toml dependencies ==="
        grep -A 20 "dependencies = \[" pyproject.toml || echo "No dependencies section found"
        echo "=== vibe_surf/__init__.py ==="
        head -5 vibe_surf/__init__.py || echo "No __init__.py found"

    # Install uv (key step for consistent environment)
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    # Create uv environment and install current repository code
    - name: Create uv environment
      shell: bash
      run: |
        echo "=== Creating uv environment ==="
        uv venv --python 3.12
        
        echo "=== Installing dependencies (all platforms use uv) ==="
        uv pip install -e .
        uv pip install pyinstaller
        echo "=== Installed packages ==="
        uv pip list

    # Verify environment using uv run (cross-platform)
    - name: Verify environment with uv run
      shell: bash
      run: |
        echo "=== Environment verification using uv run ==="
        echo "Python version:"
        uv run python --version
        echo "Testing dotenv import:"
        uv run python -c "from dotenv import load_dotenv; print('dotenv available')"
        echo "Testing vibe_surf import:"
        uv run python -c "import vibe_surf; print('vibe_surf available')"
        echo "Testing CLI import:"
        uv run python -c "from vibe_surf.cli import main; print('CLI available')"

    # Build executable using uv run (cross-platform)
    - name: Build executable with uv run
      shell: bash
      run: |
        echo "=== Building executable with uv run ==="
        echo "Python version check:"
        uv run python --version
        echo "Running PyInstaller..."
        uv run pyinstaller vibesurf.spec --clean --noconfirm

    - name: Prepare executable
      shell: bash
      run: |
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          # Windows: rename .exe file
          mv dist/vibesurf.exe dist/${{ matrix.asset_name }}
        fi
        
        # Show final result
        echo "=== Final executable ==="
        ls -lh dist/${{ matrix.asset_name }}

    - name: Test executable
      shell: bash
      run: |
        # Basic test to ensure executable runs
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          echo "Testing Windows executable..."
          ls -la dist/${{ matrix.asset_name }}
        fi

    - name: Upload executable
      uses: actions/upload-artifact@v4
      with:
        name: ${{ matrix.asset_name }}
        # Upload the executable file
        path: dist/${{ matrix.asset_name }}
        retention-days: 90

  publish-pypi:
    needs: [test, build-wheels]
    runs-on: ubuntu-latest
    environment: release
    permissions:
      id-token: write  # Required for trusted publishing
    
    steps:
    - name: Download build artifacts
      uses: actions/download-artifact@v4
      with:
        name: dist
        path: dist/
    
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1
      with:
        password: ${{ secrets.PYPI_API_TOKEN }}
        skip-existing: true  # Skip if version already exists on PyPI
        # Use the following line if you want to use trusted publishing instead
        # trusted-publishing: true

  publish-executables:
    needs: [test, build-executables]
    runs-on: ubuntu-latest
    if: github.event_name == 'release'
    permissions:
      contents: write  # Required for uploading to GitHub releases
      actions: read    # Required for downloading artifacts
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Upload executables to release
      uses: softprops/action-gh-release@v1
      with:
        files: |
          vibesurf-windows-x64.exe/vibesurf-windows-x64.exe

